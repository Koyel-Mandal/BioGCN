{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS8yDg1zc9Za"
      },
      "outputs": [],
      "source": [
        "!pip install rdkit\n",
        "!pip install shap\n",
        "!pip install Catboost\n",
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0FSv13Tee2d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOQQ2IN9d0Bt"
      },
      "outputs": [],
      "source": [
        "# import packages and modules\n",
        "\n",
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import Draw, Descriptors, AllChem\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from itertools import combinations\n",
        "import IPython\n",
        "from IPython.display import display, Image as IPImage\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from rdkit import Chem\n",
        "from rdkit.ML.Descriptors.MoleculeDescriptors import MolecularDescriptorCalculator\n",
        "import shap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy import stats\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, cohen_kappa_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "import pickle\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from rdkit.Chem import rdmolops\n",
        "from scipy.stats import pearsonr\n",
        "from rdkit.Chem import rdchem\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data, DataLoader, Batch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOU58Vo4dLA1"
      },
      "outputs": [],
      "source": [
        "# Read Input files\n",
        "\n",
        "train_df = pd.read_excel('/content/drive/MyDrive/HOB/input_gnn_20.xlsx', sheet_name = \"train\")\n",
        "print(train_df.shape)\n",
        "\n",
        "test_df = pd.read_excel('/content/drive/MyDrive/HOB/input_gnn_20.xlsx', sheet_name = \"test\")\n",
        "print(test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Izhz-N6dfWPf"
      },
      "outputs": [],
      "source": [
        "# ATC code frequency matrix\n",
        "atc_train = pd.read_excel('/content/drive/MyDrive/HOB/drug_atc.xlsx', sheet_name='train_final')\n",
        "atc_test = pd.read_excel('/content/drive/MyDrive/HOB/drug_atc.xlsx', sheet_name='test')\n",
        "atc_train_filled = atc_train[['Drug Name', 'ATC_Codes']].fillna('0').apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
        "atc_test_filled = atc_test[['Drug Name', 'ATC_Codes']].fillna('0').apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
        "atc_codes = pd.concat([atc_train_filled, atc_test_filled], axis=0).reset_index(drop=True)\n",
        "\n",
        "# Function to split ATC codes\n",
        "def split_atc_codes(atc_codes):\n",
        "    if atc_codes == '0':\n",
        "        return ['0']\n",
        "    atc_codes = atc_codes.strip()\n",
        "    parts = []\n",
        "    for code in atc_codes.split(', '):\n",
        "        code = code.strip()\n",
        "        if len(code) >= 1:\n",
        "            parts.append(code[0])  # Single char\n",
        "        if len(code) >= 3:\n",
        "            parts.append(code[:3])  # Single char and two digits\n",
        "        if len(code) >= 4:\n",
        "            parts.append(code[:4])  # Single char, two digits, single char\n",
        "        if len(code) >= 5:\n",
        "            parts.append(code[:5])  # Single char, two digits, two chars\n",
        "        parts.append(code)  # Entire string\n",
        "    return parts\n",
        "\n",
        "# Apply split function and get unique ATC code parts\n",
        "all_atc_parts = atc_codes['ATC_Codes'].apply(split_atc_codes)\n",
        "unique_atc_parts = set(part for sublist in all_atc_parts for part in sublist)\n",
        "unique_atc_parts = sorted(unique_atc_parts)\n",
        "\n",
        "unique_atc_parts = [code for code in unique_atc_parts if code != '0']\n",
        "\n",
        "atc_matrix_train = pd.DataFrame(0, index = atc_train['Drug Name'], columns = unique_atc_parts)\n",
        "atc_matrix_test = pd.DataFrame(0, index = atc_test['Drug Name'], columns = unique_atc_parts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVVqhxgsfo0O"
      },
      "outputs": [],
      "source": [
        "atc_matrix_train = pd.DataFrame(0, index=atc_train['Drug Name'], columns=unique_atc_parts)\n",
        "\n",
        "# Assuming atc_train_filled is a DataFrame or converting if it's not\n",
        "if not isinstance(atc_train_filled, pd.DataFrame):\n",
        "    atc_train_filled = pd.DataFrame(atc_train_filled)\n",
        "\n",
        "# Now iterate over rows in the DataFrame\n",
        "for i, row in atc_train_filled.iterrows():\n",
        "    atc_codes = split_atc_codes(row['ATC_Codes'])\n",
        "   # print(atc_codes)\n",
        "    for code in atc_codes:\n",
        "        if code in unique_atc_parts:\n",
        "            atc_matrix_train.at[row['Drug Name'], code] += 1\n",
        "\n",
        "# Fill NaN or missing values with 0\n",
        "atc_matrix_train.fillna(0, inplace=True)\n",
        "\n",
        "# Display the resulting matrix\n",
        "#print(atc_matrix_train)\n",
        "print(atc_matrix_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6Y-rkqcj3AH"
      },
      "outputs": [],
      "source": [
        "# Index of the row to delete\n",
        "index_to_delete = 'Selenium'\n",
        "\n",
        "# Drop the row with the specified index\n",
        "atc_matrix_train = atc_matrix_train.drop(index_to_delete)\n",
        "\n",
        "# Reset index if needed\n",
        "#drug_embeddings_df.reset_index(drop=True, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40fvz4UPfvFH"
      },
      "outputs": [],
      "source": [
        "atc_matrix_test = pd.DataFrame(0, index=atc_test['Drug Name'], columns=unique_atc_parts)\n",
        "\n",
        "# Assuming atc_test_filled is a DataFrame or converting if it's not\n",
        "if not isinstance(atc_test_filled, pd.DataFrame):\n",
        "    atc_test_filled = pd.DataFrame(atc_test_filled)\n",
        "\n",
        "# Now iterate over rows in the DataFrame\n",
        "for i, row in atc_test_filled.iterrows():\n",
        "    atc_codes = split_atc_codes(row['ATC_Codes'])\n",
        "    #print(atc_codes)\n",
        "    for code in atc_codes:\n",
        "        if code in unique_atc_parts:\n",
        "            atc_matrix_test.at[row['Drug Name'], code] += 1\n",
        "\n",
        "# Fill NaN or missing values with 0\n",
        "atc_matrix_test.fillna(0, inplace=True)\n",
        "\n",
        "# Display the resulting matrix\n",
        "#print(atc_matrix_test)\n",
        "print(atc_matrix_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYURbnP91fG6"
      },
      "outputs": [],
      "source": [
        "def preprocess_classification_atc(X):\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "    scaler = MinMaxScaler()\n",
        "    #scaler = RobustScaler()\n",
        "    X_scaled = scaler.fit_transform(X_imputed)\n",
        "    return X_scaled\n",
        "\n",
        "train_atc_norm = preprocess_classification_atc(atc_matrix_train.iloc[:, 0:])\n",
        "test_atc_norm = preprocess_classification_atc(atc_matrix_test.iloc[:, 0:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxFxH7tUt7Kw"
      },
      "outputs": [],
      "source": [
        "cols = [f'atc_{i}' for i in range(atc_matrix_train.shape[1])]\n",
        "train_atc_norm_df = pd.DataFrame(train_atc_norm, columns=cols)\n",
        "#print(train_atc_norm_df.head())\n",
        "\n",
        "\n",
        "cols = [f'atc_{i}' for i in range(atc_matrix_test.shape[1])]\n",
        "test_atc_norm_df = pd.DataFrame(test_atc_norm, columns=cols)\n",
        "#print(test_atc_norm_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKJiWEx4DNFl"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdchem\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data, Batch\n",
        "\n",
        "# Node feature encoding for hybridization\n",
        "def encode_hybridization(hybridization):\n",
        "    hybridization_map = {\n",
        "        rdchem.HybridizationType.SP: 0,\n",
        "        rdchem.HybridizationType.SP2: 1,\n",
        "        rdchem.HybridizationType.SP3: 2,\n",
        "        rdchem.HybridizationType.SP3D: 3,\n",
        "        rdchem.HybridizationType.SP3D2: 4,\n",
        "        rdchem.HybridizationType.UNSPECIFIED: 5,\n",
        "    }\n",
        "    return hybridization_map.get(hybridization, 5)\n",
        "\n",
        "# Convert SMILES to graph data object\n",
        "def smiles_to_graph(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        print(f\"Failed to parse SMILES: {smiles}\")\n",
        "        return None\n",
        "\n",
        "    # Handle single atom or molecule with no bonds\n",
        "    if mol.GetNumBonds() == 0:\n",
        "        print(f\"SMILES has no bonds: {smiles}\")\n",
        "        return None\n",
        "\n",
        "    # Node features\n",
        "    node_features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        feature = [\n",
        "            atom.GetAtomicNum(),\n",
        "            atom.GetDegree(),\n",
        "            atom.GetTotalNumHs(),\n",
        "            atom.GetImplicitValence(),\n",
        "            int(atom.GetIsAromatic()),\n",
        "            encode_hybridization(atom.GetHybridization())\n",
        "        ]\n",
        "        node_features.append(feature)\n",
        "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "    # Edge index and edge features\n",
        "    edge_index = []\n",
        "    edge_features = []\n",
        "    for bond in mol.GetBonds():\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "        edge_index.append([i, j])\n",
        "        edge_index.append([j, i])\n",
        "        edge_features.append([\n",
        "            bond.GetBondTypeAsDouble(),\n",
        "            int(bond.GetIsConjugated()),\n",
        "            int(bond.IsInRing())\n",
        "        ])\n",
        "        edge_features.append([\n",
        "            bond.GetBondTypeAsDouble(),\n",
        "            int(bond.GetIsConjugated()),\n",
        "            int(bond.IsInRing())\n",
        "        ])\n",
        "\n",
        "    if len(edge_index) == 0:\n",
        "        print(f\"SMILES has no bonds: {smiles}\")\n",
        "        return None\n",
        "\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
        "\n",
        "    # Create a PyTorch Geometric Data object\n",
        "    data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_features)\n",
        "    return data\n",
        "\n",
        "# Apply function to convert SMILES to graph and filter out invalid ones\n",
        "#train_df = pd.DataFrame({'Structure (SMILES)': ['CCO', 'O=C=O', 'CCC']})  # Dummy DataFrame\n",
        "train_df['Graph'] = train_df['Structure (SMILES)'].apply(smiles_to_graph)\n",
        "data_list = [graph for graph in train_df['Graph'] if graph is not None]  # Filter out None values\n",
        "\n",
        "\n",
        "\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, edge_dim):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.edge_mlp = nn.Linear(edge_dim, hidden_dim)\n",
        "        self.pool = global_mean_pool\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        # Initialize with a small positive bias\n",
        "        self.conv1.bias = nn.Parameter(torch.ones(hidden_dim) * 0.01)\n",
        "        self.conv2.bias = nn.Parameter(torch.ones(hidden_dim) * 0.01)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "\n",
        "        # Normalize input features\n",
        "        x = (x - x.mean(dim=0)) / (x.std(dim=0) + 1e-5)\n",
        "\n",
        "        edge_features = self.edge_mlp(edge_attr)\n",
        "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
        "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
        "        x = self.pool(x, data.batch)\n",
        "        return self.fc(x).squeeze()\n",
        "\n",
        "def train_gnn(model, data_list, targets, epochs=100, lr=0.001):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for data, target in zip(data_list, targets):\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            loss = criterion(out, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(data_list)}')\n",
        "\n",
        "# Recreate the model and train\n",
        "gnn_model = GNN(input_dim=6, hidden_dim=32, output_dim=1, edge_dim=3)\n",
        "targets = torch.randn(len(data_list))  # Replace with actual targets\n",
        "train_gnn(gnn_model, data_list, targets, epochs=100, lr=0.001)\n",
        "\n",
        "# Generate embeddings\n",
        "def generate_embeddings(model, data_list):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for data in data_list:\n",
        "            x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "            x = (x - x.mean(dim=0)) / (x.std(dim=0) + 1e-5)\n",
        "            x = F.leaky_relu(model.conv1(x, edge_index))\n",
        "            x = F.leaky_relu(model.conv2(x, edge_index))\n",
        "            embedding = model.pool(x, data.batch)\n",
        "            embeddings.append(embedding.cpu().numpy())\n",
        "    return embeddings\n",
        "\n",
        "embeddings = generate_embeddings(gnn_model, data_list)\n",
        "\n",
        "# Print statistics about the embeddings\n",
        "import numpy as np\n",
        "embeddings_array = np.array(embeddings)\n",
        "print(f\"Embeddings shape: {embeddings_array.shape}\")\n",
        "print(f\"Mean: {np.mean(embeddings_array)}\")\n",
        "print(f\"Std: {np.std(embeddings_array)}\")\n",
        "print(f\"Min: {np.min(embeddings_array)}\")\n",
        "print(f\"Max: {np.max(embeddings_array)}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvgTjwfkk6JX"
      },
      "outputs": [],
      "source": [
        "### Graph features\n",
        "def one_hot_encoding(x, permitted_list):\n",
        "    if x not in permitted_list:\n",
        "        x = permitted_list[-1]\n",
        "    return [int(x == s) for s in permitted_list]\n",
        "\n",
        "def get_atom_features(atom, use_chirality=True, hydrogens_implicit=True):\n",
        "    permitted_list_of_atoms = ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr', 'Pt', 'Hg', 'Pb', 'Unknown']\n",
        "    if not hydrogens_implicit:\n",
        "        permitted_list_of_atoms = ['H'] + permitted_list_of_atoms\n",
        "\n",
        "    atom_type_enc = one_hot_encoding(str(atom.GetSymbol()), permitted_list_of_atoms)\n",
        "    n_heavy_neighbors_enc = one_hot_encoding(int(atom.GetDegree()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
        "    formal_charge_enc = one_hot_encoding(int(atom.GetFormalCharge()), [-3, -2, -1, 0, 1, 2, 3, \"Extreme\"])\n",
        "    hybridisation_type_enc = one_hot_encoding(str(atom.GetHybridization()), [\"S\", \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\", \"OTHER\"])\n",
        "    is_in_a_ring_enc = [int(atom.IsInRing())]\n",
        "    is_aromatic_enc = [int(atom.GetIsAromatic())]\n",
        "    atomic_mass_scaled = [(atom.GetMass() - 10.812) / 116.092]\n",
        "    vdw_radius_scaled = [(Chem.GetPeriodicTable().GetRvdw(atom.GetAtomicNum()) - 1.5) / 0.6]\n",
        "    covalent_radius_scaled = [(Chem.GetPeriodicTable().GetRcovalent(atom.GetAtomicNum()) - 0.64) / 0.76]\n",
        "\n",
        "    atom_feature_vector = atom_type_enc + n_heavy_neighbors_enc + formal_charge_enc + hybridisation_type_enc + \\\n",
        "                          is_in_a_ring_enc + is_aromatic_enc + atomic_mass_scaled + vdw_radius_scaled + \\\n",
        "                          covalent_radius_scaled\n",
        "\n",
        "    if use_chirality:\n",
        "        chirality_type_enc = one_hot_encoding(str(atom.GetChiralTag()), [\"CHI_UNSPECIFIED\", \"CHI_TETRAHEDRAL_CW\", \"CHI_TETRAHEDRAL_CCW\", \"CHI_OTHER\"])\n",
        "        atom_feature_vector += chirality_type_enc\n",
        "\n",
        "    if hydrogens_implicit:\n",
        "        n_hydrogens_enc = one_hot_encoding(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
        "        atom_feature_vector += n_hydrogens_enc\n",
        "\n",
        "    return np.array(atom_feature_vector)\n",
        "\n",
        "def get_bond_features(bond, use_stereochemistry=True):\n",
        "    permitted_list_of_bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE, Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
        "    bond_type_enc = one_hot_encoding(bond.GetBondType(), permitted_list_of_bond_types)\n",
        "    bond_is_conj_enc = [int(bond.GetIsConjugated())]\n",
        "    bond_is_in_ring_enc = [int(bond.IsInRing())]\n",
        "    bond_feature_vector = bond_type_enc + bond_is_conj_enc + bond_is_in_ring_enc\n",
        "\n",
        "    if use_stereochemistry:\n",
        "        stereo_type_enc = one_hot_encoding(str(bond.GetStereo()), [\"STEREOZ\", \"STEREOE\", \"STEREOANY\", \"STEREONONE\"])\n",
        "        bond_feature_vector += stereo_type_enc\n",
        "\n",
        "    return np.array(bond_feature_vector)\n",
        "\n",
        "def smiles_to_graph(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        print(f\"Failed to parse SMILES: {smiles}\")\n",
        "        return None\n",
        "\n",
        "    node_features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        node_features.append(get_atom_features(atom))\n",
        "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "    edge_indices = []\n",
        "    edge_features = []\n",
        "    for bond in mol.GetBonds():\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "        edge_indices.extend([[i, j], [j, i]])\n",
        "        edge_features.extend([get_bond_features(bond)] * 2)\n",
        "\n",
        "    if len(edge_indices) == 0:\n",
        "        print(f\"SMILES has no bonds: {smiles}\")\n",
        "        return None\n",
        "\n",
        "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
        "    edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
        "\n",
        "    data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_features)\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CcPZvQouk_Xo"
      },
      "outputs": [],
      "source": [
        "#GCN\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, edge_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.edge_mlp = nn.Linear(edge_dim, hidden_dim)\n",
        "        self.pool = global_mean_pool\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "        x = (x - x.mean(dim=0)) / (x.std(dim=0) + 1e-5)\n",
        "        edge_features = self.edge_mlp(edge_attr)\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        return self.pool(x, data.batch)\n",
        "\n",
        "def generate_embeddings(model, data_list):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for data in data_list:\n",
        "            embedding = model(data)\n",
        "            embeddings.append(embedding.cpu().numpy())\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk98LnbVwjR8"
      },
      "outputs": [],
      "source": [
        "# Embeddings for training data\n",
        "\n",
        "train_df['Graph'] = train_df['Structure (SMILES)'].apply(smiles_to_graph)\n",
        "data_list = [graph for graph in train_df['Graph'] if graph is not None]\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = data_list[0].x.shape[1]\n",
        "edge_dim = data_list[0].edge_attr.shape[1]\n",
        "gcn_model = GCN(input_dim=input_dim, hidden_dim=32, edge_dim=edge_dim)\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = generate_embeddings(gcn_model, data_list)\n",
        "\n",
        "# Print statistics about the embeddings\n",
        "embeddings_array = np.array(embeddings)\n",
        "print(f\"Embeddings shape: {embeddings_array.shape}\")\n",
        "print(f\"Mean: {np.mean(embeddings_array)}\")\n",
        "print(f\"Std: {np.std(embeddings_array)}\")\n",
        "print(f\"Min: {np.min(embeddings_array)}\")\n",
        "print(f\"Max: {np.max(embeddings_array)}\")\n",
        "\n",
        "# Save embeddings\n",
        "#np.save('molecular_embeddings.npy', embeddings_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqZ2dp1ywPqV"
      },
      "outputs": [],
      "source": [
        "y = train_df['Class']\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEqr1wdotWEf"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim  # Add this import for the optimizer\n",
        "\n",
        "# Assuming the necessary imports and functions are defined earlier (like smiles_to_graph)\n",
        "\n",
        "# Convert SMILES to graph data\n",
        "train_df['Graph'] = train_df['Structure (SMILES)'].apply(smiles_to_graph)\n",
        "\n",
        "# Filter out invalid graphs (if any)\n",
        "data_list = [graph for graph in train_df['Graph'] if graph is not None]\n",
        "\n",
        "# Initialize the GCN model\n",
        "input_dim = data_list[0].x.shape[1]  # Input feature dimension\n",
        "edge_dim = data_list[0].edge_attr.shape[1]  # Edge feature dimension\n",
        "gcn_model = GCN(input_dim=input_dim, hidden_dim=32, edge_dim=edge_dim)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(gcn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Loss function (assuming classification task)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# DataLoader to batch the graphs\n",
        "train_loader = DataLoader(data_list, batch_size=32, shuffle=True)\n",
        "y = train_df['Class']\n",
        "# Training loop\n",
        "gcn_model.train()  # Set the model in training mode (enables dropout if used)\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()  # Reset gradients from the previous step\n",
        "        out = gcn_model(data)  # Forward pass through the model\n",
        "        loss = criterion(out, data.y)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update model parameters\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYzPPpZhbUYp"
      },
      "outputs": [],
      "source": [
        "# Assuming data_list is already created and contains valid graph data\n",
        "\n",
        "# Check if data_list is not empty\n",
        "if data_list:\n",
        "    # Get the first graph from the list\n",
        "    first_graph = data_list[2]\n",
        "\n",
        "    # Extract node features (atom features)\n",
        "    node_features = first_graph.x\n",
        "\n",
        "    # Extract edge features (bond features)\n",
        "    edge_features = first_graph.edge_attr\n",
        "\n",
        "    print(\"Node (Atom) Features:\")\n",
        "    print(f\"Shape: {node_features.shape}\")\n",
        "    print(\"First node feature vector:\")\n",
        "    print(node_features[0])\n",
        "\n",
        "    print(\"\\nEdge (Bond) Features:\")\n",
        "    print(f\"Shape: {edge_features.shape}\")\n",
        "    print(\"First edge feature vector:\")\n",
        "    print(edge_features[0])\n",
        "\n",
        "    # Optionally, you can print the meanings of each feature\n",
        "    atom_feature_names = [\n",
        "        \"Atom type (one-hot encoded)\",\n",
        "        \"Number of heavy neighbors (one-hot encoded)\",\n",
        "        \"Formal charge (one-hot encoded)\",\n",
        "        \"Hybridization type (one-hot encoded)\",\n",
        "        \"Is in ring\",\n",
        "        \"Is aromatic\",\n",
        "        \"Atomic mass (scaled)\",\n",
        "        \"Van der Waals radius (scaled)\",\n",
        "        \"Covalent radius (scaled)\",\n",
        "        \"Chirality (one-hot encoded)\",\n",
        "        \"Number of hydrogens (one-hot encoded)\"\n",
        "    ]\n",
        "\n",
        "    bond_feature_names = [\n",
        "        \"Bond type (one-hot encoded)\",\n",
        "        \"Is conjugated\",\n",
        "        \"Is in ring\",\n",
        "        \"Stereochemistry (one-hot encoded)\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nAtom Feature Meanings:\")\n",
        "    for i, name in enumerate(atom_feature_names):\n",
        "        print(f\"{i}: {name}\")\n",
        "\n",
        "    print(\"\\nBond Feature Meanings:\")\n",
        "    for i, name in enumerate(bond_feature_names):\n",
        "        print(f\"{i}: {name}\")\n",
        "else:\n",
        "    print(\"data_list is empty. Please ensure that valid graph data has been generated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wSZp5sDw4_z"
      },
      "outputs": [],
      "source": [
        "cols = [f'emb_{i}' for i in range(embeddings_array.shape[2])]\n",
        "embeddings_array_2d = embeddings_array.reshape(embeddings_array.shape[0], embeddings_array.shape[2])\n",
        "embeddings_array_df = pd.DataFrame(embeddings_array_2d, columns=cols)\n",
        "#print(embeddings_array_df.head())\n",
        "\n",
        "train_df_cp = train_df.copy()\n",
        "train_df_cp = train_df_cp.drop('Structure (SMILES)', axis=1)\n",
        "train_df_cp = train_df_cp.drop('Graph', axis=1)\n",
        "train_df_cp = train_df_cp.drop('Drug Name', axis=1)\n",
        "\n",
        "def preprocess_classification(X):\n",
        "    X.replace([np.inf, -np.inf], np.finfo(np.float64).max, inplace=True)\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "    scaler = MinMaxScaler()\n",
        "    #scaler = RobustScaler()\n",
        "    X_scaled = scaler.fit_transform(X_imputed)\n",
        "    return X_scaled\n",
        "\n",
        "y = train_df_cp['Class']\n",
        "y_df = pd.DataFrame(y, columns=['Class'])  # Name your target column as 'Target_Class' (or any name you prefer)\n",
        "\n",
        "original_features = preprocess_classification(train_df_cp.iloc[:, 0:5])\n",
        "original_features_df = pd.DataFrame(original_features, columns = ['Drug pKa',\t'Log P',\t'Half Life (hrs-1)',\t'No of Rotatable Bonds',\t'M.Wt'])\n",
        "fea_emb = pd.concat([original_features_df, embeddings_array_df], axis=1)\n",
        "train_fea_emb_df = pd.concat([fea_emb, y_df], axis =1 ) # original_features + embeddings\n",
        "\n",
        "features_mol = preprocess_classification(train_df_cp.iloc[:,0:-1])\n",
        "features_mol_df = pd.DataFrame(features_mol, columns = train_df_cp.columns[0:-1])\n",
        "fea_emb_df = pd.concat([features_mol_df, embeddings_array_df], axis=1)\n",
        "gnn_mol_features_train_df = pd.concat([fea_emb_df, y_df], axis =1 ) #features + molecular descriptors + embeddings\n",
        "\n",
        "atc_gnn_mol_features_train = pd.concat([fea_emb_df, train_atc_norm_df], axis =1 )\n",
        "atc_gnn_mol_features_train_df = pd.concat([atc_gnn_mol_features_train, y_df], axis =1 )  #features + molecular descriptors + embeddings +atc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqQRAKhnxM3q"
      },
      "outputs": [],
      "source": [
        "# Assuming you've already trained your model on the training set\n",
        "\n",
        "# Load your test data\n",
        "test_df['Graph'] = test_df['Structure (SMILES)'].apply(smiles_to_graph)\n",
        "test_data_list = [graph for graph in test_df['Graph'] if graph is not None]\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = test_data_list[0].x.shape[1]\n",
        "edge_dim = test_data_list[0].edge_attr.shape[1]\n",
        "gnn_model = GCN(input_dim=input_dim, hidden_dim=32, edge_dim=edge_dim)\n",
        "\n",
        "# Generate embeddings\n",
        "test_embeddings = generate_embeddings(gnn_model, test_data_list)\n",
        "\n",
        "# Print statistics about the embeddings\n",
        "test_embeddings_array = np.array(test_embeddings)\n",
        "print(f\"Embeddings shape: {test_embeddings_array.shape}\")\n",
        "print(f\"Mean: {np.mean(test_embeddings_array)}\")\n",
        "print(f\"Std: {np.std(test_embeddings_array)}\")\n",
        "print(f\"Min: {np.min(test_embeddings_array)}\")\n",
        "print(f\"Max: {np.max(test_embeddings_array)}\")\n",
        "\n",
        "# Save embeddings\n",
        "#np.save('molecular_embeddings.npy', embeddings_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGOsbPjrxfh0"
      },
      "outputs": [],
      "source": [
        "cols = [f'emb_{i}' for i in range(test_embeddings_array.shape[2])]\n",
        "embeddings_array_2d = test_embeddings_array.reshape(test_embeddings_array.shape[0], test_embeddings_array.shape[2])\n",
        "embeddings_array_df = pd.DataFrame(embeddings_array_2d, columns=cols)\n",
        "#print(embeddings_array_df.head())\n",
        "\n",
        "\n",
        "def preprocess_classification(X):\n",
        "    X.replace([np.inf, -np.inf], np.finfo(np.float64).max, inplace=True)\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "    scaler = MinMaxScaler()\n",
        "    #scaler = RobustScaler()\n",
        "    X_scaled = scaler.fit_transform(X_imputed)\n",
        "    return X_scaled\n",
        "\n",
        "test_df_cp = test_df.copy()\n",
        "test_df_cp = test_df_cp.drop('Structure (SMILES)', axis=1)\n",
        "test_df_cp = test_df_cp.drop('Graph', axis=1)\n",
        "test_df_cp = test_df_cp.drop('Drug Name', axis=1)\n",
        "\n",
        "y = test_df_cp['Class']\n",
        "y_df = pd.DataFrame(y, columns=['Class'])  # Name your target column as 'Target_Class' (or any name you prefer)\n",
        "\n",
        "original_features = preprocess_classification(test_df_cp.iloc[:, 0:5])\n",
        "original_features_df = pd.DataFrame(original_features, columns = ['Drug pKa',\t'Log P',\t'Half Life (hrs-1)',\t'No of Rotatable Bonds',\t'M.Wt'])\n",
        "fea_emb = pd.concat([original_features_df, embeddings_array_df], axis=1)\n",
        "test_fea_emb_df = pd.concat([fea_emb, y_df], axis =1 ) # original_features + embeddings\n",
        "\n",
        "test_features_mol = preprocess_classification(test_df_cp.iloc[:,0:-1])\n",
        "test_features_mol_df = pd.DataFrame(test_features_mol, columns = test_df_cp.columns[0:-1])\n",
        "\n",
        "test_combined_df = pd.concat([test_features_mol_df, embeddings_array_df], axis=1)\n",
        "gnn_mol_features_test_df = pd.concat([test_combined_df, y_df], axis =1 ) # original_features + embeddings + descriptors\n",
        "\n",
        "atc_gnn_mol_features_test = pd.concat([test_combined_df, test_atc_norm_df], axis = 1)\n",
        "atc_gnn_mol_features_test_df = pd.concat([atc_gnn_mol_features_test, y_df], axis =1 ) #original_features + embeddings + descriptors + atc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRpGNBISpliq"
      },
      "outputs": [],
      "source": [
        "##Graph Sage\n",
        "\n",
        "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, Batch\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdchem\n",
        "import torch.nn as nn\n",
        "\n",
        "def one_hot_encoding(x, permitted_list):\n",
        "    if x not in permitted_list:\n",
        "        x = permitted_list[-1]\n",
        "    return [int(x == s) for s in permitted_list]\n",
        "\n",
        "def get_atom_features(atom, use_chirality=True, hydrogens_implicit=True):\n",
        "    permitted_list_of_atoms = ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr', 'Pt', 'Hg', 'Pb', 'Unknown']\n",
        "    if not hydrogens_implicit:\n",
        "        permitted_list_of_atoms = ['H'] + permitted_list_of_atoms\n",
        "\n",
        "    atom_type_enc = one_hot_encoding(str(atom.GetSymbol()), permitted_list_of_atoms)\n",
        "    n_heavy_neighbors_enc = one_hot_encoding(int(atom.GetDegree()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
        "    formal_charge_enc = one_hot_encoding(int(atom.GetFormalCharge()), [-3, -2, -1, 0, 1, 2, 3, \"Extreme\"])\n",
        "    hybridisation_type_enc = one_hot_encoding(str(atom.GetHybridization()), [\"S\", \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\", \"OTHER\"])\n",
        "    is_in_a_ring_enc = [int(atom.IsInRing())]\n",
        "    is_aromatic_enc = [int(atom.GetIsAromatic())]\n",
        "    atomic_mass_scaled = [(atom.GetMass() - 10.812) / 116.092]\n",
        "    vdw_radius_scaled = [(Chem.GetPeriodicTable().GetRvdw(atom.GetAtomicNum()) - 1.5) / 0.6]\n",
        "    covalent_radius_scaled = [(Chem.GetPeriodicTable().GetRcovalent(atom.GetAtomicNum()) - 0.64) / 0.76]\n",
        "\n",
        "    atom_feature_vector = atom_type_enc + n_heavy_neighbors_enc + formal_charge_enc + hybridisation_type_enc + \\\n",
        "                          is_in_a_ring_enc + is_aromatic_enc + atomic_mass_scaled + vdw_radius_scaled + \\\n",
        "                          covalent_radius_scaled\n",
        "\n",
        "    if use_chirality:\n",
        "        chirality_type_enc = one_hot_encoding(str(atom.GetChiralTag()), [\"CHI_UNSPECIFIED\", \"CHI_TETRAHEDRAL_CW\", \"CHI_TETRAHEDRAL_CCW\", \"CHI_OTHER\"])\n",
        "        atom_feature_vector += chirality_type_enc\n",
        "\n",
        "    if hydrogens_implicit:\n",
        "        n_hydrogens_enc = one_hot_encoding(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
        "        atom_feature_vector += n_hydrogens_enc\n",
        "\n",
        "    return np.array(atom_feature_vector)\n",
        "\n",
        "def get_bond_features(bond, use_stereochemistry=True):\n",
        "    permitted_list_of_bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE, Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
        "    bond_type_enc = one_hot_encoding(bond.GetBondType(), permitted_list_of_bond_types)\n",
        "    bond_is_conj_enc = [int(bond.GetIsConjugated())]\n",
        "    bond_is_in_ring_enc = [int(bond.IsInRing())]\n",
        "    bond_feature_vector = bond_type_enc + bond_is_conj_enc + bond_is_in_ring_enc\n",
        "\n",
        "    if use_stereochemistry:\n",
        "        stereo_type_enc = one_hot_encoding(str(bond.GetStereo()), [\"STEREOZ\", \"STEREOE\", \"STEREOANY\", \"STEREONONE\"])\n",
        "        bond_feature_vector += stereo_type_enc\n",
        "\n",
        "    return np.array(bond_feature_vector)\n",
        "\n",
        "def smiles_to_graph(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        print(f\"Failed to parse SMILES: {smiles}\")\n",
        "        return None\n",
        "\n",
        "    node_features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        node_features.append(get_atom_features(atom))\n",
        "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "    edge_indices = []\n",
        "    edge_features = []\n",
        "    for bond in mol.GetBonds():\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "        edge_indices.extend([[i, j], [j, i]])\n",
        "        edge_features.extend([get_bond_features(bond)] * 2)\n",
        "\n",
        "    if len(edge_indices) == 0:\n",
        "        print(f\"SMILES has no bonds: {smiles}\")\n",
        "        return None\n",
        "\n",
        "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
        "    edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
        "\n",
        "    data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_features)\n",
        "    return data\n",
        "\n",
        "\n",
        "class GraphSAGE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, edge_dim):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
        "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
        "        self.edge_mlp = nn.Linear(edge_dim, hidden_dim)\n",
        "        self.pool = global_mean_pool\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "\n",
        "        # Normalize input features\n",
        "        x = (x - x.mean(dim=0)) / (x.std(dim=0) + 1e-5)\n",
        "\n",
        "        # Apply edge features\n",
        "        edge_features = self.edge_mlp(edge_attr)\n",
        "\n",
        "        # First GraphSAGE layer\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "\n",
        "        # Second GraphSAGE layer\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "\n",
        "        # Global pooling\n",
        "        x = self.pool(x, batch)\n",
        "\n",
        "        # Final linear layer\n",
        "        return self.fc(x)\n",
        "\n",
        "def train_graphsage(model, data_list, targets, epochs=100, lr=0.001):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for data, target in zip(data_list, targets):\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            loss = criterion(out.squeeze(), target.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(data_list)}')\n",
        "\n",
        "def generate_embeddings(model, data_list):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for data in data_list:\n",
        "            x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "            x = (x - x.mean(dim=0)) / (x.std(dim=0) + 1e-5)\n",
        "            x = F.relu(model.conv1(x, edge_index))\n",
        "            x = F.relu(model.conv2(x, edge_index))\n",
        "            embedding = model.pool(x, batch)\n",
        "            embeddings.append(embedding.cpu().numpy())\n",
        "    return embeddings\n",
        "\n",
        "# Example usage\n",
        "train_df['Graph'] = train_df['Structure (SMILES)'].apply(smiles_to_graph)\n",
        "data_list = [graph for graph in train_df['Graph'] if graph is not None]\n",
        "targets = torch.tensor(train_df['Class'].values)\n",
        "\n",
        "input_dim = data_list[0].x.shape[1]\n",
        "edge_dim = data_list[0].edge_attr.shape[1]\n",
        "graphsage_model = GraphSAGE(input_dim=input_dim, hidden_dim=64, output_dim=1, edge_dim=edge_dim)\n",
        "\n",
        "# Train the model\n",
        "train_graphsage(graphsage_model, data_list, targets, epochs=100, lr=0.001)\n",
        "\n",
        "# Generate embeddings for training set\n",
        "embeddings = generate_embeddings(graphsage_model, data_list)\n",
        "\n",
        "# Print statistics about the embeddings\n",
        "embeddings_array = np.array(embeddings)\n",
        "print(f\"Embeddings shape: {embeddings_array.shape}\")\n",
        "print(f\"Mean: {np.mean(embeddings_array)}\")\n",
        "print(f\"Std: {np.std(embeddings_array)}\")\n",
        "print(f\"Min: {np.min(embeddings_array)}\")\n",
        "print(f\"Max: {np.max(embeddings_array)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Td_x-gzLBmSI"
      },
      "outputs": [],
      "source": [
        "# parameter settings code\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, balanced_accuracy_score, recall_score, precision_score, f1_score, cohen_kappa_score\n",
        "\n",
        "# Dense model\n",
        "class DenseModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, dropout_rate=0.5):\n",
        "        super(DenseModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class PyTorchModelWrapper(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, hidden_dim1=128, hidden_dim2=64, lr=0.01, epochs=100, dropout_rate=0.5):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim1 = hidden_dim1\n",
        "        self.hidden_dim2 = hidden_dim2\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.dropout_rate = dropout_rate  # Add dropout_rate\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = None\n",
        "        self.classes_ = np.array([0, 1])  # Binary classification\n",
        "\n",
        "    def _initialize_model(self):\n",
        "        return DenseModel(self.input_dim, self.hidden_dim1, self.hidden_dim2, 1, self.dropout_rate).to(self.device)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model = self._initialize_model()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).view(-1, 1))\n",
        "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "        self.model.train()\n",
        "        for epoch in range(self.epochs):\n",
        "            for inputs, labels in loader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model has not been fitted yet.\")\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "            outputs = self.model(X_tensor)\n",
        "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
        "        return np.hstack([1 - probs, probs])  # Return probabilities for both classes\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        return np.argmax(probs, axis=1)\n",
        "\n",
        "def custom_cross_val_score(estimator, X, y, cv, scoring):\n",
        "    scores = []\n",
        "    for train_index, test_index in cv.split(X, y):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        estimator.fit(X_train, y_train)\n",
        "        y_pred = estimator.predict(X_test)\n",
        "        y_pred_proba = estimator.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        if scoring == 'accuracy':\n",
        "            score = accuracy_score(y_test, y_pred)\n",
        "        elif scoring == 'roc_auc':\n",
        "            score = roc_auc_score(y_test, y_pred_proba)\n",
        "        elif scoring == 'balanced_accuracy':\n",
        "            score = balanced_accuracy_score(y_test, y_pred)\n",
        "        elif scoring == 'recall':\n",
        "            score = recall_score(y_test, y_pred)\n",
        "        elif scoring == 'precision':\n",
        "            score = precision_score(y_test, y_pred)\n",
        "        elif scoring == 'f1':\n",
        "            score = f1_score(y_test, y_pred)\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    return np.array(scores)\n",
        "\n",
        "def run_grid_search(X_train, y_train):\n",
        "    param_grid = {\n",
        "        'hidden_dim1': [32, 64, 128, 256],\n",
        "        'hidden_dim2': [16, 32, 64],\n",
        "        'lr': [0.001, 0.0001, 0.01],\n",
        "        'dropout_rate': [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "    }\n",
        "\n",
        "    clf = PyTorchModelWrapper(input_dim=X_train.shape[1])\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    grid_search = GridSearchCV(clf, param_grid, cv=cv, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Write all results to a text file\n",
        "    with open(\"/content/drive/MyDrive/HOB/Rerun/grid_search_results_gcn_fm.txt\", \"w\") as f:\n",
        "        f.write(\"Results for PyTorch classifier:\\n\\n\")\n",
        "        for mean_score, params in zip(grid_search.cv_results_[\"mean_test_score\"], grid_search.cv_results_[\"params\"]):\n",
        "            f.write(f\"Mean accuracy: {mean_score:.4f}, Params: {params}\\n\")\n",
        "\n",
        "    best_params = grid_search.best_params_\n",
        "    final_clf = clf.set_params(**best_params)\n",
        "\n",
        "    results = {}\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for metric in ['accuracy', 'roc_auc', 'balanced_accuracy', 'recall', 'precision', 'f1']:\n",
        "        scores = custom_cross_val_score(final_clf, X_train, y_train, cv, scoring=metric)\n",
        "        results[metric.capitalize()] = scores.mean()\n",
        "\n",
        "    # Kappa score\n",
        "    kappa_scores = []\n",
        "    for train_index, test_index in cv.split(X_train, y_train):\n",
        "        X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
        "        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
        "        final_clf.fit(X_train_fold, y_train_fold)\n",
        "        y_pred = final_clf.predict(X_test_fold)\n",
        "        kappa = cohen_kappa_score(y_test_fold, y_pred)\n",
        "        kappa_scores.append(kappa)\n",
        "    results[\"Kappa\"] = np.mean(kappa_scores)\n",
        "\n",
        "    # Append best results to the text file\n",
        "    with open(\"/content/drive/MyDrive/HOB/Rerun/grid_search_results_gcn_fm.txt\", \"a\") as f:\n",
        "        f.write(\"\\nBest Parameters:\\n\")\n",
        "        for key, value in best_params.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "        f.write(\"\\nBest Results:\\n\")\n",
        "        for key, value in results.items():\n",
        "            f.write(f\"{key}: {value:.4f}\\n\")\n",
        "\n",
        "    # Save best results to Excel file\n",
        "    df = pd.DataFrame([best_params | results])\n",
        "    df.to_excel(\"/content/drive/MyDrive/HOB/Rerun/best_results_gcn_fm.xlsx\", index=False)\n",
        "\n",
        "    return best_params, results\n",
        "\n",
        "def main():\n",
        "    # Load your data\n",
        "    X_train_tensor = torch.tensor(gnn_mol_features_train_df.values[:, :-1], dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(gnn_mol_features_train_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    # Convert to numpy for GridSearchCV\n",
        "    X_train = X_train_tensor.numpy()\n",
        "    y_train = y_train_tensor.numpy().ravel()\n",
        "\n",
        "    # Run grid search\n",
        "    best_params, results = run_grid_search(X_train, y_train)\n",
        "\n",
        "    print(\"\\nBest parameters and results have been saved to 'grid_search_results_gcn_fm.txt' and 'best_results_gcn_fm.xlsx'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9AQ2i9_tSMn"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "X_train_tensor = torch.tensor(train_fea_emb_df.values[:,:-1], dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(train_fea_emb_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(test_fea_emb_df.values[:,:-1], dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(test_fea_emb_df['Class'].values, dtype=torch.float32).view(-1, 1)'''\n",
        "\n",
        "X_train_tensor = torch.tensor(atc_gnn_mol_features_train_df.values[:,:-1], dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(atc_gnn_mol_features_train_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(atc_gnn_mol_features_test_df.values[:,:-1], dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(atc_gnn_mol_features_test_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "'''\n",
        "X_train_tensor = torch.tensor(gnn_mol_features_train_df.values[:,:-1], dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(gnn_mol_features_train_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(gnn_mol_features_test_df.values[:,:-1], dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(gnn_mol_features_test_df['Class'].values, dtype=torch.float32).view(-1, 1)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHyRBONm5GiN"
      },
      "outputs": [],
      "source": [
        "#Cross validation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, SubsetRandomSampler\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, roc_curve\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class DenseModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, dropout_rate=0.5):\n",
        "        super(DenseModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def train_dense_model(model, train_loader, criterion, optimizer, epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_preds).flatten(), np.array(all_labels).flatten()\n",
        "\n",
        "def cross_validate_dense_model(X, y, input_dim, hidden_dim1, hidden_dim2, output_dim, criterion, n_splits=5, n_repeats=10, epochs=100, batch_size=32):\n",
        "    cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats)\n",
        "    metrics = {\n",
        "        'accuracy': [], 'balanced_accuracy': [], 'precision': [],\n",
        "        'recall': [], 'f1': [], 'auc': [], 'kappa' : []\n",
        "    }\n",
        "    all_roc_data = []\n",
        "\n",
        "    dataset = TensorDataset(torch.FloatTensor(X), torch.FloatTensor(y))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X)):\n",
        "        print(f\"Fold {fold + 1}/{n_splits * n_repeats}\")\n",
        "\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "        val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "        model = DenseModel(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
        "        optimizer = optim.Adam(model.parameters(), lr= 0.0001)\n",
        "\n",
        "        train_dense_model(model, train_loader, criterion, optimizer, epochs)\n",
        "\n",
        "        val_preds, val_labels = evaluate_model(model, val_loader)\n",
        "\n",
        "        metrics['accuracy'].append(accuracy_score(val_labels, val_preds > 0.5))\n",
        "        metrics['balanced_accuracy'].append(balanced_accuracy_score(val_labels, val_preds > 0.5))\n",
        "        metrics['precision'].append(precision_score(val_labels, val_preds > 0.5))\n",
        "        metrics['recall'].append(recall_score(val_labels, val_preds > 0.5))\n",
        "        metrics['f1'].append(f1_score(val_labels, val_preds > 0.5))\n",
        "        metrics['auc'].append(roc_auc_score(val_labels, val_preds))\n",
        "        metrics['kappa'].append(cohen_kappa_score(val_labels, val_preds > 0.5))\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(val_labels, val_preds)\n",
        "        all_roc_data.append((fpr, tpr, metrics['auc'][-1]))\n",
        "\n",
        "        print(f\"Fold {fold + 1} - Accuracy: {metrics['accuracy'][-1]:.4f}, Balanced Accuracy: {metrics['balanced_accuracy'][-1]:.4f}\")\n",
        "\n",
        "    avg_metrics = {metric: np.mean(scores) for metric, scores in metrics.items()}\n",
        "    std_metrics = {metric: np.std(scores) for metric, scores in metrics.items()}\n",
        "\n",
        "    print(\"\\nAverage Metrics:\")\n",
        "    for metric, value in avg_metrics.items():\n",
        "        print(f\"{metric.capitalize()}: {value:.4f} ± {std_metrics[metric]:.4f}\")\n",
        "\n",
        "    # Plot ROC curves\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "    mean_tpr = np.zeros_like(mean_fpr)\n",
        "    for roc in all_roc_data:\n",
        "        fpr, tpr, auc_score = roc\n",
        "        mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
        "    mean_tpr /= len(all_roc_data)\n",
        "    mean_auc = np.mean([roc[2] for roc in all_roc_data])\n",
        "    plt.plot(mean_fpr, mean_tpr, lw=2, label=f'Mean ROC (AUC = {mean_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='r', lw=2, label='Random Guess')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=24)\n",
        "    plt.ylabel('True Positive Rate', fontsize=24)\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=24)\n",
        "    plt.legend(loc=\"lower right\", fontsize=18)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/MyDrive/HOB/Rerun/roc_curve_mol.png', format='png', bbox_inches='tight')\n",
        "    plt.savefig('/content/drive/MyDrive/HOB/Rerun/roc_curve_mol.eps', format='eps', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return avg_metrics, std_metrics\n",
        "\n",
        "# Usage\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "hidden_dim1 = 32\n",
        "hidden_dim2 = 16\n",
        "output_dim = 1\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "avg_metrics, std_metrics = cross_validate_dense_model(\n",
        "    X_train_tensor.numpy(), y_train_tensor.numpy(),\n",
        "    input_dim, hidden_dim1, hidden_dim2, output_dim, criterion,\n",
        "    n_splits=5, n_repeats=10, epochs=100, batch_size=32\n",
        ")\n",
        "def save_metrics_to_excel(avg_metrics, std_metrics, filename='metrics.xlsx'):\n",
        "    # Combine avg and std metrics into a single DataFrame\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Metric': list(avg_metrics.keys()),\n",
        "        'Average': list(avg_metrics.values()),\n",
        "        'Standard Deviation': list(std_metrics.values())\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame to an Excel file\n",
        "    metrics_df.to_excel(filename, index=False)\n",
        "\n",
        "save_metrics_to_excel(avg_metrics, std_metrics, '/content/drive/MyDrive/HOB/Rerun/metrics.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGulywlfPw8T"
      },
      "outputs": [],
      "source": [
        "#Different sampling techniques using cross validation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, SubsetRandomSampler\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, roc_curve\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "\n",
        "class DenseModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, dropout_rate=0.5):\n",
        "        super(DenseModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def train_dense_model(model, train_loader, criterion, optimizer, epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_preds).flatten(), np.array(all_labels).flatten()\n",
        "\n",
        "\n",
        "def cross_validate_dense_model(X, y, input_dim, hidden_dim1, hidden_dim2, output_dim, criterion,\n",
        "                               n_splits=5, n_repeats=10, epochs=100, batch_size=32, sampling_method='smote'):\n",
        "    cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n",
        "    metrics = {\n",
        "        'accuracy': [], 'balanced_accuracy': [], 'precision': [],\n",
        "        'recall': [], 'f1': [], 'auc': [], 'kappa': []\n",
        "    }\n",
        "    all_roc_data = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X)):\n",
        "        print(f\"Fold {fold + 1}/{n_splits * n_repeats}\")\n",
        "\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Apply sampling method\n",
        "        if sampling_method == 'ros':\n",
        "            sampler = RandomOverSampler(random_state=42)\n",
        "        elif sampling_method == 'rus':\n",
        "            sampler = RandomUnderSampler(random_state=42)\n",
        "        elif sampling_method == 'smote':\n",
        "            sampler = SMOTE(random_state=42)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid sampling method. Choose 'ros', 'rus', or 'smote'.\")\n",
        "\n",
        "        X_train_resampled, y_train_resampled = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        train_dataset = TensorDataset(torch.FloatTensor(X_train_resampled), torch.FloatTensor(y_train_resampled))\n",
        "        val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "        model = DenseModel(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
        "        optimizer = optim.Adam(model.parameters(), lr= 0.0001)\n",
        "\n",
        "        train_dense_model(model, train_loader, criterion, optimizer, epochs)\n",
        "\n",
        "        val_preds, val_labels = evaluate_model(model, val_loader)\n",
        "\n",
        "        metrics['accuracy'].append(accuracy_score(val_labels, val_preds > 0.5))\n",
        "        metrics['balanced_accuracy'].append(balanced_accuracy_score(val_labels, val_preds > 0.5))\n",
        "        metrics['precision'].append(precision_score(val_labels, val_preds > 0.5))\n",
        "        metrics['recall'].append(recall_score(val_labels, val_preds > 0.5))\n",
        "        metrics['f1'].append(f1_score(val_labels, val_preds > 0.5))\n",
        "        metrics['auc'].append(roc_auc_score(val_labels, val_preds))\n",
        "        metrics['kappa'].append(cohen_kappa_score(val_labels, val_preds > 0.5))\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(val_labels, val_preds)\n",
        "        all_roc_data.append((fpr, tpr, metrics['auc'][-1]))\n",
        "\n",
        "        print(f\"Fold {fold + 1} - Accuracy: {metrics['accuracy'][-1]:.4f}, Balanced Accuracy: {metrics['balanced_accuracy'][-1]:.4f}\")\n",
        "\n",
        "    avg_metrics = {metric: np.mean(scores) for metric, scores in metrics.items()}\n",
        "    std_metrics = {metric: np.std(scores) for metric, scores in metrics.items()}\n",
        "\n",
        "    print(\"\\nAverage Metrics:\")\n",
        "    for metric, value in avg_metrics.items():\n",
        "        print(f\"{metric.capitalize()}: {value:.4f} ± {std_metrics[metric]:.4f}\")\n",
        "\n",
        "    # Plot ROC curves\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "    mean_tpr = np.zeros_like(mean_fpr)\n",
        "    for roc in all_roc_data:\n",
        "        fpr, tpr, auc_score = roc\n",
        "        mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
        "    mean_tpr /= len(all_roc_data)\n",
        "    mean_auc = np.mean([roc[2] for roc in all_roc_data])\n",
        "    plt.plot(mean_fpr, mean_tpr, lw=2, label=f'Mean ROC (AUC = {mean_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='r', lw=2, label='Random Guess')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=24)\n",
        "    plt.ylabel('True Positive Rate', fontsize=24)\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=24)\n",
        "    plt.legend(loc=\"lower right\", fontsize=18)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/MyDrive/HOB/Rerun/roc_curve_mol.png', format='png', bbox_inches='tight')\n",
        "    plt.savefig('/content/drive/MyDrive/HOB/Rerun/roc_curve_mol.eps', format='eps', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return avg_metrics, std_metrics\n",
        "\n",
        "# Usage\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "hidden_dim1 = 32\n",
        "hidden_dim2 = 16\n",
        "output_dim = 1\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "avg_metrics, std_metrics = cross_validate_dense_model(\n",
        "    X_train_tensor.numpy(), y_train_tensor.numpy(),\n",
        "    input_dim, hidden_dim1, hidden_dim2, output_dim, criterion,\n",
        "    n_splits=5, n_repeats=10, epochs=100, batch_size=32, sampling_method='smote'\n",
        ")\n",
        "\n",
        "save_metrics_to_excel(avg_metrics, std_metrics, '/content/drive/MyDrive/HOB/Rerun/metrics.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2199ANAXuaF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, roc_curve, classification_report\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Usage\n",
        "'''\n",
        "X_train_tensor = torch.tensor(atc_gnn_mol_features_train_df.values[:,:-1], dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(atc_gnn_mol_features_train_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(atc_gnn_mol_features_test_df.values[:,:-1], dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(atc_gnn_mol_features_test_df['Class'].values, dtype=torch.float32).view(-1, 1)'''\n",
        "\n",
        "X_train_tensor = torch.tensor(gnn_mol_features_train_df.values[:,:-1], dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(gnn_mol_features_train_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(gnn_mol_features_test_df.values[:,:-1], dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(gnn_mol_features_test_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "\n",
        "class DenseModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, dropout_rate=0.3):\n",
        "        super(DenseModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def train_dense_model(model, train_loader, criterion, optimizer, epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_preds).flatten(), np.array(all_labels).flatten()\n",
        "\n",
        "def run_experiment(X_train, y_train, X_test, y_test, input_dim, hidden_dim1, hidden_dim2, output_dim, criterion, epochs=100, batch_size=32):\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    model = DenseModel(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    train_dense_model(model, train_loader, criterion, optimizer, epochs)\n",
        "\n",
        "    test_preds, test_labels = evaluate_model(model, test_loader)\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(test_labels, test_preds > 0.5),\n",
        "        'balanced_accuracy': balanced_accuracy_score(test_labels, test_preds > 0.5),\n",
        "        'precision': precision_score(test_labels, test_preds > 0.5),\n",
        "        'recall': recall_score(test_labels, test_preds > 0.5),\n",
        "        'f1': f1_score(test_labels, test_preds > 0.5),\n",
        "        'auc': roc_auc_score(test_labels, test_preds),\n",
        "        'kappa': cohen_kappa_score(test_labels, test_preds > 0.5)\n",
        "    }\n",
        "\n",
        "    class_report = classification_report(test_labels, test_preds > 0.5, output_dict=True)\n",
        "\n",
        "    return metrics, class_report, model, test_preds, test_labels\n",
        "\n",
        "def main(X_train, y_train, X_test, y_test, input_dim, hidden_dim1, hidden_dim2, output_dim, criterion, n_runs=10, epochs=100, batch_size=32):\n",
        "    all_metrics = []\n",
        "    best_accuracy = 0\n",
        "    best_model = None\n",
        "    best_class_report = None\n",
        "    best_predictions = None\n",
        "    best_labels = None\n",
        "    best_metrics = None\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"Run {run + 1}/{n_runs}\")\n",
        "        metrics, class_report, model, test_preds, test_labels = run_experiment(X_train, y_train, X_test, y_test, input_dim, hidden_dim1, hidden_dim2, output_dim, criterion, epochs, batch_size)\n",
        "        all_metrics.append(metrics)\n",
        "\n",
        "        if metrics['accuracy'] > best_accuracy:\n",
        "            best_accuracy = metrics['accuracy']\n",
        "            best_model = model\n",
        "            best_class_report = class_report\n",
        "            best_predictions = test_preds\n",
        "            best_labels = test_labels\n",
        "            best_metrics = metrics\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_metrics = pd.DataFrame(all_metrics).mean().to_dict()\n",
        "    std_metrics = pd.DataFrame(all_metrics).std().to_dict()\n",
        "\n",
        "    # Create dataframes for average and best results\n",
        "    avg_df = pd.DataFrame([avg_metrics])\n",
        "    best_df = pd.DataFrame([best_metrics])\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs('/content/drive/MyDrive/HOB/Rerun', exist_ok=True)\n",
        "\n",
        "    # Save average results to CSV\n",
        "  #  avg_df.to_csv('/content/drive/MyDrive/HOB/average_results.csv', index=False)\n",
        "\n",
        "    # Save best results to CSV\n",
        " #   best_df.to_csv('/content/drive/MyDrive/HOB/best_results.csv', index=False)\n",
        "\n",
        "    # Save best predictions in a text file\n",
        " #   np.savetxt('/content/drive/MyDrive/HOB/best_predictions.txt', best_predictions, fmt='%.4f')\n",
        "\n",
        "    # Save best class-level results\n",
        "#    with open('/content/drive/MyDrive/HOB/best_class_results.txt', 'w') as f:\n",
        "  #      f.write(str(best_class_report))\n",
        "\n",
        "    # Print average results\n",
        "    print(\"\\nAverage Metrics:\")\n",
        "    for metric, value in avg_metrics.items():\n",
        "        print(f\"{metric.capitalize()}: {value:.4f} ± {std_metrics[metric]:.4f}\")\n",
        "\n",
        "    return avg_metrics, std_metrics, best_model, avg_df, best_df, best_predictions, best_labels\n",
        "\n",
        "\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "hidden_dim1 = 64\n",
        "hidden_dim2 = 64\n",
        "output_dim = 1\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "avg_metrics, std_metrics, best_model, avg_df, best_df, best_predictions, best_labels = main(\n",
        "    X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor,\n",
        "    input_dim, hidden_dim1, hidden_dim2, output_dim, criterion,\n",
        "    n_runs=10, epochs=100, batch_size=32\n",
        ")\n",
        "\n",
        "# Save the best model\n",
        "torch.save(best_model.state_dict(), '/content/drive/MyDrive/HOB/Rerun/best_model.pth')\n",
        "\n",
        "# Print dataframes\n",
        "print(\"\\nAverage Results Dataframe:\")\n",
        "print(avg_df)\n",
        "print(\"\\nBest Results Dataframe:\")\n",
        "print(best_df)\n",
        "\n",
        "# Create a dataframe with best predictions and true labels\n",
        "best_predictions_df = pd.DataFrame({\n",
        "    'True_Label': best_labels,\n",
        "    'Predicted_Probability': best_predictions,\n",
        "    'Predicted_Class': (best_predictions > 0.5).astype(int)\n",
        "})\n",
        "best_predictions_df.to_csv('/content/drive/MyDrive/HOB/Rerun/class_fm_run10.csv', index=False)\n",
        "avg_df.to_csv('/content/drive/MyDrive/HOB/Rerun/avg.csv', index=False)\n",
        "best_df.to_csv('/content/drive/MyDrive/HOB/Rerun/best.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExATwYtK4t63"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, roc_curve, classification_report\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Usage\n",
        "'''\n",
        "X_train_tensor = torch.tensor(atc_gnn_mol_features_train_df.values[:,:-1], dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(atc_gnn_mol_features_train_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(atc_gnn_mol_features_test_df.values[:,:-1], dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(atc_gnn_mol_features_test_df['Class'].values, dtype=torch.float32).view(-1, 1)'''\n",
        "\n",
        "X_train_tensor = torch.tensor(gnn_mol_features_train_df.values[:,:-1], dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(gnn_mol_features_train_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(gnn_mol_features_test_df.values[:,:-1], dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(gnn_mol_features_test_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "\n",
        "class DenseModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, dropout_rate=0.3):\n",
        "        super(DenseModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def train_dense_model(model, train_loader, criterion, optimizer, epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_preds).flatten(), np.array(all_labels).flatten()\n",
        "\n",
        "def run_experiment(X_train, y_train, X_test, y_test, input_dim, hidden_dim1, hidden_dim2, output_dim, criterion, epochs=100, batch_size=32, sampling_method='smote'):\n",
        "    # Convert tensors to numpy arrays for resampling\n",
        "    X_train_np = X_train.cpu().numpy()\n",
        "    y_train_np = y_train.cpu().numpy().ravel()\n",
        "    X_test_np = X_test.cpu().numpy()\n",
        "    y_test_np = y_test.cpu().numpy().ravel()\n",
        "\n",
        "    # Apply sampling method\n",
        "    if sampling_method == 'ros':\n",
        "        sampler = RandomOverSampler(random_state=42)\n",
        "    elif sampling_method == 'rus':\n",
        "        sampler = RandomUnderSampler(random_state=42)\n",
        "    elif sampling_method == 'smote':\n",
        "        sampler = SMOTE(random_state=42)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid sampling method. Choose 'ros', 'rus', or 'smote'.\")\n",
        "\n",
        "    X_train_resampled, y_train_resampled = sampler.fit_resample(X_train_np, y_train_np)\n",
        "    X_test_resampled, y_test_resampled = X_test_np, y_test_np  # We don't resample the test set\n",
        "\n",
        "    # Convert back to tensors\n",
        "    X_train_tensor = torch.tensor(X_train_resampled, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train_resampled, dtype=torch.float32).view(-1, 1)\n",
        "    X_test_tensor = torch.tensor(X_test_resampled, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test_resampled, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    model = DenseModel(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    train_dense_model(model, train_loader, criterion, optimizer, epochs)\n",
        "\n",
        "    test_preds, test_labels = evaluate_model(model, test_loader)\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(test_labels, test_preds > 0.5),\n",
        "        'balanced_accuracy': balanced_accuracy_score(test_labels, test_preds > 0.5),\n",
        "        'precision': precision_score(test_labels, test_preds > 0.5),\n",
        "        'recall': recall_score(test_labels, test_preds > 0.5),\n",
        "        'f1': f1_score(test_labels, test_preds > 0.5),\n",
        "        'auc': roc_auc_score(test_labels, test_preds),\n",
        "        'kappa': cohen_kappa_score(test_labels, test_preds > 0.5)\n",
        "    }\n",
        "\n",
        "    class_report = classification_report(test_labels, test_preds > 0.5, output_dict=True)\n",
        "\n",
        "    return metrics, class_report, model, test_preds, test_labels\n",
        "\n",
        "def main(X_train, y_train, X_test, y_test, input_dim, hidden_dim1, hidden_dim2, output_dim, criterion, n_runs=10, epochs=100, batch_size=32):\n",
        "    all_metrics = []\n",
        "    best_accuracy = 0\n",
        "    best_model = None\n",
        "    best_class_report = None\n",
        "    best_predictions = None\n",
        "    best_labels = None\n",
        "    best_metrics = None\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"Run {run + 1}/{n_runs}\")\n",
        "        metrics, class_report, model, test_preds, test_labels = run_experiment(X_train, y_train, X_test, y_test, input_dim, hidden_dim1, hidden_dim2, output_dim, criterion, epochs, batch_size, sampling_method = 'smote')\n",
        "        all_metrics.append(metrics)\n",
        "\n",
        "        if metrics['accuracy'] > best_accuracy:\n",
        "            best_accuracy = metrics['accuracy']\n",
        "            best_model = model\n",
        "            best_class_report = class_report\n",
        "            best_predictions = test_preds\n",
        "            best_labels = test_labels\n",
        "            best_metrics = metrics\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_metrics = pd.DataFrame(all_metrics).mean().to_dict()\n",
        "    std_metrics = pd.DataFrame(all_metrics).std().to_dict()\n",
        "\n",
        "    # Create dataframes for average and best results\n",
        "    avg_df = pd.DataFrame([avg_metrics])\n",
        "    best_df = pd.DataFrame([best_metrics])\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs('/content/drive/MyDrive/HOB/', exist_ok=True)\n",
        "\n",
        "    # Save average results to CSV\n",
        "  #  avg_df.to_csv('/content/drive/MyDrive/HOB/average_results.csv', index=False)\n",
        "\n",
        "    # Save best results to CSV\n",
        " #   best_df.to_csv('/content/drive/MyDrive/HOB/best_results.csv', index=False)\n",
        "\n",
        "    # Save best predictions in a text file\n",
        " #   np.savetxt('/content/drive/MyDrive/HOB/best_predictions.txt', best_predictions, fmt='%.4f')\n",
        "\n",
        "    # Save best class-level results\n",
        "#    with open('/content/drive/MyDrive/HOB/best_class_results.txt', 'w') as f:\n",
        "  #      f.write(str(best_class_report))\n",
        "\n",
        "    # Print average results\n",
        "    print(\"\\nAverage Metrics:\")\n",
        "    for metric, value in avg_metrics.items():\n",
        "        print(f\"{metric.capitalize()}: {value:.4f} ± {std_metrics[metric]:.4f}\")\n",
        "\n",
        "    return avg_metrics, std_metrics, best_model, avg_df, best_df, best_predictions, best_labels\n",
        "\n",
        "\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "hidden_dim1 = 64\n",
        "hidden_dim2 = 64\n",
        "output_dim = 1\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "avg_metrics, std_metrics, best_model, avg_df, best_df, best_predictions, best_labels = main(\n",
        "    X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor,\n",
        "    input_dim, hidden_dim1, hidden_dim2, output_dim, criterion,\n",
        "    n_runs=10, epochs=100, batch_size=32\n",
        ")\n",
        "\n",
        "# Save the best model\n",
        "torch.save(best_model.state_dict(), '/content/drive/MyDrive/HOB/Rerun/best_model.pth')\n",
        "\n",
        "# Print dataframes\n",
        "print(\"\\nAverage Results Dataframe:\")\n",
        "print(avg_df)\n",
        "print(\"\\nBest Results Dataframe:\")\n",
        "print(best_df)\n",
        "\n",
        "# Create a dataframe with best predictions and true labels\n",
        "best_predictions_df = pd.DataFrame({\n",
        "    'True_Label': best_labels,\n",
        "    'Predicted_Probability': best_predictions,\n",
        "    'Predicted_Class': (best_predictions > 0.5).astype(int)\n",
        "})\n",
        "best_predictions_df.to_csv('/content/drive/MyDrive/HOB/Rerun/class_smotefm_run10.csv', index=False)\n",
        "avg_df.to_csv('/content/drive/MyDrive/HOB/Rerun/avg.csv', index=False)\n",
        "best_df.to_csv('/content/drive/MyDrive/HOB/Rerun/best.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}