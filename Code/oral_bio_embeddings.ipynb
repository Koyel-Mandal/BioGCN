{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS8yDg1zc9Za",
        "outputId": "87c1ad0f-52e4-41c8-b343-d1de0603d1c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.3.6-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (10.4.0)\n",
            "Downloading rdkit-2024.3.6-cp310-cp310-manylinux_2_28_x86_64.whl (32.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.3.6\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.46.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.6)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (3.1.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
            "Collecting Catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from Catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from Catboost) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from Catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from Catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from Catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from Catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from Catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->Catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->Catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->Catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Catboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Catboost) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Catboost) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->Catboost) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->Catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Catboost\n",
            "Successfully installed Catboost-1.2.7\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->torch_geometric) (0.2.0)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit\n",
        "!pip install shap\n",
        "!pip install Catboost\n",
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0FSv13Tee2d",
        "outputId": "be84b836-0639-4ccd-a767-0443647b6be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOQQ2IN9d0Bt"
      },
      "outputs": [],
      "source": [
        "# import packages and modules\n",
        "\n",
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import Draw, Descriptors, AllChem\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from itertools import combinations\n",
        "import IPython\n",
        "from IPython.display import display, Image as IPImage\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from rdkit import Chem\n",
        "from rdkit.ML.Descriptors.MoleculeDescriptors import MolecularDescriptorCalculator\n",
        "import shap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy import stats\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, cohen_kappa_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "import pickle\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from rdkit.Chem import rdmolops\n",
        "from scipy.stats import pearsonr\n",
        "from rdkit.Chem import rdchem\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data, DataLoader, Batch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOU58Vo4dLA1",
        "outputId": "0da7be57-97ae-4cb4-9e41-16b975ee5000"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(697, 43)\n",
            "(45, 43)\n"
          ]
        }
      ],
      "source": [
        "# Read Input files\n",
        "\n",
        "train_df = pd.read_excel('/content/drive/MyDrive/HOB/input_gnn_20.xlsx', sheet_name = \"train\")\n",
        "print(train_df.shape)\n",
        "\n",
        "test_df = pd.read_excel('/content/drive/MyDrive/HOB/input_gnn_20.xlsx', sheet_name = \"test\")\n",
        "print(test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Izhz-N6dfWPf"
      },
      "outputs": [],
      "source": [
        "# ATC code frequency matrix\n",
        "atc_train = pd.read_excel('/content/drive/MyDrive/HOB/drug_atc.xlsx', sheet_name='train_final')\n",
        "atc_test = pd.read_excel('/content/drive/MyDrive/HOB/drug_atc.xlsx', sheet_name='test')\n",
        "atc_train_filled = atc_train[['Drug Name', 'ATC_Codes']].fillna('0').apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
        "atc_test_filled = atc_test[['Drug Name', 'ATC_Codes']].fillna('0').apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
        "atc_codes = pd.concat([atc_train_filled, atc_test_filled], axis=0).reset_index(drop=True)\n",
        "\n",
        "# Function to split ATC codes\n",
        "def split_atc_codes(atc_codes):\n",
        "    if atc_codes == '0':\n",
        "        return ['0']\n",
        "    atc_codes = atc_codes.strip()\n",
        "    parts = []\n",
        "    for code in atc_codes.split(', '):\n",
        "        code = code.strip()\n",
        "        if len(code) >= 1:\n",
        "            parts.append(code[0])  # Single char\n",
        "        if len(code) >= 3:\n",
        "            parts.append(code[:3])  # Single char and two digits\n",
        "        if len(code) >= 4:\n",
        "            parts.append(code[:4])  # Single char, two digits, single char\n",
        "        if len(code) >= 5:\n",
        "            parts.append(code[:5])  # Single char, two digits, two chars\n",
        "        parts.append(code)  # Entire string\n",
        "    return parts\n",
        "\n",
        "# Apply split function and get unique ATC code parts\n",
        "all_atc_parts = atc_codes['ATC_Codes'].apply(split_atc_codes)\n",
        "unique_atc_parts = set(part for sublist in all_atc_parts for part in sublist)\n",
        "unique_atc_parts = sorted(unique_atc_parts)\n",
        "\n",
        "unique_atc_parts = [code for code in unique_atc_parts if code != '0']\n",
        "\n",
        "atc_matrix_train = pd.DataFrame(0, index = atc_train['Drug Name'], columns = unique_atc_parts)\n",
        "atc_matrix_test = pd.DataFrame(0, index = atc_test['Drug Name'], columns = unique_atc_parts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVVqhxgsfo0O",
        "outputId": "b8ff0998-9c83-4531-8663-0030c3d4c68b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(698, 2088)\n"
          ]
        }
      ],
      "source": [
        "atc_matrix_train = pd.DataFrame(0, index=atc_train['Drug Name'], columns=unique_atc_parts)\n",
        "\n",
        "# Assuming atc_train_filled is a DataFrame or converting if it's not\n",
        "if not isinstance(atc_train_filled, pd.DataFrame):\n",
        "    atc_train_filled = pd.DataFrame(atc_train_filled)\n",
        "\n",
        "# Now iterate over rows in the DataFrame\n",
        "for i, row in atc_train_filled.iterrows():\n",
        "    atc_codes = split_atc_codes(row['ATC_Codes'])\n",
        "   # print(atc_codes)\n",
        "    for code in atc_codes:\n",
        "        if code in unique_atc_parts:\n",
        "            atc_matrix_train.at[row['Drug Name'], code] += 1\n",
        "\n",
        "# Fill NaN or missing values with 0\n",
        "atc_matrix_train.fillna(0, inplace=True)\n",
        "\n",
        "# Display the resulting matrix\n",
        "#print(atc_matrix_train)\n",
        "print(atc_matrix_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6Y-rkqcj3AH"
      },
      "outputs": [],
      "source": [
        "# Index of the row to delete\n",
        "index_to_delete = 'Selenium'\n",
        "\n",
        "# Drop the row with the specified index\n",
        "atc_matrix_train = atc_matrix_train.drop(index_to_delete)\n",
        "\n",
        "# Reset index if needed\n",
        "#drug_embeddings_df.reset_index(drop=True, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40fvz4UPfvFH",
        "outputId": "84bcde65-82fa-4ca2-9bb5-e84c4712c40b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(45, 2088)\n"
          ]
        }
      ],
      "source": [
        "atc_matrix_test = pd.DataFrame(0, index=atc_test['Drug Name'], columns=unique_atc_parts)\n",
        "\n",
        "# Assuming atc_test_filled is a DataFrame or converting if it's not\n",
        "if not isinstance(atc_test_filled, pd.DataFrame):\n",
        "    atc_test_filled = pd.DataFrame(atc_test_filled)\n",
        "\n",
        "# Now iterate over rows in the DataFrame\n",
        "for i, row in atc_test_filled.iterrows():\n",
        "    atc_codes = split_atc_codes(row['ATC_Codes'])\n",
        "    #print(atc_codes)\n",
        "    for code in atc_codes:\n",
        "        if code in unique_atc_parts:\n",
        "            atc_matrix_test.at[row['Drug Name'], code] += 1\n",
        "\n",
        "# Fill NaN or missing values with 0\n",
        "atc_matrix_test.fillna(0, inplace=True)\n",
        "\n",
        "# Display the resulting matrix\n",
        "#print(atc_matrix_test)\n",
        "print(atc_matrix_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYURbnP91fG6"
      },
      "outputs": [],
      "source": [
        "def preprocess_classification_atc(X):\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "    scaler = MinMaxScaler()\n",
        "    #scaler = RobustScaler()\n",
        "    X_scaled = scaler.fit_transform(X_imputed)\n",
        "    return X_scaled\n",
        "\n",
        "train_atc_norm = preprocess_classification_atc(atc_matrix_train.iloc[:, 0:])\n",
        "test_atc_norm = preprocess_classification_atc(atc_matrix_test.iloc[:, 0:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxFxH7tUt7Kw"
      },
      "outputs": [],
      "source": [
        "cols = [f'atc_{i}' for i in range(atc_matrix_train.shape[1])]\n",
        "train_atc_norm_df = pd.DataFrame(train_atc_norm, columns=cols)\n",
        "#print(train_atc_norm_df.head())\n",
        "\n",
        "\n",
        "cols = [f'atc_{i}' for i in range(atc_matrix_test.shape[1])]\n",
        "test_atc_norm_df = pd.DataFrame(test_atc_norm, columns=cols)\n",
        "#print(test_atc_norm_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKJiWEx4DNFl"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdchem\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data, Batch\n",
        "\n",
        "# Node feature encoding for hybridization\n",
        "def encode_hybridization(hybridization):\n",
        "    hybridization_map = {\n",
        "        rdchem.HybridizationType.SP: 0,\n",
        "        rdchem.HybridizationType.SP2: 1,\n",
        "        rdchem.HybridizationType.SP3: 2,\n",
        "        rdchem.HybridizationType.SP3D: 3,\n",
        "        rdchem.HybridizationType.SP3D2: 4,\n",
        "        rdchem.HybridizationType.UNSPECIFIED: 5,\n",
        "    }\n",
        "    return hybridization_map.get(hybridization, 5)\n",
        "\n",
        "# Convert SMILES to graph data object\n",
        "def smiles_to_graph(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        print(f\"Failed to parse SMILES: {smiles}\")\n",
        "        return None\n",
        "\n",
        "    # Handle single atom or molecule with no bonds\n",
        "    if mol.GetNumBonds() == 0:\n",
        "        print(f\"SMILES has no bonds: {smiles}\")\n",
        "        return None\n",
        "\n",
        "    # Node features\n",
        "    node_features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        feature = [\n",
        "            atom.GetAtomicNum(),\n",
        "            atom.GetDegree(),\n",
        "            atom.GetTotalNumHs(),\n",
        "            atom.GetImplicitValence(),\n",
        "            int(atom.GetIsAromatic()),\n",
        "            encode_hybridization(atom.GetHybridization())\n",
        "        ]\n",
        "        node_features.append(feature)\n",
        "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "    # Edge index and edge features\n",
        "    edge_index = []\n",
        "    edge_features = []\n",
        "    for bond in mol.GetBonds():\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "        edge_index.append([i, j])\n",
        "        edge_index.append([j, i])\n",
        "        edge_features.append([\n",
        "            bond.GetBondTypeAsDouble(),\n",
        "            int(bond.GetIsConjugated()),\n",
        "            int(bond.IsInRing())\n",
        "        ])\n",
        "        edge_features.append([\n",
        "            bond.GetBondTypeAsDouble(),\n",
        "            int(bond.GetIsConjugated()),\n",
        "            int(bond.IsInRing())\n",
        "        ])\n",
        "\n",
        "    if len(edge_index) == 0:\n",
        "        print(f\"SMILES has no bonds: {smiles}\")\n",
        "        return None\n",
        "\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
        "\n",
        "    # Create a PyTorch Geometric Data object\n",
        "    data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_features)\n",
        "    return data\n",
        "\n",
        "# Apply function to convert SMILES to graph and filter out invalid ones\n",
        "#train_df = pd.DataFrame({'Structure (SMILES)': ['CCO', 'O=C=O', 'CCC']})  # Dummy DataFrame\n",
        "train_df['Graph'] = train_df['Structure (SMILES)'].apply(smiles_to_graph)\n",
        "data_list = [graph for graph in train_df['Graph'] if graph is not None]  # Filter out None values\n",
        "\n",
        "\n",
        "\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, edge_dim):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.edge_mlp = nn.Linear(edge_dim, hidden_dim)\n",
        "        self.pool = global_mean_pool\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        # Initialize with a small positive bias\n",
        "        self.conv1.bias = nn.Parameter(torch.ones(hidden_dim) * 0.01)\n",
        "        self.conv2.bias = nn.Parameter(torch.ones(hidden_dim) * 0.01)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "\n",
        "        # Normalize input features\n",
        "        x = (x - x.mean(dim=0)) / (x.std(dim=0) + 1e-5)\n",
        "\n",
        "        edge_features = self.edge_mlp(edge_attr)\n",
        "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
        "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
        "        x = self.pool(x, data.batch)\n",
        "        return self.fc(x).squeeze()\n",
        "\n",
        "def train_gnn(model, data_list, targets, epochs=100, lr=0.001):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for data, target in zip(data_list, targets):\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            loss = criterion(out, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(data_list)}')\n",
        "\n",
        "# Recreate the model and train\n",
        "gnn_model = GNN(input_dim=6, hidden_dim=32, output_dim=1, edge_dim=3)\n",
        "targets = torch.randn(len(data_list))  # Replace with actual targets\n",
        "train_gnn(gnn_model, data_list, targets, epochs=100, lr=0.001)\n",
        "\n",
        "# Generate embeddings\n",
        "def generate_embeddings(model, data_list):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for data in data_list:\n",
        "            x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "            x = (x - x.mean(dim=0)) / (x.std(dim=0) + 1e-5)\n",
        "            x = F.leaky_relu(model.conv1(x, edge_index))\n",
        "            x = F.leaky_relu(model.conv2(x, edge_index))\n",
        "            embedding = model.pool(x, data.batch)\n",
        "            embeddings.append(embedding.cpu().numpy())\n",
        "    return embeddings\n",
        "\n",
        "embeddings = generate_embeddings(gnn_model, data_list)\n",
        "\n",
        "# Print statistics about the embeddings\n",
        "import numpy as np\n",
        "embeddings_array = np.array(embeddings)\n",
        "print(f\"Embeddings shape: {embeddings_array.shape}\")\n",
        "print(f\"Mean: {np.mean(embeddings_array)}\")\n",
        "print(f\"Std: {np.std(embeddings_array)}\")\n",
        "print(f\"Min: {np.min(embeddings_array)}\")\n",
        "print(f\"Max: {np.max(embeddings_array)}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvgTjwfkk6JX"
      },
      "outputs": [],
      "source": [
        "### Graph features\n",
        "def one_hot_encoding(x, permitted_list):\n",
        "    if x not in permitted_list:\n",
        "        x = permitted_list[-1]\n",
        "    return [int(x == s) for s in permitted_list]\n",
        "\n",
        "def get_atom_features(atom, use_chirality=True, hydrogens_implicit=True):\n",
        "    permitted_list_of_atoms = ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr', 'Pt', 'Hg', 'Pb', 'Unknown']\n",
        "    if not hydrogens_implicit:\n",
        "        permitted_list_of_atoms = ['H'] + permitted_list_of_atoms\n",
        "\n",
        "    atom_type_enc = one_hot_encoding(str(atom.GetSymbol()), permitted_list_of_atoms)\n",
        "    n_heavy_neighbors_enc = one_hot_encoding(int(atom.GetDegree()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
        "    formal_charge_enc = one_hot_encoding(int(atom.GetFormalCharge()), [-3, -2, -1, 0, 1, 2, 3, \"Extreme\"])\n",
        "    hybridisation_type_enc = one_hot_encoding(str(atom.GetHybridization()), [\"S\", \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\", \"OTHER\"])\n",
        "    is_in_a_ring_enc = [int(atom.IsInRing())]\n",
        "    is_aromatic_enc = [int(atom.GetIsAromatic())]\n",
        "    atomic_mass_scaled = [(atom.GetMass() - 10.812) / 116.092]\n",
        "    vdw_radius_scaled = [(Chem.GetPeriodicTable().GetRvdw(atom.GetAtomicNum()) - 1.5) / 0.6]\n",
        "    covalent_radius_scaled = [(Chem.GetPeriodicTable().GetRcovalent(atom.GetAtomicNum()) - 0.64) / 0.76]\n",
        "\n",
        "    atom_feature_vector = atom_type_enc + n_heavy_neighbors_enc + formal_charge_enc + hybridisation_type_enc + \\\n",
        "                          is_in_a_ring_enc + is_aromatic_enc + atomic_mass_scaled + vdw_radius_scaled + \\\n",
        "                          covalent_radius_scaled\n",
        "\n",
        "    if use_chirality:\n",
        "        chirality_type_enc = one_hot_encoding(str(atom.GetChiralTag()), [\"CHI_UNSPECIFIED\", \"CHI_TETRAHEDRAL_CW\", \"CHI_TETRAHEDRAL_CCW\", \"CHI_OTHER\"])\n",
        "        atom_feature_vector += chirality_type_enc\n",
        "\n",
        "    if hydrogens_implicit:\n",
        "        n_hydrogens_enc = one_hot_encoding(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
        "        atom_feature_vector += n_hydrogens_enc\n",
        "\n",
        "    return np.array(atom_feature_vector)\n",
        "\n",
        "def get_bond_features(bond, use_stereochemistry=True):\n",
        "    permitted_list_of_bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE, Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
        "    bond_type_enc = one_hot_encoding(bond.GetBondType(), permitted_list_of_bond_types)\n",
        "    bond_is_conj_enc = [int(bond.GetIsConjugated())]\n",
        "    bond_is_in_ring_enc = [int(bond.IsInRing())]\n",
        "    bond_feature_vector = bond_type_enc + bond_is_conj_enc + bond_is_in_ring_enc\n",
        "\n",
        "    if use_stereochemistry:\n",
        "        stereo_type_enc = one_hot_encoding(str(bond.GetStereo()), [\"STEREOZ\", \"STEREOE\", \"STEREOANY\", \"STEREONONE\"])\n",
        "        bond_feature_vector += stereo_type_enc\n",
        "\n",
        "    return np.array(bond_feature_vector)\n",
        "\n",
        "def smiles_to_graph(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        print(f\"Failed to parse SMILES: {smiles}\")\n",
        "        return None\n",
        "\n",
        "    node_features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        node_features.append(get_atom_features(atom))\n",
        "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "    edge_indices = []\n",
        "    edge_features = []\n",
        "    for bond in mol.GetBonds():\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "        edge_indices.extend([[i, j], [j, i]])\n",
        "        edge_features.extend([get_bond_features(bond)] * 2)\n",
        "\n",
        "    if len(edge_indices) == 0:\n",
        "        print(f\"SMILES has no bonds: {smiles}\")\n",
        "        return None\n",
        "\n",
        "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
        "    edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
        "\n",
        "    data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_features)\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CcPZvQouk_Xo",
        "outputId": "c2642072-72c4-42c0-bce4-fd0d9887c8f0"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3d2f238bb796>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#GCN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGCN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "#GCN\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, edge_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.edge_mlp = nn.Linear(edge_dim, hidden_dim)\n",
        "        self.pool = global_mean_pool\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "        x = (x - x.mean(dim=0)) / (x.std(dim=0) + 1e-5)\n",
        "        edge_features = self.edge_mlp(edge_attr)\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        return self.pool(x, data.batch)\n",
        "\n",
        "def generate_embeddings(model, data_list):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for data in data_list:\n",
        "            embedding = model(data)\n",
        "            embeddings.append(embedding.cpu().numpy())\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk98LnbVwjR8",
        "outputId": "fa4ac710-d9b1-4a32-f7a9-8d5c5974fbd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings shape: (697, 1, 32)\n",
            "Mean: 0.07732225209474564\n",
            "Std: 0.07413024455308914\n",
            "Min: 0.0\n",
            "Max: 0.4506397247314453\n"
          ]
        }
      ],
      "source": [
        "# Embeddings for training data\n",
        "\n",
        "train_df['Graph'] = train_df['Structure (SMILES)'].apply(smiles_to_graph)\n",
        "data_list = [graph for graph in train_df['Graph'] if graph is not None]\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = data_list[0].x.shape[1]\n",
        "edge_dim = data_list[0].edge_attr.shape[1]\n",
        "gcn_model = GCN(input_dim=input_dim, hidden_dim=32, edge_dim=edge_dim)\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = generate_embeddings(gcn_model, data_list)\n",
        "\n",
        "# Print statistics about the embeddings\n",
        "embeddings_array = np.array(embeddings)\n",
        "print(f\"Embeddings shape: {embeddings_array.shape}\")\n",
        "print(f\"Mean: {np.mean(embeddings_array)}\")\n",
        "print(f\"Std: {np.std(embeddings_array)}\")\n",
        "print(f\"Min: {np.min(embeddings_array)}\")\n",
        "print(f\"Max: {np.max(embeddings_array)}\")\n",
        "\n",
        "# Save embeddings\n",
        "#np.save('molecular_embeddings.npy', embeddings_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "hqZ2dp1ywPqV",
        "outputId": "9cde5216-c006-41a2-ee17-34b882627f85"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>692</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>693</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>694</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>696</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>697 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "0      1\n",
              "1      1\n",
              "2      1\n",
              "3      1\n",
              "4      1\n",
              "      ..\n",
              "692    0\n",
              "693    0\n",
              "694    0\n",
              "695    0\n",
              "696    0\n",
              "Name: Class, Length: 697, dtype: int64"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = train_df['Class']\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "UEqr1wdotWEf",
        "outputId": "932a8437-c09b-478d-ae4e-c008d41013b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "cross_entropy_loss(): argument 'target' (position 2) must be Tensor, not NoneType",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1879b74c0906>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reset gradients from the previous step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forward pass through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m         return F.cross_entropy(\n\u001b[0m\u001b[1;32m   1294\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3480\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'target' (position 2) must be Tensor, not NoneType"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim  # Add this import for the optimizer\n",
        "\n",
        "# Assuming the necessary imports and functions are defined earlier (like smiles_to_graph)\n",
        "\n",
        "# Convert SMILES to graph data\n",
        "train_df['Graph'] = train_df['Structure (SMILES)'].apply(smiles_to_graph)\n",
        "\n",
        "# Filter out invalid graphs (if any)\n",
        "data_list = [graph for graph in train_df['Graph'] if graph is not None]\n",
        "\n",
        "# Initialize the GCN model\n",
        "input_dim = data_list[0].x.shape[1]  # Input feature dimension\n",
        "edge_dim = data_list[0].edge_attr.shape[1]  # Edge feature dimension\n",
        "gcn_model = GCN(input_dim=input_dim, hidden_dim=32, edge_dim=edge_dim)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(gcn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Loss function (assuming classification task)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# DataLoader to batch the graphs\n",
        "train_loader = DataLoader(data_list, batch_size=32, shuffle=True)\n",
        "y = train_df['Class']\n",
        "# Training loop\n",
        "gcn_model.train()  # Set the model in training mode (enables dropout if used)\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()  # Reset gradients from the previous step\n",
        "        out = gcn_model(data)  # Forward pass through the model\n",
        "        loss = criterion(out, data.y)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update model parameters\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYzPPpZhbUYp",
        "outputId": "05266bb3-e26d-4bdf-f8d0-2dfc485a994f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node (Atom) Features:\n",
            "Shape: torch.Size([22, 79])\n",
            "First node feature vector:\n",
            "tensor([1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0103, 0.3333, 0.1579, 1.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000])\n",
            "\n",
            "Edge (Bond) Features:\n",
            "Shape: torch.Size([48, 10])\n",
            "First edge feature vector:\n",
            "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
            "\n",
            "Atom Feature Meanings:\n",
            "0: Atom type (one-hot encoded)\n",
            "1: Number of heavy neighbors (one-hot encoded)\n",
            "2: Formal charge (one-hot encoded)\n",
            "3: Hybridization type (one-hot encoded)\n",
            "4: Is in ring\n",
            "5: Is aromatic\n",
            "6: Atomic mass (scaled)\n",
            "7: Van der Waals radius (scaled)\n",
            "8: Covalent radius (scaled)\n",
            "9: Chirality (one-hot encoded)\n",
            "10: Number of hydrogens (one-hot encoded)\n",
            "\n",
            "Bond Feature Meanings:\n",
            "0: Bond type (one-hot encoded)\n",
            "1: Is conjugated\n",
            "2: Is in ring\n",
            "3: Stereochemistry (one-hot encoded)\n"
          ]
        }
      ],
      "source": [
        "# Assuming data_list is already created and contains valid graph data\n",
        "\n",
        "# Check if data_list is not empty\n",
        "if data_list:\n",
        "    # Get the first graph from the list\n",
        "    first_graph = data_list[2]\n",
        "\n",
        "    # Extract node features (atom features)\n",
        "    node_features = first_graph.x\n",
        "\n",
        "    # Extract edge features (bond features)\n",
        "    edge_features = first_graph.edge_attr\n",
        "\n",
        "    print(\"Node (Atom) Features:\")\n",
        "    print(f\"Shape: {node_features.shape}\")\n",
        "    print(\"First node feature vector:\")\n",
        "    print(node_features[0])\n",
        "\n",
        "    print(\"\\nEdge (Bond) Features:\")\n",
        "    print(f\"Shape: {edge_features.shape}\")\n",
        "    print(\"First edge feature vector:\")\n",
        "    print(edge_features[0])\n",
        "\n",
        "    # Optionally, you can print the meanings of each feature\n",
        "    atom_feature_names = [\n",
        "        \"Atom type (one-hot encoded)\",\n",
        "        \"Number of heavy neighbors (one-hot encoded)\",\n",
        "        \"Formal charge (one-hot encoded)\",\n",
        "        \"Hybridization type (one-hot encoded)\",\n",
        "        \"Is in ring\",\n",
        "        \"Is aromatic\",\n",
        "        \"Atomic mass (scaled)\",\n",
        "        \"Van der Waals radius (scaled)\",\n",
        "        \"Covalent radius (scaled)\",\n",
        "        \"Chirality (one-hot encoded)\",\n",
        "        \"Number of hydrogens (one-hot encoded)\"\n",
        "    ]\n",
        "\n",
        "    bond_feature_names = [\n",
        "        \"Bond type (one-hot encoded)\",\n",
        "        \"Is conjugated\",\n",
        "        \"Is in ring\",\n",
        "        \"Stereochemistry (one-hot encoded)\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nAtom Feature Meanings:\")\n",
        "    for i, name in enumerate(atom_feature_names):\n",
        "        print(f\"{i}: {name}\")\n",
        "\n",
        "    print(\"\\nBond Feature Meanings:\")\n",
        "    for i, name in enumerate(bond_feature_names):\n",
        "        print(f\"{i}: {name}\")\n",
        "else:\n",
        "    print(\"data_list is empty. Please ensure that valid graph data has been generated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wSZp5sDw4_z"
      },
      "outputs": [],
      "source": [
        "cols = [f'emb_{i}' for i in range(embeddings_array.shape[2])]\n",
        "embeddings_array_2d = embeddings_array.reshape(embeddings_array.shape[0], embeddings_array.shape[2])\n",
        "embeddings_array_df = pd.DataFrame(embeddings_array_2d, columns=cols)\n",
        "#print(embeddings_array_df.head())\n",
        "\n",
        "train_df_cp = train_df.copy()\n",
        "train_df_cp = train_df_cp.drop('Structure (SMILES)', axis=1)\n",
        "train_df_cp = train_df_cp.drop('Graph', axis=1)\n",
        "train_df_cp = train_df_cp.drop('Drug Name', axis=1)\n",
        "\n",
        "def preprocess_classification(X):\n",
        "    X.replace([np.inf, -np.inf], np.finfo(np.float64).max, inplace=True)\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "    scaler = MinMaxScaler()\n",
        "    #scaler = RobustScaler()\n",
        "    X_scaled = scaler.fit_transform(X_imputed)\n",
        "    return X_scaled\n",
        "\n",
        "y = train_df_cp['Class']\n",
        "y_df = pd.DataFrame(y, columns=['Class'])  # Name your target column as 'Target_Class' (or any name you prefer)\n",
        "\n",
        "original_features = preprocess_classification(train_df_cp.iloc[:, 0:5])\n",
        "original_features_df = pd.DataFrame(original_features, columns = ['Drug pKa',\t'Log P',\t'Half Life (hrs-1)',\t'No of Rotatable Bonds',\t'M.Wt'])\n",
        "fea_emb = pd.concat([original_features_df, embeddings_array_df], axis=1)\n",
        "train_fea_emb_df = pd.concat([fea_emb, y_df], axis =1 ) # original_features + embeddings\n",
        "\n",
        "features_mol = preprocess_classification(train_df_cp.iloc[:,0:-1])\n",
        "features_mol_df = pd.DataFrame(features_mol, columns = train_df_cp.columns[0:-1])\n",
        "fea_emb_df = pd.concat([features_mol_df, embeddings_array_df], axis=1)\n",
        "gnn_mol_features_train_df = pd.concat([fea_emb_df, y_df], axis =1 ) #features + molecular descriptors + embeddings\n",
        "\n",
        "atc_gnn_mol_features_train = pd.concat([fea_emb_df, train_atc_norm_df], axis =1 )\n",
        "atc_gnn_mol_features_train_df = pd.concat([atc_gnn_mol_features_train, y_df], axis =1 )  #features + molecular descriptors + embeddings +atc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqQRAKhnxM3q",
        "outputId": "061e3a70-1ac3-46ae-ef49-876a7992bc84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings shape: (45, 1, 32)\n",
            "Mean: 0.08334869891405106\n",
            "Std: 0.07598452270030975\n",
            "Min: 0.0\n",
            "Max: 0.45660170912742615\n"
          ]
        }
      ],
      "source": [
        "# Assuming you've already trained your model on the training set\n",
        "\n",
        "# Load your test data\n",
        "test_df['Graph'] = test_df['Structure (SMILES)'].apply(smiles_to_graph)\n",
        "test_data_list = [graph for graph in test_df['Graph'] if graph is not None]\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = test_data_list[0].x.shape[1]\n",
        "edge_dim = test_data_list[0].edge_attr.shape[1]\n",
        "gnn_model = GCN(input_dim=input_dim, hidden_dim=32, edge_dim=edge_dim)\n",
        "\n",
        "# Generate embeddings\n",
        "test_embeddings = generate_embeddings(gnn_model, test_data_list)\n",
        "\n",
        "# Print statistics about the embeddings\n",
        "test_embeddings_array = np.array(test_embeddings)\n",
        "print(f\"Embeddings shape: {test_embeddings_array.shape}\")\n",
        "print(f\"Mean: {np.mean(test_embeddings_array)}\")\n",
        "print(f\"Std: {np.std(test_embeddings_array)}\")\n",
        "print(f\"Min: {np.min(test_embeddings_array)}\")\n",
        "print(f\"Max: {np.max(test_embeddings_array)}\")\n",
        "\n",
        "# Save embeddings\n",
        "#np.save('molecular_embeddings.npy', embeddings_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGOsbPjrxfh0"
      },
      "outputs": [],
      "source": [
        "cols = [f'emb_{i}' for i in range(test_embeddings_array.shape[2])]\n",
        "embeddings_array_2d = test_embeddings_array.reshape(test_embeddings_array.shape[0], test_embeddings_array.shape[2])\n",
        "embeddings_array_df = pd.DataFrame(embeddings_array_2d, columns=cols)\n",
        "#print(embeddings_array_df.head())\n",
        "\n",
        "\n",
        "def preprocess_classification(X):\n",
        "    X.replace([np.inf, -np.inf], np.finfo(np.float64).max, inplace=True)\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "    scaler = MinMaxScaler()\n",
        "    #scaler = RobustScaler()\n",
        "    X_scaled = scaler.fit_transform(X_imputed)\n",
        "    return X_scaled\n",
        "\n",
        "test_df_cp = test_df.copy()\n",
        "test_df_cp = test_df_cp.drop('Structure (SMILES)', axis=1)\n",
        "test_df_cp = test_df_cp.drop('Graph', axis=1)\n",
        "test_df_cp = test_df_cp.drop('Drug Name', axis=1)\n",
        "\n",
        "y = test_df_cp['Class']\n",
        "y_df = pd.DataFrame(y, columns=['Class'])  # Name your target column as 'Target_Class' (or any name you prefer)\n",
        "\n",
        "original_features = preprocess_classification(test_df_cp.iloc[:, 0:5])\n",
        "original_features_df = pd.DataFrame(original_features, columns = ['Drug pKa',\t'Log P',\t'Half Life (hrs-1)',\t'No of Rotatable Bonds',\t'M.Wt'])\n",
        "fea_emb = pd.concat([original_features_df, embeddings_array_df], axis=1)\n",
        "test_fea_emb_df = pd.concat([fea_emb, y_df], axis =1 ) # original_features + embeddings\n",
        "\n",
        "test_features_mol = preprocess_classification(test_df_cp.iloc[:,0:-1])\n",
        "test_features_mol_df = pd.DataFrame(test_features_mol, columns = test_df_cp.columns[0:-1])\n",
        "\n",
        "test_combined_df = pd.concat([test_features_mol_df, embeddings_array_df], axis=1)\n",
        "gnn_mol_features_test_df = pd.concat([test_combined_df, y_df], axis =1 ) # original_features + embeddings + descriptors\n",
        "\n",
        "atc_gnn_mol_features_test = pd.concat([test_combined_df, test_atc_norm_df], axis = 1)\n",
        "atc_gnn_mol_features_test_df = pd.concat([atc_gnn_mol_features_test, y_df], axis =1 ) #original_features + embeddings + descriptors + atc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRpGNBISpliq"
      },
      "outputs": [],
      "source": [
        "##Graph Sage\n",
        "\n",
        "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, Batch\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdchem\n",
        "import torch.nn as nn\n",
        "\n",
        "def one_hot_encoding(x, permitted_list):\n",
        "    if x not in permitted_list:\n",
        "        x = permitted_list[-1]\n",
        "    return [int(x == s) for s in permitted_list]\n",
        "\n",
        "def get_atom_features(atom, use_chirality=True, hydrogens_implicit=True):\n",
        "    permitted_list_of_atoms = ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr', 'Pt', 'Hg', 'Pb', 'Unknown']\n",
        "    if not hydrogens_implicit:\n",
        "        permitted_list_of_atoms = ['H'] + permitted_list_of_atoms\n",
        "\n",
        "    atom_type_enc = one_hot_encoding(str(atom.GetSymbol()), permitted_list_of_atoms)\n",
        "    n_heavy_neighbors_enc = one_hot_encoding(int(atom.GetDegree()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
        "    formal_charge_enc = one_hot_encoding(int(atom.GetFormalCharge()), [-3, -2, -1, 0, 1, 2, 3, \"Extreme\"])\n",
        "    hybridisation_type_enc = one_hot_encoding(str(atom.GetHybridization()), [\"S\", \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\", \"OTHER\"])\n",
        "    is_in_a_ring_enc = [int(atom.IsInRing())]\n",
        "    is_aromatic_enc = [int(atom.GetIsAromatic())]\n",
        "    atomic_mass_scaled = [(atom.GetMass() - 10.812) / 116.092]\n",
        "    vdw_radius_scaled = [(Chem.GetPeriodicTable().GetRvdw(atom.GetAtomicNum()) - 1.5) / 0.6]\n",
        "    covalent_radius_scaled = [(Chem.GetPeriodicTable().GetRcovalent(atom.GetAtomicNum()) - 0.64) / 0.76]\n",
        "\n",
        "    atom_feature_vector = atom_type_enc + n_heavy_neighbors_enc + formal_charge_enc + hybridisation_type_enc + \\\n",
        "                          is_in_a_ring_enc + is_aromatic_enc + atomic_mass_scaled + vdw_radius_scaled + \\\n",
        "                          covalent_radius_scaled\n",
        "\n",
        "    if use_chirality:\n",
        "        chirality_type_enc = one_hot_encoding(str(atom.GetChiralTag()), [\"CHI_UNSPECIFIED\", \"CHI_TETRAHEDRAL_CW\", \"CHI_TETRAHEDRAL_CCW\", \"CHI_OTHER\"])\n",
        "        atom_feature_vector += chirality_type_enc\n",
        "\n",
        "    if hydrogens_implicit:\n",
        "        n_hydrogens_enc = one_hot_encoding(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
        "        atom_feature_vector += n_hydrogens_enc\n",
        "\n",
        "    return np.array(atom_feature_vector)\n",
        "\n",
        "def get_bond_features(bond, use_stereochemistry=True):\n",
        "    permitted_list_of_bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE, Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
        "    bond_type_enc = one_hot_encoding(bond.GetBondType(), permitted_list_of_bond_types)\n",
        "    bond_is_conj_enc = [int(bond.GetIsConjugated())]\n",
        "    bond_is_in_ring_enc = [int(bond.IsInRing())]\n",
        "    bond_feature_vector = bond_type_enc + bond_is_conj_enc + bond_is_in_ring_enc\n",
        "\n",
        "    if use_stereochemistry:\n",
        "        stereo_type_enc = one_hot_encoding(str(bond.GetStereo()), [\"STEREOZ\", \"STEREOE\", \"STEREOANY\", \"STEREONONE\"])\n",
        "        bond_feature_vector += stereo_type_enc\n",
        "\n",
        "    return np.array(bond_feature_vector)\n",
        "\n",
        "def smiles_to_graph(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        print(f\"Failed to parse SMILES: {smiles}\")\n",
        "        return None\n",
        "\n",
        "    node_features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        node_features.append(get_atom_features(atom))\n",
        "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "    edge_indices = []\n",
        "    edge_features = []\n",
        "    for bond in mol.GetBonds():\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "        edge_indices.extend([[i, j], [j, i]])\n",
        "        edge_features.extend([get_bond_features(bond)] * 2)\n",
        "\n",
        "    if len(edge_indices) == 0:\n",
        "        print(f\"SMILES has no bonds: {smiles}\")\n",
        "        return None\n",
        "\n",
        "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
        "    edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
        "\n",
        "    data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_features)\n",
        "    return data\n",
        "\n",
        "\n",
        "class GraphSAGE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, edge_dim):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
        "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
        "        self.edge_mlp = nn.Linear(edge_dim, hidden_dim)\n",
        "        self.pool = global_mean_pool\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "\n",
        "        # Normalize input features\n",
        "        x = (x - x.mean(dim=0)) / (x.std(dim=0) + 1e-5)\n",
        "\n",
        "        # Apply edge features\n",
        "        edge_features = self.edge_mlp(edge_attr)\n",
        "\n",
        "        # First GraphSAGE layer\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "\n",
        "        # Second GraphSAGE layer\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "\n",
        "        # Global pooling\n",
        "        x = self.pool(x, batch)\n",
        "\n",
        "        # Final linear layer\n",
        "        return self.fc(x)\n",
        "\n",
        "def train_graphsage(model, data_list, targets, epochs=100, lr=0.001):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for data, target in zip(data_list, targets):\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            loss = criterion(out.squeeze(), target.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(data_list)}')\n",
        "\n",
        "def generate_embeddings(model, data_list):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for data in data_list:\n",
        "            x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "            x = (x - x.mean(dim=0)) / (x.std(dim=0) + 1e-5)\n",
        "            x = F.relu(model.conv1(x, edge_index))\n",
        "            x = F.relu(model.conv2(x, edge_index))\n",
        "            embedding = model.pool(x, batch)\n",
        "            embeddings.append(embedding.cpu().numpy())\n",
        "    return embeddings\n",
        "\n",
        "# Example usage\n",
        "train_df['Graph'] = train_df['Structure (SMILES)'].apply(smiles_to_graph)\n",
        "data_list = [graph for graph in train_df['Graph'] if graph is not None]\n",
        "targets = torch.tensor(train_df['Class'].values)\n",
        "\n",
        "input_dim = data_list[0].x.shape[1]\n",
        "edge_dim = data_list[0].edge_attr.shape[1]\n",
        "graphsage_model = GraphSAGE(input_dim=input_dim, hidden_dim=64, output_dim=1, edge_dim=edge_dim)\n",
        "\n",
        "# Train the model\n",
        "train_graphsage(graphsage_model, data_list, targets, epochs=100, lr=0.001)\n",
        "\n",
        "# Generate embeddings for training set\n",
        "embeddings = generate_embeddings(graphsage_model, data_list)\n",
        "\n",
        "# Print statistics about the embeddings\n",
        "embeddings_array = np.array(embeddings)\n",
        "print(f\"Embeddings shape: {embeddings_array.shape}\")\n",
        "print(f\"Mean: {np.mean(embeddings_array)}\")\n",
        "print(f\"Std: {np.std(embeddings_array)}\")\n",
        "print(f\"Min: {np.min(embeddings_array)}\")\n",
        "print(f\"Max: {np.max(embeddings_array)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Td_x-gzLBmSI",
        "outputId": "ff4e7fc9-ffec-4119-bfc9-1e28808e2500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best parameters and results have been saved to 'grid_search_results_gcn_fm.txt' and 'best_results_gcn_fm.xlsx'\n"
          ]
        }
      ],
      "source": [
        "# parameter settings code\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, balanced_accuracy_score, recall_score, precision_score, f1_score, cohen_kappa_score\n",
        "\n",
        "# Dense model\n",
        "class DenseModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, dropout_rate=0.5):\n",
        "        super(DenseModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class PyTorchModelWrapper(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, hidden_dim1=128, hidden_dim2=64, lr=0.01, epochs=100, dropout_rate=0.5):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim1 = hidden_dim1\n",
        "        self.hidden_dim2 = hidden_dim2\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.dropout_rate = dropout_rate  # Add dropout_rate\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = None\n",
        "        self.classes_ = np.array([0, 1])  # Binary classification\n",
        "\n",
        "    def _initialize_model(self):\n",
        "        return DenseModel(self.input_dim, self.hidden_dim1, self.hidden_dim2, 1, self.dropout_rate).to(self.device)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model = self._initialize_model()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).view(-1, 1))\n",
        "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "        self.model.train()\n",
        "        for epoch in range(self.epochs):\n",
        "            for inputs, labels in loader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model has not been fitted yet.\")\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "            outputs = self.model(X_tensor)\n",
        "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
        "        return np.hstack([1 - probs, probs])  # Return probabilities for both classes\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        return np.argmax(probs, axis=1)\n",
        "\n",
        "def custom_cross_val_score(estimator, X, y, cv, scoring):\n",
        "    scores = []\n",
        "    for train_index, test_index in cv.split(X, y):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        estimator.fit(X_train, y_train)\n",
        "        y_pred = estimator.predict(X_test)\n",
        "        y_pred_proba = estimator.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        if scoring == 'accuracy':\n",
        "            score = accuracy_score(y_test, y_pred)\n",
        "        elif scoring == 'roc_auc':\n",
        "            score = roc_auc_score(y_test, y_pred_proba)\n",
        "        elif scoring == 'balanced_accuracy':\n",
        "            score = balanced_accuracy_score(y_test, y_pred)\n",
        "        elif scoring == 'recall':\n",
        "            score = recall_score(y_test, y_pred)\n",
        "        elif scoring == 'precision':\n",
        "            score = precision_score(y_test, y_pred)\n",
        "        elif scoring == 'f1':\n",
        "            score = f1_score(y_test, y_pred)\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    return np.array(scores)\n",
        "\n",
        "def run_grid_search(X_train, y_train):\n",
        "    param_grid = {\n",
        "        'hidden_dim1': [32, 64, 128, 256],\n",
        "        'hidden_dim2': [16, 32, 64],\n",
        "        'lr': [0.001, 0.0001, 0.01],\n",
        "        'dropout_rate': [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "    }\n",
        "\n",
        "    clf = PyTorchModelWrapper(input_dim=X_train.shape[1])\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    grid_search = GridSearchCV(clf, param_grid, cv=cv, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Write all results to a text file\n",
        "    with open(\"/content/drive/MyDrive/HOB/Rerun/grid_search_results_gcn_fm.txt\", \"w\") as f:\n",
        "        f.write(\"Results for PyTorch classifier:\\n\\n\")\n",
        "        for mean_score, params in zip(grid_search.cv_results_[\"mean_test_score\"], grid_search.cv_results_[\"params\"]):\n",
        "            f.write(f\"Mean accuracy: {mean_score:.4f}, Params: {params}\\n\")\n",
        "\n",
        "    best_params = grid_search.best_params_\n",
        "    final_clf = clf.set_params(**best_params)\n",
        "\n",
        "    results = {}\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for metric in ['accuracy', 'roc_auc', 'balanced_accuracy', 'recall', 'precision', 'f1']:\n",
        "        scores = custom_cross_val_score(final_clf, X_train, y_train, cv, scoring=metric)\n",
        "        results[metric.capitalize()] = scores.mean()\n",
        "\n",
        "    # Kappa score\n",
        "    kappa_scores = []\n",
        "    for train_index, test_index in cv.split(X_train, y_train):\n",
        "        X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
        "        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
        "        final_clf.fit(X_train_fold, y_train_fold)\n",
        "        y_pred = final_clf.predict(X_test_fold)\n",
        "        kappa = cohen_kappa_score(y_test_fold, y_pred)\n",
        "        kappa_scores.append(kappa)\n",
        "    results[\"Kappa\"] = np.mean(kappa_scores)\n",
        "\n",
        "    # Append best results to the text file\n",
        "    with open(\"/content/drive/MyDrive/HOB/Rerun/grid_search_results_gcn_fm.txt\", \"a\") as f:\n",
        "        f.write(\"\\nBest Parameters:\\n\")\n",
        "        for key, value in best_params.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "        f.write(\"\\nBest Results:\\n\")\n",
        "        for key, value in results.items():\n",
        "            f.write(f\"{key}: {value:.4f}\\n\")\n",
        "\n",
        "    # Save best results to Excel file\n",
        "    df = pd.DataFrame([best_params | results])\n",
        "    df.to_excel(\"/content/drive/MyDrive/HOB/Rerun/best_results_gcn_fm.xlsx\", index=False)\n",
        "\n",
        "    return best_params, results\n",
        "\n",
        "def main():\n",
        "    # Load your data\n",
        "    X_train_tensor = torch.tensor(gnn_mol_features_train_df.values[:, :-1], dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(gnn_mol_features_train_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    # Convert to numpy for GridSearchCV\n",
        "    X_train = X_train_tensor.numpy()\n",
        "    y_train = y_train_tensor.numpy().ravel()\n",
        "\n",
        "    # Run grid search\n",
        "    best_params, results = run_grid_search(X_train, y_train)\n",
        "\n",
        "    print(\"\\nBest parameters and results have been saved to 'grid_search_results_gcn_fm.txt' and 'best_results_gcn_fm.xlsx'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "S9AQ2i9_tSMn",
        "outputId": "615ae543-bcca-44ac-ce3d-5fb340473e7c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nX_train_tensor = torch.tensor(gnn_mol_features_train_df.values[:,:-1], dtype=torch.float32)\\ny_train_tensor = torch.tensor(gnn_mol_features_train_df['Class'].values, dtype=torch.float32).view(-1, 1)\\nX_test_tensor = torch.tensor(gnn_mol_features_test_df.values[:,:-1], dtype=torch.float32)\\ny_test_tensor = torch.tensor(gnn_mol_features_test_df['Class'].values, dtype=torch.float32).view(-1, 1)\""
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "X_train_tensor = torch.tensor(train_fea_emb_df.values[:,:-1], dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(train_fea_emb_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(test_fea_emb_df.values[:,:-1], dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(test_fea_emb_df['Class'].values, dtype=torch.float32).view(-1, 1)'''\n",
        "\n",
        "X_train_tensor = torch.tensor(atc_gnn_mol_features_train_df.values[:,:-1], dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(atc_gnn_mol_features_train_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(atc_gnn_mol_features_test_df.values[:,:-1], dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(atc_gnn_mol_features_test_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "'''\n",
        "X_train_tensor = torch.tensor(gnn_mol_features_train_df.values[:,:-1], dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(gnn_mol_features_train_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(gnn_mol_features_test_df.values[:,:-1], dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(gnn_mol_features_test_df['Class'].values, dtype=torch.float32).view(-1, 1)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHyRBONm5GiN",
        "outputId": "6cb54693-0a8a-42a9-dff1-86dae9ca6bb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1/50\n",
            "Epoch 10/100, Loss: 0.5931341217623817\n",
            "Epoch 20/100, Loss: 0.5369629876481162\n",
            "Epoch 30/100, Loss: 0.4966844502422545\n",
            "Epoch 40/100, Loss: 0.46197225981288487\n",
            "Epoch 50/100, Loss: 0.4216574645704693\n",
            "Epoch 60/100, Loss: 0.3713489721218745\n",
            "Epoch 70/100, Loss: 0.316034912235207\n",
            "Epoch 80/100, Loss: 0.26691240982876885\n",
            "Epoch 90/100, Loss: 0.24263381212949753\n",
            "Epoch 100/100, Loss: 0.2061309094230334\n",
            "Fold 1 - Accuracy: 0.8000, Balanced Accuracy: 0.6061\n",
            "Fold 2/50\n",
            "Epoch 10/100, Loss: 0.6421741876337264\n",
            "Epoch 20/100, Loss: 0.5548690325684018\n",
            "Epoch 30/100, Loss: 0.5140771485037274\n",
            "Epoch 40/100, Loss: 0.49797484775384265\n",
            "Epoch 50/100, Loss: 0.4615841077433692\n",
            "Epoch 60/100, Loss: 0.42403654754161835\n",
            "Epoch 70/100, Loss: 0.38398225605487823\n",
            "Epoch 80/100, Loss: 0.3348555548323525\n",
            "Epoch 90/100, Loss: 0.301211675008138\n",
            "Epoch 100/100, Loss: 0.26275601651933456\n",
            "Fold 2 - Accuracy: 0.8000, Balanced Accuracy: 0.6407\n",
            "Fold 3/50\n",
            "Epoch 10/100, Loss: 0.6170327597194247\n",
            "Epoch 20/100, Loss: 0.5525340735912323\n",
            "Epoch 30/100, Loss: 0.5126617782645755\n",
            "Epoch 40/100, Loss: 0.49418887661563027\n",
            "Epoch 50/100, Loss: 0.46265114347139996\n",
            "Epoch 60/100, Loss: 0.4309441198905309\n",
            "Epoch 70/100, Loss: 0.39020997616979813\n",
            "Epoch 80/100, Loss: 0.3424500558111403\n",
            "Epoch 90/100, Loss: 0.3028415184881952\n",
            "Epoch 100/100, Loss: 0.26042312797572875\n",
            "Fold 3 - Accuracy: 0.7698, Balanced Accuracy: 0.5493\n",
            "Fold 4/50\n",
            "Epoch 10/100, Loss: 0.6382408506340451\n",
            "Epoch 20/100, Loss: 0.5344603144460254\n",
            "Epoch 30/100, Loss: 0.49531539115640855\n",
            "Epoch 40/100, Loss: 0.46026456024911666\n",
            "Epoch 50/100, Loss: 0.4097773962550693\n",
            "Epoch 60/100, Loss: 0.36255598564942676\n",
            "Epoch 70/100, Loss: 0.3129781401819653\n",
            "Epoch 80/100, Loss: 0.27265451931291157\n",
            "Epoch 90/100, Loss: 0.2278296964036094\n",
            "Epoch 100/100, Loss: 0.20597290827168357\n",
            "Fold 4 - Accuracy: 0.7626, Balanced Accuracy: 0.6049\n",
            "Fold 5/50\n",
            "Epoch 10/100, Loss: 0.5718013445536295\n",
            "Epoch 20/100, Loss: 0.5213798069291644\n",
            "Epoch 30/100, Loss: 0.49237148132589126\n",
            "Epoch 40/100, Loss: 0.4626898301972283\n",
            "Epoch 50/100, Loss: 0.41209520068433547\n",
            "Epoch 60/100, Loss: 0.3834532317188051\n",
            "Epoch 70/100, Loss: 0.3434854480955336\n",
            "Epoch 80/100, Loss: 0.30143019060293835\n",
            "Epoch 90/100, Loss: 0.2678356137540605\n",
            "Epoch 100/100, Loss: 0.23094058533509573\n",
            "Fold 5 - Accuracy: 0.8058, Balanced Accuracy: 0.5983\n",
            "Fold 6/50\n",
            "Epoch 10/100, Loss: 0.6465877725018395\n",
            "Epoch 20/100, Loss: 0.5584556443823708\n",
            "Epoch 30/100, Loss: 0.5023016697830625\n",
            "Epoch 40/100, Loss: 0.4813584238290787\n",
            "Epoch 50/100, Loss: 0.4394298295180003\n",
            "Epoch 60/100, Loss: 0.39381149742338395\n",
            "Epoch 70/100, Loss: 0.34946541239817935\n",
            "Epoch 80/100, Loss: 0.3020554615391625\n",
            "Epoch 90/100, Loss: 0.2738850803838836\n",
            "Epoch 100/100, Loss: 0.22476800200011995\n",
            "Fold 6 - Accuracy: 0.7143, Balanced Accuracy: 0.5525\n",
            "Fold 7/50\n",
            "Epoch 10/100, Loss: 0.6829644176695082\n",
            "Epoch 20/100, Loss: 0.584586931599511\n",
            "Epoch 30/100, Loss: 0.539731020728747\n",
            "Epoch 40/100, Loss: 0.5074975457456377\n",
            "Epoch 50/100, Loss: 0.47658587495485943\n",
            "Epoch 60/100, Loss: 0.4357269008954366\n",
            "Epoch 70/100, Loss: 0.40631014108657837\n",
            "Epoch 80/100, Loss: 0.35044079025586444\n",
            "Epoch 90/100, Loss: 0.3083094788922204\n",
            "Epoch 100/100, Loss: 0.29662542459037566\n",
            "Fold 7 - Accuracy: 0.8071, Balanced Accuracy: 0.5742\n",
            "Fold 8/50\n",
            "Epoch 10/100, Loss: 0.6319246590137482\n",
            "Epoch 20/100, Loss: 0.5322174396779802\n",
            "Epoch 30/100, Loss: 0.5014372832245297\n",
            "Epoch 40/100, Loss: 0.44879113799995846\n",
            "Epoch 50/100, Loss: 0.4268345799711015\n",
            "Epoch 60/100, Loss: 0.3775532907909817\n",
            "Epoch 70/100, Loss: 0.3402510318491194\n",
            "Epoch 80/100, Loss: 0.3068849237428771\n",
            "Epoch 90/100, Loss: 0.265073305202855\n",
            "Epoch 100/100, Loss: 0.21937133620182672\n",
            "Fold 8 - Accuracy: 0.7698, Balanced Accuracy: 0.6278\n",
            "Fold 9/50\n",
            "Epoch 10/100, Loss: 0.5797047581937578\n",
            "Epoch 20/100, Loss: 0.5271477020449109\n",
            "Epoch 30/100, Loss: 0.5039770238929324\n",
            "Epoch 40/100, Loss: 0.46263667609956527\n",
            "Epoch 50/100, Loss: 0.41396229962507886\n",
            "Epoch 60/100, Loss: 0.373060190015369\n",
            "Epoch 70/100, Loss: 0.32593873971038395\n",
            "Epoch 80/100, Loss: 0.2844211931029956\n",
            "Epoch 90/100, Loss: 0.23868590841690698\n",
            "Epoch 100/100, Loss: 0.20658169231481022\n",
            "Fold 9 - Accuracy: 0.8058, Balanced Accuracy: 0.6113\n",
            "Fold 10/50\n",
            "Epoch 10/100, Loss: 0.6658310261037614\n",
            "Epoch 20/100, Loss: 0.5646329936054018\n",
            "Epoch 30/100, Loss: 0.5248224006758796\n",
            "Epoch 40/100, Loss: 0.49654190242290497\n",
            "Epoch 50/100, Loss: 0.4515618549452888\n",
            "Epoch 60/100, Loss: 0.4176569597588645\n",
            "Epoch 70/100, Loss: 0.36754383477899766\n",
            "Epoch 80/100, Loss: 0.3198542760478126\n",
            "Epoch 90/100, Loss: 0.2865872225827641\n",
            "Epoch 100/100, Loss: 0.2373858185278045\n",
            "Fold 10 - Accuracy: 0.8129, Balanced Accuracy: 0.5758\n",
            "Fold 11/50\n",
            "Epoch 10/100, Loss: 0.5856459074550204\n",
            "Epoch 20/100, Loss: 0.5331064562002817\n",
            "Epoch 30/100, Loss: 0.49554579787784153\n",
            "Epoch 40/100, Loss: 0.4655264483557807\n",
            "Epoch 50/100, Loss: 0.4172358363866806\n",
            "Epoch 60/100, Loss: 0.3794039289156596\n",
            "Epoch 70/100, Loss: 0.32148006392849815\n",
            "Epoch 80/100, Loss: 0.2842953676978747\n",
            "Epoch 90/100, Loss: 0.2501685255103641\n",
            "Epoch 100/100, Loss: 0.22016370958752102\n",
            "Fold 11 - Accuracy: 0.8000, Balanced Accuracy: 0.6176\n",
            "Fold 12/50\n",
            "Epoch 10/100, Loss: 0.6379348801241981\n",
            "Epoch 20/100, Loss: 0.5322413742542267\n",
            "Epoch 30/100, Loss: 0.5026659617821375\n",
            "Epoch 40/100, Loss: 0.46807941297690075\n",
            "Epoch 50/100, Loss: 0.4358933038181729\n",
            "Epoch 60/100, Loss: 0.3955189850595262\n",
            "Epoch 70/100, Loss: 0.3626624710030026\n",
            "Epoch 80/100, Loss: 0.3192915055486891\n",
            "Epoch 90/100, Loss: 0.2754575096898609\n",
            "Epoch 100/100, Loss: 0.2394763289226426\n",
            "Fold 12 - Accuracy: 0.8071, Balanced Accuracy: 0.6129\n",
            "Fold 13/50\n",
            "Epoch 10/100, Loss: 0.6459011303053962\n",
            "Epoch 20/100, Loss: 0.5440682007206811\n",
            "Epoch 30/100, Loss: 0.49910565548472935\n",
            "Epoch 40/100, Loss: 0.4642108778158824\n",
            "Epoch 50/100, Loss: 0.4249550931983524\n",
            "Epoch 60/100, Loss: 0.3644231806198756\n",
            "Epoch 70/100, Loss: 0.32432328164577484\n",
            "Epoch 80/100, Loss: 0.28462031069729066\n",
            "Epoch 90/100, Loss: 0.23426340685950386\n",
            "Epoch 100/100, Loss: 0.21090194334586462\n",
            "Fold 13 - Accuracy: 0.7338, Balanced Accuracy: 0.5647\n",
            "Fold 14/50\n",
            "Epoch 10/100, Loss: 0.6443451411194272\n",
            "Epoch 20/100, Loss: 0.5698438949055142\n",
            "Epoch 30/100, Loss: 0.5239819602833854\n",
            "Epoch 40/100, Loss: 0.48573799431324005\n",
            "Epoch 50/100, Loss: 0.4551405409971873\n",
            "Epoch 60/100, Loss: 0.38629965318573845\n",
            "Epoch 70/100, Loss: 0.35206381811036\n",
            "Epoch 80/100, Loss: 0.30757156593932045\n",
            "Epoch 90/100, Loss: 0.26675863977935577\n",
            "Epoch 100/100, Loss: 0.2273023244407442\n",
            "Fold 14 - Accuracy: 0.8273, Balanced Accuracy: 0.6680\n",
            "Fold 15/50\n",
            "Epoch 10/100, Loss: 0.5931826796796587\n",
            "Epoch 20/100, Loss: 0.5442593279812071\n",
            "Epoch 30/100, Loss: 0.504142529434628\n",
            "Epoch 40/100, Loss: 0.4658620771434572\n",
            "Epoch 50/100, Loss: 0.4264645477135976\n",
            "Epoch 60/100, Loss: 0.3751054149534967\n",
            "Epoch 70/100, Loss: 0.32825970235798096\n",
            "Epoch 80/100, Loss: 0.28524042914311093\n",
            "Epoch 90/100, Loss: 0.2439761840634876\n",
            "Epoch 100/100, Loss: 0.20209092812405693\n",
            "Fold 15 - Accuracy: 0.7986, Balanced Accuracy: 0.6174\n",
            "Fold 16/50\n",
            "Epoch 10/100, Loss: 0.6167081760035621\n",
            "Epoch 20/100, Loss: 0.5183585716618432\n",
            "Epoch 30/100, Loss: 0.49231066472000545\n",
            "Epoch 40/100, Loss: 0.4490436779128181\n",
            "Epoch 50/100, Loss: 0.4066622215840552\n",
            "Epoch 60/100, Loss: 0.37724575565920937\n",
            "Epoch 70/100, Loss: 0.32731837862067753\n",
            "Epoch 80/100, Loss: 0.2982099826137225\n",
            "Epoch 90/100, Loss: 0.2682279042071766\n",
            "Epoch 100/100, Loss: 0.23031218349933624\n",
            "Fold 16 - Accuracy: 0.7786, Balanced Accuracy: 0.6143\n",
            "Fold 17/50\n",
            "Epoch 10/100, Loss: 0.6572742727067735\n",
            "Epoch 20/100, Loss: 0.5411614427963892\n",
            "Epoch 30/100, Loss: 0.4942576289176941\n",
            "Epoch 40/100, Loss: 0.4743937510583136\n",
            "Epoch 50/100, Loss: 0.4353334738148583\n",
            "Epoch 60/100, Loss: 0.4049937095906999\n",
            "Epoch 70/100, Loss: 0.37357085280948216\n",
            "Epoch 80/100, Loss: 0.33274096416102517\n",
            "Epoch 90/100, Loss: 0.2818382639023993\n",
            "Epoch 100/100, Loss: 0.2533498199449645\n",
            "Fold 17 - Accuracy: 0.7857, Balanced Accuracy: 0.6088\n",
            "Fold 18/50\n",
            "Epoch 10/100, Loss: 0.6219894786675771\n",
            "Epoch 20/100, Loss: 0.55057778623369\n",
            "Epoch 30/100, Loss: 0.5131460328896841\n",
            "Epoch 40/100, Loss: 0.47365763783454895\n",
            "Epoch 50/100, Loss: 0.4439597676197688\n",
            "Epoch 60/100, Loss: 0.3702537674042914\n",
            "Epoch 70/100, Loss: 0.32749002012941575\n",
            "Epoch 80/100, Loss: 0.2893475087152587\n",
            "Epoch 90/100, Loss: 0.24692608416080475\n",
            "Epoch 100/100, Loss: 0.21109815521372688\n",
            "Fold 18 - Accuracy: 0.8273, Balanced Accuracy: 0.6704\n",
            "Fold 19/50\n",
            "Epoch 10/100, Loss: 0.618347532219357\n",
            "Epoch 20/100, Loss: 0.5423798561096191\n",
            "Epoch 30/100, Loss: 0.5143658055199517\n",
            "Epoch 40/100, Loss: 0.48556503819094765\n",
            "Epoch 50/100, Loss: 0.42836596733993954\n",
            "Epoch 60/100, Loss: 0.371836985150973\n",
            "Epoch 70/100, Loss: 0.31714630209737354\n",
            "Epoch 80/100, Loss: 0.27638933642043007\n",
            "Epoch 90/100, Loss: 0.24896615660852855\n",
            "Epoch 100/100, Loss: 0.21781818982627657\n",
            "Fold 19 - Accuracy: 0.8273, Balanced Accuracy: 0.6125\n",
            "Fold 20/50\n",
            "Epoch 10/100, Loss: 0.5949338542090522\n",
            "Epoch 20/100, Loss: 0.5260326895448897\n",
            "Epoch 30/100, Loss: 0.49552440146605176\n",
            "Epoch 40/100, Loss: 0.4560665388902028\n",
            "Epoch 50/100, Loss: 0.44037455320358276\n",
            "Epoch 60/100, Loss: 0.39738625039656955\n",
            "Epoch 70/100, Loss: 0.3462225380871031\n",
            "Epoch 80/100, Loss: 0.3165694127480189\n",
            "Epoch 90/100, Loss: 0.2715654869874318\n",
            "Epoch 100/100, Loss: 0.23699688497516844\n",
            "Fold 20 - Accuracy: 0.7266, Balanced Accuracy: 0.5084\n",
            "Fold 21/50\n",
            "Epoch 10/100, Loss: 0.6243028309610155\n",
            "Epoch 20/100, Loss: 0.5357395261526108\n",
            "Epoch 30/100, Loss: 0.5031244903802872\n",
            "Epoch 40/100, Loss: 0.4703989260726505\n",
            "Epoch 50/100, Loss: 0.4199758337603675\n",
            "Epoch 60/100, Loss: 0.3964141557614009\n",
            "Epoch 70/100, Loss: 0.34413525544934803\n",
            "Epoch 80/100, Loss: 0.30051295790407395\n",
            "Epoch 90/100, Loss: 0.26579056680202484\n",
            "Epoch 100/100, Loss: 0.23169844845930734\n",
            "Fold 21 - Accuracy: 0.7429, Balanced Accuracy: 0.5333\n",
            "Fold 22/50\n",
            "Epoch 10/100, Loss: 0.6075977351930406\n",
            "Epoch 20/100, Loss: 0.5355103860298792\n",
            "Epoch 30/100, Loss: 0.5111219816737704\n",
            "Epoch 40/100, Loss: 0.46062180565463173\n",
            "Epoch 50/100, Loss: 0.42254358530044556\n",
            "Epoch 60/100, Loss: 0.36528339154190487\n",
            "Epoch 70/100, Loss: 0.3328605211443371\n",
            "Epoch 80/100, Loss: 0.296331476006243\n",
            "Epoch 90/100, Loss: 0.2611755149232017\n",
            "Epoch 100/100, Loss: 0.21669214218854904\n",
            "Fold 22 - Accuracy: 0.7643, Balanced Accuracy: 0.5800\n",
            "Fold 23/50\n",
            "Epoch 10/100, Loss: 0.656679113705953\n",
            "Epoch 20/100, Loss: 0.5743857092327542\n",
            "Epoch 30/100, Loss: 0.537244685822063\n",
            "Epoch 40/100, Loss: 0.5000587072637346\n",
            "Epoch 50/100, Loss: 0.4658339238829083\n",
            "Epoch 60/100, Loss: 0.4211238995194435\n",
            "Epoch 70/100, Loss: 0.35662641955746543\n",
            "Epoch 80/100, Loss: 0.31643106622828376\n",
            "Epoch 90/100, Loss: 0.2811060945192973\n",
            "Epoch 100/100, Loss: 0.24940578308370379\n",
            "Fold 23 - Accuracy: 0.8273, Balanced Accuracy: 0.6539\n",
            "Fold 24/50\n",
            "Epoch 10/100, Loss: 0.5680769814385308\n",
            "Epoch 20/100, Loss: 0.5121080295907127\n",
            "Epoch 30/100, Loss: 0.4912649409638511\n",
            "Epoch 40/100, Loss: 0.4448227236668269\n",
            "Epoch 50/100, Loss: 0.4004536552561654\n",
            "Epoch 60/100, Loss: 0.3634343023101489\n",
            "Epoch 70/100, Loss: 0.3240816353095902\n",
            "Epoch 80/100, Loss: 0.2803056604332394\n",
            "Epoch 90/100, Loss: 0.2419061834613482\n",
            "Epoch 100/100, Loss: 0.2220607805583212\n",
            "Fold 24 - Accuracy: 0.8345, Balanced Accuracy: 0.6415\n",
            "Fold 25/50\n",
            "Epoch 10/100, Loss: 0.6183663705984751\n",
            "Epoch 20/100, Loss: 0.5484239061673483\n",
            "Epoch 30/100, Loss: 0.5172595944669511\n",
            "Epoch 40/100, Loss: 0.49298065569665694\n",
            "Epoch 50/100, Loss: 0.4640858835644192\n",
            "Epoch 60/100, Loss: 0.4196159127685759\n",
            "Epoch 70/100, Loss: 0.36878972086641526\n",
            "Epoch 80/100, Loss: 0.32482065591547227\n",
            "Epoch 90/100, Loss: 0.2841689818435245\n",
            "Epoch 100/100, Loss: 0.24315633790360558\n",
            "Fold 25 - Accuracy: 0.7986, Balanced Accuracy: 0.6380\n",
            "Fold 26/50\n",
            "Epoch 10/100, Loss: 0.6340899500581954\n",
            "Epoch 20/100, Loss: 0.5602792948484421\n",
            "Epoch 30/100, Loss: 0.5156510803434584\n",
            "Epoch 40/100, Loss: 0.4782864915000068\n",
            "Epoch 50/100, Loss: 0.43423327886395985\n",
            "Epoch 60/100, Loss: 0.38038996275928283\n",
            "Epoch 70/100, Loss: 0.33523579355743194\n",
            "Epoch 80/100, Loss: 0.322720516886976\n",
            "Epoch 90/100, Loss: 0.27604110207822585\n",
            "Epoch 100/100, Loss: 0.2333240649766392\n",
            "Fold 26 - Accuracy: 0.7643, Balanced Accuracy: 0.5734\n",
            "Fold 27/50\n",
            "Epoch 10/100, Loss: 0.6488391160964966\n",
            "Epoch 20/100, Loss: 0.5497778091165755\n",
            "Epoch 30/100, Loss: 0.5260310338603126\n",
            "Epoch 40/100, Loss: 0.48866042991479236\n",
            "Epoch 50/100, Loss: 0.4658943149778578\n",
            "Epoch 60/100, Loss: 0.41332487927542794\n",
            "Epoch 70/100, Loss: 0.3656301928891076\n",
            "Epoch 80/100, Loss: 0.32043255037731594\n",
            "Epoch 90/100, Loss: 0.283549539744854\n",
            "Epoch 100/100, Loss: 0.2537903015812238\n",
            "Fold 27 - Accuracy: 0.8429, Balanced Accuracy: 0.6741\n",
            "Fold 28/50\n",
            "Epoch 10/100, Loss: 0.6591703693072001\n",
            "Epoch 20/100, Loss: 0.6067415409617953\n",
            "Epoch 30/100, Loss: 0.5368891970978843\n",
            "Epoch 40/100, Loss: 0.510938349697325\n",
            "Epoch 50/100, Loss: 0.47769997186130947\n",
            "Epoch 60/100, Loss: 0.462572596139378\n",
            "Epoch 70/100, Loss: 0.41052334838443333\n",
            "Epoch 80/100, Loss: 0.38114164190159905\n",
            "Epoch 90/100, Loss: 0.3209172818395827\n",
            "Epoch 100/100, Loss: 0.27819473130835426\n",
            "Fold 28 - Accuracy: 0.7554, Balanced Accuracy: 0.5899\n",
            "Fold 29/50\n",
            "Epoch 10/100, Loss: 0.6485390696260664\n",
            "Epoch 20/100, Loss: 0.5440011438396242\n",
            "Epoch 30/100, Loss: 0.5111182481050491\n",
            "Epoch 40/100, Loss: 0.4677500045961804\n",
            "Epoch 50/100, Loss: 0.449855281247033\n",
            "Epoch 60/100, Loss: 0.4015323850843642\n",
            "Epoch 70/100, Loss: 0.3502320961819755\n",
            "Epoch 80/100, Loss: 0.29903668496343827\n",
            "Epoch 90/100, Loss: 0.2607080770863427\n",
            "Epoch 100/100, Loss: 0.22282154775328106\n",
            "Fold 29 - Accuracy: 0.8417, Balanced Accuracy: 0.6207\n",
            "Fold 30/50\n",
            "Epoch 10/100, Loss: 0.6752297182877859\n",
            "Epoch 20/100, Loss: 0.6157084008057913\n",
            "Epoch 30/100, Loss: 0.5452046046654383\n",
            "Epoch 40/100, Loss: 0.49938397275076973\n",
            "Epoch 50/100, Loss: 0.49720167948140037\n",
            "Epoch 60/100, Loss: 0.4562659627861447\n",
            "Epoch 70/100, Loss: 0.411386145485772\n",
            "Epoch 80/100, Loss: 0.3581372747818629\n",
            "Epoch 90/100, Loss: 0.311214121679465\n",
            "Epoch 100/100, Loss: 0.2718621674511168\n",
            "Fold 30 - Accuracy: 0.7554, Balanced Accuracy: 0.5564\n",
            "Fold 31/50\n",
            "Epoch 10/100, Loss: 0.644849118259218\n",
            "Epoch 20/100, Loss: 0.5953136715624068\n",
            "Epoch 30/100, Loss: 0.5361402514908049\n",
            "Epoch 40/100, Loss: 0.5202105806933509\n",
            "Epoch 50/100, Loss: 0.48367056416140664\n",
            "Epoch 60/100, Loss: 0.44582009977764553\n",
            "Epoch 70/100, Loss: 0.4089427375131183\n",
            "Epoch 80/100, Loss: 0.36102406349447036\n",
            "Epoch 90/100, Loss: 0.31729919380611843\n",
            "Epoch 100/100, Loss: 0.2748754413591491\n",
            "Fold 31 - Accuracy: 0.8071, Balanced Accuracy: 0.6328\n",
            "Fold 32/50\n",
            "Epoch 10/100, Loss: 0.6600675582885742\n",
            "Epoch 20/100, Loss: 0.5544064011838701\n",
            "Epoch 30/100, Loss: 0.5181416190332837\n",
            "Epoch 40/100, Loss: 0.48232395781411064\n",
            "Epoch 50/100, Loss: 0.452918807665507\n",
            "Epoch 60/100, Loss: 0.4114068994919459\n",
            "Epoch 70/100, Loss: 0.3703491638104121\n",
            "Epoch 80/100, Loss: 0.32245829204718274\n",
            "Epoch 90/100, Loss: 0.28864099582036334\n",
            "Epoch 100/100, Loss: 0.2403356490863694\n",
            "Fold 32 - Accuracy: 0.8000, Balanced Accuracy: 0.6176\n",
            "Fold 33/50\n",
            "Epoch 10/100, Loss: 0.6002729899353452\n",
            "Epoch 20/100, Loss: 0.5329602609078089\n",
            "Epoch 30/100, Loss: 0.5075678295559354\n",
            "Epoch 40/100, Loss: 0.4668031682570775\n",
            "Epoch 50/100, Loss: 0.44721632036897874\n",
            "Epoch 60/100, Loss: 0.3951198061307271\n",
            "Epoch 70/100, Loss: 0.3439209113518397\n",
            "Epoch 80/100, Loss: 0.3046972014837795\n",
            "Epoch 90/100, Loss: 0.2692670499285062\n",
            "Epoch 100/100, Loss: 0.21645057863659328\n",
            "Fold 33 - Accuracy: 0.7626, Balanced Accuracy: 0.5475\n",
            "Fold 34/50\n",
            "Epoch 10/100, Loss: 0.6357346773147583\n",
            "Epoch 20/100, Loss: 0.5461025668515099\n",
            "Epoch 30/100, Loss: 0.5019653836886088\n",
            "Epoch 40/100, Loss: 0.4597023477156957\n",
            "Epoch 50/100, Loss: 0.42235970083210206\n",
            "Epoch 60/100, Loss: 0.34745538151926464\n",
            "Epoch 70/100, Loss: 0.3218184212843577\n",
            "Epoch 80/100, Loss: 0.2735824030306604\n",
            "Epoch 90/100, Loss: 0.2227950385875172\n",
            "Epoch 100/100, Loss: 0.1988726390732659\n",
            "Fold 34 - Accuracy: 0.7914, Balanced Accuracy: 0.6270\n",
            "Fold 35/50\n",
            "Epoch 10/100, Loss: 0.5969456666045718\n",
            "Epoch 20/100, Loss: 0.536879418624772\n",
            "Epoch 30/100, Loss: 0.5027121173010932\n",
            "Epoch 40/100, Loss: 0.47624970310264164\n",
            "Epoch 50/100, Loss: 0.42509283290969\n",
            "Epoch 60/100, Loss: 0.37843236161602867\n",
            "Epoch 70/100, Loss: 0.3274858800901307\n",
            "Epoch 80/100, Loss: 0.2816363697250684\n",
            "Epoch 90/100, Loss: 0.24715490142504373\n",
            "Epoch 100/100, Loss: 0.22960824767748514\n",
            "Fold 35 - Accuracy: 0.7698, Balanced Accuracy: 0.5990\n",
            "Fold 36/50\n",
            "Epoch 10/100, Loss: 0.7013296286265055\n",
            "Epoch 20/100, Loss: 0.5618149042129517\n",
            "Epoch 30/100, Loss: 0.5106554941998588\n",
            "Epoch 40/100, Loss: 0.4757729901207818\n",
            "Epoch 50/100, Loss: 0.44352412389384377\n",
            "Epoch 60/100, Loss: 0.4048644569185045\n",
            "Epoch 70/100, Loss: 0.36052532990773517\n",
            "Epoch 80/100, Loss: 0.32223839312791824\n",
            "Epoch 90/100, Loss: 0.28458907786342835\n",
            "Epoch 100/100, Loss: 0.24424029141664505\n",
            "Fold 36 - Accuracy: 0.7857, Balanced Accuracy: 0.5874\n",
            "Fold 37/50\n",
            "Epoch 10/100, Loss: 0.6182892653677199\n",
            "Epoch 20/100, Loss: 0.5433723645077811\n",
            "Epoch 30/100, Loss: 0.5129431817266676\n",
            "Epoch 40/100, Loss: 0.48344238599141437\n",
            "Epoch 50/100, Loss: 0.43620773322052425\n",
            "Epoch 60/100, Loss: 0.4167652726173401\n",
            "Epoch 70/100, Loss: 0.38548338578806984\n",
            "Epoch 80/100, Loss: 0.3231683588690228\n",
            "Epoch 90/100, Loss: 0.2669829974571864\n",
            "Epoch 100/100, Loss: 0.24556226283311844\n",
            "Fold 37 - Accuracy: 0.8143, Balanced Accuracy: 0.6270\n",
            "Fold 38/50\n",
            "Epoch 10/100, Loss: 0.6110449300871955\n",
            "Epoch 20/100, Loss: 0.5366009689039655\n",
            "Epoch 30/100, Loss: 0.4961397002140681\n",
            "Epoch 40/100, Loss: 0.457247041993671\n",
            "Epoch 50/100, Loss: 0.417687252163887\n",
            "Epoch 60/100, Loss: 0.36134927968184155\n",
            "Epoch 70/100, Loss: 0.32326210372977787\n",
            "Epoch 80/100, Loss: 0.28947070489327115\n",
            "Epoch 90/100, Loss: 0.2470552639828788\n",
            "Epoch 100/100, Loss: 0.21728389668795797\n",
            "Fold 38 - Accuracy: 0.7698, Balanced Accuracy: 0.6104\n",
            "Fold 39/50\n",
            "Epoch 10/100, Loss: 0.6675665080547333\n",
            "Epoch 20/100, Loss: 0.5625506109661527\n",
            "Epoch 30/100, Loss: 0.5380283064312406\n",
            "Epoch 40/100, Loss: 0.49646417962180245\n",
            "Epoch 50/100, Loss: 0.4554204311635759\n",
            "Epoch 60/100, Loss: 0.4108424269490772\n",
            "Epoch 70/100, Loss: 0.3468293026089668\n",
            "Epoch 80/100, Loss: 0.3203854146930907\n",
            "Epoch 90/100, Loss: 0.27053245819277233\n",
            "Epoch 100/100, Loss: 0.24464109788338342\n",
            "Fold 39 - Accuracy: 0.8201, Balanced Accuracy: 0.5933\n",
            "Fold 40/50\n",
            "Epoch 10/100, Loss: 0.6675467524263594\n",
            "Epoch 20/100, Loss: 0.5482201940483518\n",
            "Epoch 30/100, Loss: 0.5215179539389081\n",
            "Epoch 40/100, Loss: 0.49031033284134334\n",
            "Epoch 50/100, Loss: 0.4569143172767427\n",
            "Epoch 60/100, Loss: 0.4084730413224962\n",
            "Epoch 70/100, Loss: 0.37686145140065086\n",
            "Epoch 80/100, Loss: 0.3398995167679257\n",
            "Epoch 90/100, Loss: 0.29017549753189087\n",
            "Epoch 100/100, Loss: 0.2614731573396259\n",
            "Fold 40 - Accuracy: 0.7554, Balanced Accuracy: 0.5690\n",
            "Fold 41/50\n",
            "Epoch 10/100, Loss: 0.5958525637785593\n",
            "Epoch 20/100, Loss: 0.5341983702447679\n",
            "Epoch 30/100, Loss: 0.49118663039472366\n",
            "Epoch 40/100, Loss: 0.45590046876006657\n",
            "Epoch 50/100, Loss: 0.4083138389719857\n",
            "Epoch 60/100, Loss: 0.37780114346080357\n",
            "Epoch 70/100, Loss: 0.3287013860212432\n",
            "Epoch 80/100, Loss: 0.2745270977417628\n",
            "Epoch 90/100, Loss: 0.24146070910824668\n",
            "Epoch 100/100, Loss: 0.21071104208628336\n",
            "Fold 41 - Accuracy: 0.7714, Balanced Accuracy: 0.6199\n",
            "Fold 42/50\n",
            "Epoch 10/100, Loss: 0.6214384602175819\n",
            "Epoch 20/100, Loss: 0.5356436255905364\n",
            "Epoch 30/100, Loss: 0.5151153372393714\n",
            "Epoch 40/100, Loss: 0.48036569522486794\n",
            "Epoch 50/100, Loss: 0.44431577457322013\n",
            "Epoch 60/100, Loss: 0.4030579576889674\n",
            "Epoch 70/100, Loss: 0.35564302487505806\n",
            "Epoch 80/100, Loss: 0.3162539816564984\n",
            "Epoch 90/100, Loss: 0.26000942207045025\n",
            "Epoch 100/100, Loss: 0.24236855821477044\n",
            "Fold 42 - Accuracy: 0.7857, Balanced Accuracy: 0.6197\n",
            "Fold 43/50\n",
            "Epoch 10/100, Loss: 0.6830922199620141\n",
            "Epoch 20/100, Loss: 0.5639120870166354\n",
            "Epoch 30/100, Loss: 0.5203254289097257\n",
            "Epoch 40/100, Loss: 0.4914430015616947\n",
            "Epoch 50/100, Loss: 0.45456110106574166\n",
            "Epoch 60/100, Loss: 0.3956199685732524\n",
            "Epoch 70/100, Loss: 0.3539892261226972\n",
            "Epoch 80/100, Loss: 0.31437836752997506\n",
            "Epoch 90/100, Loss: 0.27354500111606384\n",
            "Epoch 100/100, Loss: 0.22458883540497887\n",
            "Fold 43 - Accuracy: 0.8129, Balanced Accuracy: 0.5771\n",
            "Fold 44/50\n",
            "Epoch 10/100, Loss: 0.5785936646991305\n",
            "Epoch 20/100, Loss: 0.5239207976394229\n",
            "Epoch 30/100, Loss: 0.5007637391487757\n",
            "Epoch 40/100, Loss: 0.45872191256946987\n",
            "Epoch 50/100, Loss: 0.42295774817466736\n",
            "Epoch 60/100, Loss: 0.37364662769767976\n",
            "Epoch 70/100, Loss: 0.3313998273677296\n",
            "Epoch 80/100, Loss: 0.28288447029060787\n",
            "Epoch 90/100, Loss: 0.23803563995493782\n",
            "Epoch 100/100, Loss: 0.22064235392544004\n",
            "Fold 44 - Accuracy: 0.7554, Balanced Accuracy: 0.6019\n",
            "Fold 45/50\n",
            "Epoch 10/100, Loss: 0.6921812759505378\n",
            "Epoch 20/100, Loss: 0.6023146079646217\n",
            "Epoch 30/100, Loss: 0.5459959374533759\n",
            "Epoch 40/100, Loss: 0.5150149282481935\n",
            "Epoch 50/100, Loss: 0.4735320011774699\n",
            "Epoch 60/100, Loss: 0.4236256248421139\n",
            "Epoch 70/100, Loss: 0.3763850513431761\n",
            "Epoch 80/100, Loss: 0.33936752958430183\n",
            "Epoch 90/100, Loss: 0.3011245040429963\n",
            "Epoch 100/100, Loss: 0.2646991353895929\n",
            "Fold 45 - Accuracy: 0.8201, Balanced Accuracy: 0.6229\n",
            "Fold 46/50\n",
            "Epoch 10/100, Loss: 0.6401364770200517\n",
            "Epoch 20/100, Loss: 0.5529289295276006\n",
            "Epoch 30/100, Loss: 0.5141586578554578\n",
            "Epoch 40/100, Loss: 0.4837322152323193\n",
            "Epoch 50/100, Loss: 0.4417288965649075\n",
            "Epoch 60/100, Loss: 0.4111074772146013\n",
            "Epoch 70/100, Loss: 0.371559985809856\n",
            "Epoch 80/100, Loss: 0.32440058059162563\n",
            "Epoch 90/100, Loss: 0.2898913001020749\n",
            "Epoch 100/100, Loss: 0.2613421794441011\n",
            "Fold 46 - Accuracy: 0.8429, Balanced Accuracy: 0.6339\n",
            "Fold 47/50\n",
            "Epoch 10/100, Loss: 0.5842378636201223\n",
            "Epoch 20/100, Loss: 0.5255717701382108\n",
            "Epoch 30/100, Loss: 0.4966624296373791\n",
            "Epoch 40/100, Loss: 0.4647452218665017\n",
            "Epoch 50/100, Loss: 0.42440639270676506\n",
            "Epoch 60/100, Loss: 0.37895436667733723\n",
            "Epoch 70/100, Loss: 0.3412245445781284\n",
            "Epoch 80/100, Loss: 0.3008653041389253\n",
            "Epoch 90/100, Loss: 0.2712893204556571\n",
            "Epoch 100/100, Loss: 0.24173658506737816\n",
            "Fold 47 - Accuracy: 0.7500, Balanced Accuracy: 0.5790\n",
            "Fold 48/50\n",
            "Epoch 10/100, Loss: 0.6813295351134406\n",
            "Epoch 20/100, Loss: 0.5780734734402763\n",
            "Epoch 30/100, Loss: 0.5144025865528319\n",
            "Epoch 40/100, Loss: 0.48201365106635624\n",
            "Epoch 50/100, Loss: 0.46466758184962803\n",
            "Epoch 60/100, Loss: 0.3973489850759506\n",
            "Epoch 70/100, Loss: 0.36706965913375217\n",
            "Epoch 80/100, Loss: 0.3221505491269959\n",
            "Epoch 90/100, Loss: 0.29121190475092995\n",
            "Epoch 100/100, Loss: 0.2573508388466305\n",
            "Fold 48 - Accuracy: 0.8489, Balanced Accuracy: 0.6132\n",
            "Fold 49/50\n",
            "Epoch 10/100, Loss: 0.6607925991217295\n",
            "Epoch 20/100, Loss: 0.5644461148315005\n",
            "Epoch 30/100, Loss: 0.5044514702426063\n",
            "Epoch 40/100, Loss: 0.48173443145222133\n",
            "Epoch 50/100, Loss: 0.4406183809041977\n",
            "Epoch 60/100, Loss: 0.41971876720587414\n",
            "Epoch 70/100, Loss: 0.3649814509683185\n",
            "Epoch 80/100, Loss: 0.3151748643981086\n",
            "Epoch 90/100, Loss: 0.26703209429979324\n",
            "Epoch 100/100, Loss: 0.23572418010897106\n",
            "Fold 49 - Accuracy: 0.7770, Balanced Accuracy: 0.6361\n",
            "Fold 50/50\n",
            "Epoch 10/100, Loss: 0.6053319507175021\n",
            "Epoch 20/100, Loss: 0.5382805135515001\n",
            "Epoch 30/100, Loss: 0.5023094879256355\n",
            "Epoch 40/100, Loss: 0.4639357361528609\n",
            "Epoch 50/100, Loss: 0.42283312645223403\n",
            "Epoch 60/100, Loss: 0.3807327863242891\n",
            "Epoch 70/100, Loss: 0.33591219617260826\n",
            "Epoch 80/100, Loss: 0.2913508175147904\n",
            "Epoch 90/100, Loss: 0.2466157285703553\n",
            "Epoch 100/100, Loss: 0.23610729227463403\n",
            "Fold 50 - Accuracy: 0.7770, Balanced Accuracy: 0.6033\n",
            "\n",
            "Average Metrics:\n",
            "Accuracy: 0.7901 ± 0.0321\n",
            "Balanced_accuracy: 0.6043 ± 0.0345\n",
            "Precision: 0.8083 ± 0.0339\n",
            "Recall: 0.9531 ± 0.0228\n",
            "F1: 0.8742 ± 0.0211\n",
            "Auc: 0.7486 ± 0.0437\n",
            "Kappa: 0.2593 ± 0.0797\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
          ]
        }
      ],
      "source": [
        "#Cross validation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, SubsetRandomSampler\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, roc_curve\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class DenseModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, dropout_rate=0.5):\n",
        "        super(DenseModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def train_dense_model(model, train_loader, criterion, optimizer, epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_preds).flatten(), np.array(all_labels).flatten()\n",
        "\n",
        "def cross_validate_dense_model(X, y, input_dim, hidden_dim1, hidden_dim2, output_dim, criterion, n_splits=5, n_repeats=10, epochs=100, batch_size=32):\n",
        "    cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats)\n",
        "    metrics = {\n",
        "        'accuracy': [], 'balanced_accuracy': [], 'precision': [],\n",
        "        'recall': [], 'f1': [], 'auc': [], 'kappa' : []\n",
        "    }\n",
        "    all_roc_data = []\n",
        "\n",
        "    dataset = TensorDataset(torch.FloatTensor(X), torch.FloatTensor(y))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X)):\n",
        "        print(f\"Fold {fold + 1}/{n_splits * n_repeats}\")\n",
        "\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "        val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "        model = DenseModel(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
        "        optimizer = optim.Adam(model.parameters(), lr= 0.0001)\n",
        "\n",
        "        train_dense_model(model, train_loader, criterion, optimizer, epochs)\n",
        "\n",
        "        val_preds, val_labels = evaluate_model(model, val_loader)\n",
        "\n",
        "        metrics['accuracy'].append(accuracy_score(val_labels, val_preds > 0.5))\n",
        "        metrics['balanced_accuracy'].append(balanced_accuracy_score(val_labels, val_preds > 0.5))\n",
        "        metrics['precision'].append(precision_score(val_labels, val_preds > 0.5))\n",
        "        metrics['recall'].append(recall_score(val_labels, val_preds > 0.5))\n",
        "        metrics['f1'].append(f1_score(val_labels, val_preds > 0.5))\n",
        "        metrics['auc'].append(roc_auc_score(val_labels, val_preds))\n",
        "        metrics['kappa'].append(cohen_kappa_score(val_labels, val_preds > 0.5))\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(val_labels, val_preds)\n",
        "        all_roc_data.append((fpr, tpr, metrics['auc'][-1]))\n",
        "\n",
        "        print(f\"Fold {fold + 1} - Accuracy: {metrics['accuracy'][-1]:.4f}, Balanced Accuracy: {metrics['balanced_accuracy'][-1]:.4f}\")\n",
        "\n",
        "    avg_metrics = {metric: np.mean(scores) for metric, scores in metrics.items()}\n",
        "    std_metrics = {metric: np.std(scores) for metric, scores in metrics.items()}\n",
        "\n",
        "    print(\"\\nAverage Metrics:\")\n",
        "    for metric, value in avg_metrics.items():\n",
        "        print(f\"{metric.capitalize()}: {value:.4f} ± {std_metrics[metric]:.4f}\")\n",
        "\n",
        "    # Plot ROC curves\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "    mean_tpr = np.zeros_like(mean_fpr)\n",
        "    for roc in all_roc_data:\n",
        "        fpr, tpr, auc_score = roc\n",
        "        mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
        "    mean_tpr /= len(all_roc_data)\n",
        "    mean_auc = np.mean([roc[2] for roc in all_roc_data])\n",
        "    plt.plot(mean_fpr, mean_tpr, lw=2, label=f'Mean ROC (AUC = {mean_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='r', lw=2, label='Random Guess')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=24)\n",
        "    plt.ylabel('True Positive Rate', fontsize=24)\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=24)\n",
        "    plt.legend(loc=\"lower right\", fontsize=18)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/MyDrive/HOB/Rerun/roc_curve_mol.png', format='png', bbox_inches='tight')\n",
        "    plt.savefig('/content/drive/MyDrive/HOB/Rerun/roc_curve_mol.eps', format='eps', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return avg_metrics, std_metrics\n",
        "\n",
        "# Usage\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "hidden_dim1 = 32\n",
        "hidden_dim2 = 16\n",
        "output_dim = 1\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "avg_metrics, std_metrics = cross_validate_dense_model(\n",
        "    X_train_tensor.numpy(), y_train_tensor.numpy(),\n",
        "    input_dim, hidden_dim1, hidden_dim2, output_dim, criterion,\n",
        "    n_splits=5, n_repeats=10, epochs=100, batch_size=32\n",
        ")\n",
        "def save_metrics_to_excel(avg_metrics, std_metrics, filename='metrics.xlsx'):\n",
        "    # Combine avg and std metrics into a single DataFrame\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Metric': list(avg_metrics.keys()),\n",
        "        'Average': list(avg_metrics.values()),\n",
        "        'Standard Deviation': list(std_metrics.values())\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame to an Excel file\n",
        "    metrics_df.to_excel(filename, index=False)\n",
        "\n",
        "save_metrics_to_excel(avg_metrics, std_metrics, '/content/drive/MyDrive/HOB/Rerun/metrics.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGulywlfPw8T",
        "outputId": "1fb4e542-bca8-40ba-df6f-58049a1957ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1/50\n",
            "Epoch 10/100, Loss: 0.6807075628527889\n",
            "Epoch 20/100, Loss: 0.6289697532300595\n",
            "Epoch 30/100, Loss: 0.526373831210313\n",
            "Epoch 40/100, Loss: 0.4035666408362212\n",
            "Epoch 50/100, Loss: 0.32668699931215356\n",
            "Epoch 60/100, Loss: 0.2544406216453623\n",
            "Epoch 70/100, Loss: 0.21775850846811576\n",
            "Epoch 80/100, Loss: 0.1632593439684974\n",
            "Epoch 90/100, Loss: 0.1386447482638889\n",
            "Epoch 100/100, Loss: 0.11701670509797556\n",
            "Fold 1 - Accuracy: 0.7429, Balanced Accuracy: 0.5842\n",
            "Fold 2/50\n",
            "Epoch 10/100, Loss: 0.6807695222752435\n",
            "Epoch 20/100, Loss: 0.6186227394001824\n",
            "Epoch 30/100, Loss: 0.5046769987259593\n",
            "Epoch 40/100, Loss: 0.37596595713070463\n",
            "Epoch 50/100, Loss: 0.3027234109384673\n",
            "Epoch 60/100, Loss: 0.22623135814709322\n",
            "Epoch 70/100, Loss: 0.18413542636803218\n",
            "Epoch 80/100, Loss: 0.1463113416518484\n",
            "Epoch 90/100, Loss: 0.12367848839078631\n",
            "Epoch 100/100, Loss: 0.09959333642785038\n",
            "Fold 2 - Accuracy: 0.7429, Balanced Accuracy: 0.6824\n",
            "Fold 3/50\n",
            "Epoch 10/100, Loss: 0.6679619462401779\n",
            "Epoch 20/100, Loss: 0.5907173995618467\n",
            "Epoch 30/100, Loss: 0.4764682032443859\n",
            "Epoch 40/100, Loss: 0.3786440160539415\n",
            "Epoch 50/100, Loss: 0.29863703857969354\n",
            "Epoch 60/100, Loss: 0.23268812287736823\n",
            "Epoch 70/100, Loss: 0.1873659226629469\n",
            "Epoch 80/100, Loss: 0.15159752247510133\n",
            "Epoch 90/100, Loss: 0.11817952273068605\n",
            "Epoch 100/100, Loss: 0.09691920945489849\n",
            "Fold 3 - Accuracy: 0.7338, Balanced Accuracy: 0.6690\n",
            "Fold 4/50\n",
            "Epoch 10/100, Loss: 0.6784913584038064\n",
            "Epoch 20/100, Loss: 0.6191589302486844\n",
            "Epoch 30/100, Loss: 0.503955415001622\n",
            "Epoch 40/100, Loss: 0.38155622504375597\n",
            "Epoch 50/100, Loss: 0.2888937724961175\n",
            "Epoch 60/100, Loss: 0.22056212745330953\n",
            "Epoch 70/100, Loss: 0.16926855124809123\n",
            "Epoch 80/100, Loss: 0.1289807720868676\n",
            "Epoch 90/100, Loss: 0.10741762361592716\n",
            "Epoch 100/100, Loss: 0.08914185384357418\n",
            "Fold 4 - Accuracy: 0.6835, Balanced Accuracy: 0.5754\n",
            "Fold 5/50\n",
            "Epoch 10/100, Loss: 0.6788322726885477\n",
            "Epoch 20/100, Loss: 0.6188353366321988\n",
            "Epoch 30/100, Loss: 0.5030984437024152\n",
            "Epoch 40/100, Loss: 0.396618836455875\n",
            "Epoch 50/100, Loss: 0.30658677054776085\n",
            "Epoch 60/100, Loss: 0.2355757118375213\n",
            "Epoch 70/100, Loss: 0.1886625118829586\n",
            "Epoch 80/100, Loss: 0.15194627670226274\n",
            "Epoch 90/100, Loss: 0.11553597174308917\n",
            "Epoch 100/100, Loss: 0.10003096113602321\n",
            "Fold 5 - Accuracy: 0.7338, Balanced Accuracy: 0.6063\n",
            "Fold 6/50\n",
            "Epoch 10/100, Loss: 0.6752214586293256\n",
            "Epoch 20/100, Loss: 0.6087712667606495\n",
            "Epoch 30/100, Loss: 0.47437022240073595\n",
            "Epoch 40/100, Loss: 0.35949599411752486\n",
            "Epoch 50/100, Loss: 0.2781858631858119\n",
            "Epoch 60/100, Loss: 0.21235778596666124\n",
            "Epoch 70/100, Loss: 0.17073677921736682\n",
            "Epoch 80/100, Loss: 0.14004496540184375\n",
            "Epoch 90/100, Loss: 0.1090335728669608\n",
            "Epoch 100/100, Loss: 0.09369627082789386\n",
            "Fold 6 - Accuracy: 0.7214, Balanced Accuracy: 0.6480\n",
            "Fold 7/50\n",
            "Epoch 10/100, Loss: 0.6841961675220065\n",
            "Epoch 20/100, Loss: 0.6284403513979029\n",
            "Epoch 30/100, Loss: 0.49156950800507154\n",
            "Epoch 40/100, Loss: 0.36709959860201236\n",
            "Epoch 50/100, Loss: 0.26751602634235666\n",
            "Epoch 60/100, Loss: 0.21365575657950509\n",
            "Epoch 70/100, Loss: 0.16999689020492412\n",
            "Epoch 80/100, Loss: 0.12461191788315773\n",
            "Epoch 90/100, Loss: 0.11454761773347855\n",
            "Epoch 100/100, Loss: 0.08539687883522776\n",
            "Fold 7 - Accuracy: 0.7071, Balanced Accuracy: 0.6031\n",
            "Fold 8/50\n",
            "Epoch 10/100, Loss: 0.6818230174205921\n",
            "Epoch 20/100, Loss: 0.6158155202865601\n",
            "Epoch 30/100, Loss: 0.4909963508447011\n",
            "Epoch 40/100, Loss: 0.3814835449059804\n",
            "Epoch 50/100, Loss: 0.2892833865351147\n",
            "Epoch 60/100, Loss: 0.22046765409134053\n",
            "Epoch 70/100, Loss: 0.18186807742825262\n",
            "Epoch 80/100, Loss: 0.1398088440850929\n",
            "Epoch 90/100, Loss: 0.11809475195628626\n",
            "Epoch 100/100, Loss: 0.09160509857314604\n",
            "Fold 8 - Accuracy: 0.7554, Balanced Accuracy: 0.6414\n",
            "Fold 9/50\n",
            "Epoch 10/100, Loss: 0.6816863130640101\n",
            "Epoch 20/100, Loss: 0.6042432652579414\n",
            "Epoch 30/100, Loss: 0.47107969500400404\n",
            "Epoch 40/100, Loss: 0.3544046249654558\n",
            "Epoch 50/100, Loss: 0.26586877361491873\n",
            "Epoch 60/100, Loss: 0.2113393114672767\n",
            "Epoch 70/100, Loss: 0.15754156173379333\n",
            "Epoch 80/100, Loss: 0.12825894548937125\n",
            "Epoch 90/100, Loss: 0.09751657644907634\n",
            "Epoch 100/100, Loss: 0.08496372180956381\n",
            "Fold 9 - Accuracy: 0.7698, Balanced Accuracy: 0.6533\n",
            "Fold 10/50\n",
            "Epoch 10/100, Loss: 0.6779888073603312\n",
            "Epoch 20/100, Loss: 0.6101728236233747\n",
            "Epoch 30/100, Loss: 0.49036645889282227\n",
            "Epoch 40/100, Loss: 0.38749586211310494\n",
            "Epoch 50/100, Loss: 0.29456316541742394\n",
            "Epoch 60/100, Loss: 0.23953348894913992\n",
            "Epoch 70/100, Loss: 0.18947634100914001\n",
            "Epoch 80/100, Loss: 0.15591469020755203\n",
            "Epoch 90/100, Loss: 0.1286341111969065\n",
            "Epoch 100/100, Loss: 0.10423180626498328\n",
            "Fold 10 - Accuracy: 0.6691, Balanced Accuracy: 0.5474\n",
            "Fold 11/50\n",
            "Epoch 10/100, Loss: 0.6841976775063409\n",
            "Epoch 20/100, Loss: 0.6070621697990982\n",
            "Epoch 30/100, Loss: 0.4887513087855445\n",
            "Epoch 40/100, Loss: 0.39007749822404647\n",
            "Epoch 50/100, Loss: 0.2956002801656723\n",
            "Epoch 60/100, Loss: 0.24206340257768277\n",
            "Epoch 70/100, Loss: 0.18796059047734295\n",
            "Epoch 80/100, Loss: 0.14580509618476586\n",
            "Epoch 90/100, Loss: 0.1261646126707395\n",
            "Epoch 100/100, Loss: 0.09747285589023873\n",
            "Fold 11 - Accuracy: 0.6786, Balanced Accuracy: 0.6047\n",
            "Fold 12/50\n",
            "Epoch 10/100, Loss: 0.6784596200342532\n",
            "Epoch 20/100, Loss: 0.6286083879294219\n",
            "Epoch 30/100, Loss: 0.534493002626631\n",
            "Epoch 40/100, Loss: 0.43023242829022584\n",
            "Epoch 50/100, Loss: 0.3369186614398603\n",
            "Epoch 60/100, Loss: 0.25540724948600485\n",
            "Epoch 70/100, Loss: 0.21318802292700167\n",
            "Epoch 80/100, Loss: 0.1611537864362752\n",
            "Epoch 90/100, Loss: 0.14052689185848943\n",
            "Epoch 100/100, Loss: 0.10971436621966185\n",
            "Fold 12 - Accuracy: 0.8000, Balanced Accuracy: 0.6708\n",
            "Fold 13/50\n",
            "Epoch 10/100, Loss: 0.6811475091510348\n",
            "Epoch 20/100, Loss: 0.6179264342343366\n",
            "Epoch 30/100, Loss: 0.48360568284988403\n",
            "Epoch 40/100, Loss: 0.36113360634556524\n",
            "Epoch 50/100, Loss: 0.25591266376000865\n",
            "Epoch 60/100, Loss: 0.19545669356981912\n",
            "Epoch 70/100, Loss: 0.14770611513544013\n",
            "Epoch 80/100, Loss: 0.11423468589782715\n",
            "Epoch 90/100, Loss: 0.1029346157555227\n",
            "Epoch 100/100, Loss: 0.07721271628031025\n",
            "Fold 13 - Accuracy: 0.7482, Balanced Accuracy: 0.6345\n",
            "Fold 14/50\n",
            "Epoch 10/100, Loss: 0.6841518815074649\n",
            "Epoch 20/100, Loss: 0.6120316748108182\n",
            "Epoch 30/100, Loss: 0.4933467688305037\n",
            "Epoch 40/100, Loss: 0.38025605039937155\n",
            "Epoch 50/100, Loss: 0.31097122175352915\n",
            "Epoch 60/100, Loss: 0.22704692823546274\n",
            "Epoch 70/100, Loss: 0.1965872376625027\n",
            "Epoch 80/100, Loss: 0.14995062670537404\n",
            "Epoch 90/100, Loss: 0.1285999579621213\n",
            "Epoch 100/100, Loss: 0.10200486724664058\n",
            "Fold 14 - Accuracy: 0.6691, Balanced Accuracy: 0.5739\n",
            "Fold 15/50\n",
            "Epoch 10/100, Loss: 0.6713350238623442\n",
            "Epoch 20/100, Loss: 0.592778863730254\n",
            "Epoch 30/100, Loss: 0.46548467764148005\n",
            "Epoch 40/100, Loss: 0.3613680650790532\n",
            "Epoch 50/100, Loss: 0.27194320327705807\n",
            "Epoch 60/100, Loss: 0.22060630801651213\n",
            "Epoch 70/100, Loss: 0.17552147502148593\n",
            "Epoch 80/100, Loss: 0.1418738282389111\n",
            "Epoch 90/100, Loss: 0.11936603845269592\n",
            "Epoch 100/100, Loss: 0.10158575671138587\n",
            "Fold 15 - Accuracy: 0.7914, Balanced Accuracy: 0.7047\n",
            "Fold 16/50\n",
            "Epoch 10/100, Loss: 0.6832739070609763\n",
            "Epoch 20/100, Loss: 0.6180633394806473\n",
            "Epoch 30/100, Loss: 0.4919388305257868\n",
            "Epoch 40/100, Loss: 0.362021753081569\n",
            "Epoch 50/100, Loss: 0.2685321453544829\n",
            "Epoch 60/100, Loss: 0.20496220721138847\n",
            "Epoch 70/100, Loss: 0.16171283302483735\n",
            "Epoch 80/100, Loss: 0.12854984085317012\n",
            "Epoch 90/100, Loss: 0.10252986158485766\n",
            "Epoch 100/100, Loss: 0.08964774909394758\n",
            "Fold 16 - Accuracy: 0.6500, Balanced Accuracy: 0.4867\n",
            "Fold 17/50\n",
            "Epoch 10/100, Loss: 0.6647713096053512\n",
            "Epoch 20/100, Loss: 0.574042209872493\n",
            "Epoch 30/100, Loss: 0.4426686399512821\n",
            "Epoch 40/100, Loss: 0.33817390711219225\n",
            "Epoch 50/100, Loss: 0.2555281983481513\n",
            "Epoch 60/100, Loss: 0.18059449091001792\n",
            "Epoch 70/100, Loss: 0.13960292466260768\n",
            "Epoch 80/100, Loss: 0.11587985577406706\n",
            "Epoch 90/100, Loss: 0.08400145145478072\n",
            "Epoch 100/100, Loss: 0.07233809569367657\n",
            "Fold 17 - Accuracy: 0.7857, Balanced Accuracy: 0.6893\n",
            "Fold 18/50\n",
            "Epoch 10/100, Loss: 0.6798654331101311\n",
            "Epoch 20/100, Loss: 0.6173687201959116\n",
            "Epoch 30/100, Loss: 0.5051489880791417\n",
            "Epoch 40/100, Loss: 0.3869802819357978\n",
            "Epoch 50/100, Loss: 0.2935369274130574\n",
            "Epoch 60/100, Loss: 0.21924883347970467\n",
            "Epoch 70/100, Loss: 0.18218870847313492\n",
            "Epoch 80/100, Loss: 0.1373571386491811\n",
            "Epoch 90/100, Loss: 0.11456334949643524\n",
            "Epoch 100/100, Loss: 0.0984370903008514\n",
            "Fold 18 - Accuracy: 0.7554, Balanced Accuracy: 0.6471\n",
            "Fold 19/50\n",
            "Epoch 10/100, Loss: 0.6832107614587855\n",
            "Epoch 20/100, Loss: 0.6401249722198203\n",
            "Epoch 30/100, Loss: 0.5460176589312377\n",
            "Epoch 40/100, Loss: 0.43945296163912173\n",
            "Epoch 50/100, Loss: 0.34417516545013144\n",
            "Epoch 60/100, Loss: 0.2779495346325415\n",
            "Epoch 70/100, Loss: 0.21223270175633607\n",
            "Epoch 80/100, Loss: 0.18446470383140776\n",
            "Epoch 90/100, Loss: 0.13838800815520463\n",
            "Epoch 100/100, Loss: 0.12084587470248893\n",
            "Fold 19 - Accuracy: 0.7122, Balanced Accuracy: 0.6308\n",
            "Fold 20/50\n",
            "Epoch 10/100, Loss: 0.6717008629015514\n",
            "Epoch 20/100, Loss: 0.5945907703467778\n",
            "Epoch 30/100, Loss: 0.48793203809431623\n",
            "Epoch 40/100, Loss: 0.36320372351578306\n",
            "Epoch 50/100, Loss: 0.2791042607277632\n",
            "Epoch 60/100, Loss: 0.2297054161982877\n",
            "Epoch 70/100, Loss: 0.17899618590516703\n",
            "Epoch 80/100, Loss: 0.14527791259544237\n",
            "Epoch 90/100, Loss: 0.11752450279891491\n",
            "Epoch 100/100, Loss: 0.1025058241294963\n",
            "Fold 20 - Accuracy: 0.7698, Balanced Accuracy: 0.6856\n",
            "Fold 21/50\n",
            "Epoch 10/100, Loss: 0.6874889157436512\n",
            "Epoch 20/100, Loss: 0.6306629997712595\n",
            "Epoch 30/100, Loss: 0.5278798849494369\n",
            "Epoch 40/100, Loss: 0.4155688120259179\n",
            "Epoch 50/100, Loss: 0.319407821253494\n",
            "Epoch 60/100, Loss: 0.2464356897053895\n",
            "Epoch 70/100, Loss: 0.20454146519855218\n",
            "Epoch 80/100, Loss: 0.16752091381284925\n",
            "Epoch 90/100, Loss: 0.13462850037548277\n",
            "Epoch 100/100, Loss: 0.11246464967175766\n",
            "Fold 21 - Accuracy: 0.7500, Balanced Accuracy: 0.6619\n",
            "Fold 22/50\n",
            "Epoch 10/100, Loss: 0.6884607142872281\n",
            "Epoch 20/100, Loss: 0.6349837426786069\n",
            "Epoch 30/100, Loss: 0.5254152803509323\n",
            "Epoch 40/100, Loss: 0.4017326942196599\n",
            "Epoch 50/100, Loss: 0.2990579511280413\n",
            "Epoch 60/100, Loss: 0.24275722547813697\n",
            "Epoch 70/100, Loss: 0.18814888541345243\n",
            "Epoch 80/100, Loss: 0.1379652371009191\n",
            "Epoch 90/100, Loss: 0.12703413364511948\n",
            "Epoch 100/100, Loss: 0.09646657688750161\n",
            "Fold 22 - Accuracy: 0.6571, Balanced Accuracy: 0.5636\n",
            "Fold 23/50\n",
            "Epoch 10/100, Loss: 0.6771147118674384\n",
            "Epoch 20/100, Loss: 0.6061575589356599\n",
            "Epoch 30/100, Loss: 0.4862267761318772\n",
            "Epoch 40/100, Loss: 0.3706409986372347\n",
            "Epoch 50/100, Loss: 0.28196924262576634\n",
            "Epoch 60/100, Loss: 0.22060225352093024\n",
            "Epoch 70/100, Loss: 0.18706192517722095\n",
            "Epoch 80/100, Loss: 0.1509157148776231\n",
            "Epoch 90/100, Loss: 0.11228784946379838\n",
            "Epoch 100/100, Loss: 0.09232843239550237\n",
            "Fold 23 - Accuracy: 0.7122, Balanced Accuracy: 0.5897\n",
            "Fold 24/50\n",
            "Epoch 10/100, Loss: 0.6776769691043429\n",
            "Epoch 20/100, Loss: 0.6145957951192502\n",
            "Epoch 30/100, Loss: 0.49838311032012655\n",
            "Epoch 40/100, Loss: 0.3843478383841338\n",
            "Epoch 50/100, Loss: 0.3026261969848915\n",
            "Epoch 60/100, Loss: 0.23271914737092125\n",
            "Epoch 70/100, Loss: 0.18543195917650504\n",
            "Epoch 80/100, Loss: 0.15418481661213768\n",
            "Epoch 90/100, Loss: 0.1296178698539734\n",
            "Epoch 100/100, Loss: 0.11115936258876766\n",
            "Fold 24 - Accuracy: 0.7626, Balanced Accuracy: 0.6402\n",
            "Fold 25/50\n",
            "Epoch 10/100, Loss: 0.6840494126081467\n",
            "Epoch 20/100, Loss: 0.6299208785806384\n",
            "Epoch 30/100, Loss: 0.5165397280028888\n",
            "Epoch 40/100, Loss: 0.39764230272599627\n",
            "Epoch 50/100, Loss: 0.29373674946171896\n",
            "Epoch 60/100, Loss: 0.2399523604129042\n",
            "Epoch 70/100, Loss: 0.18182957864233426\n",
            "Epoch 80/100, Loss: 0.14326344457055842\n",
            "Epoch 90/100, Loss: 0.11869495681353978\n",
            "Epoch 100/100, Loss: 0.09594573689225529\n",
            "Fold 25 - Accuracy: 0.7194, Balanced Accuracy: 0.6564\n",
            "Fold 26/50\n",
            "Epoch 10/100, Loss: 0.6802836678646229\n",
            "Epoch 20/100, Loss: 0.6105248994297452\n",
            "Epoch 30/100, Loss: 0.48130303069397257\n",
            "Epoch 40/100, Loss: 0.35992507029462745\n",
            "Epoch 50/100, Loss: 0.2624620188165594\n",
            "Epoch 60/100, Loss: 0.20351779598880698\n",
            "Epoch 70/100, Loss: 0.1515019447715194\n",
            "Epoch 80/100, Loss: 0.11332785269176518\n",
            "Epoch 90/100, Loss: 0.08687496557831764\n",
            "Epoch 100/100, Loss: 0.07687120153396218\n",
            "Fold 26 - Accuracy: 0.6714, Balanced Accuracy: 0.5433\n",
            "Fold 27/50\n",
            "Epoch 10/100, Loss: 0.6737205915980868\n",
            "Epoch 20/100, Loss: 0.5957243817823904\n",
            "Epoch 30/100, Loss: 0.4805198035858296\n",
            "Epoch 40/100, Loss: 0.3765911316430127\n",
            "Epoch 50/100, Loss: 0.29090517814512606\n",
            "Epoch 60/100, Loss: 0.23876842690838707\n",
            "Epoch 70/100, Loss: 0.18305708605934073\n",
            "Epoch 80/100, Loss: 0.14075978321057778\n",
            "Epoch 90/100, Loss: 0.12115680150411746\n",
            "Epoch 100/100, Loss: 0.10549548765023549\n",
            "Fold 27 - Accuracy: 0.7500, Balanced Accuracy: 0.6688\n",
            "Fold 28/50\n",
            "Epoch 10/100, Loss: 0.6810175776481628\n",
            "Epoch 20/100, Loss: 0.6326592078915348\n",
            "Epoch 30/100, Loss: 0.548571585505097\n",
            "Epoch 40/100, Loss: 0.4448379576206207\n",
            "Epoch 50/100, Loss: 0.3537949765170062\n",
            "Epoch 60/100, Loss: 0.27988984242633536\n",
            "Epoch 70/100, Loss: 0.2145197590192159\n",
            "Epoch 80/100, Loss: 0.16676528724255385\n",
            "Epoch 90/100, Loss: 0.13887187176280552\n",
            "Epoch 100/100, Loss: 0.11657751365392297\n",
            "Fold 28 - Accuracy: 0.8129, Balanced Accuracy: 0.7597\n",
            "Fold 29/50\n",
            "Epoch 10/100, Loss: 0.67354916036129\n",
            "Epoch 20/100, Loss: 0.6088802112000329\n",
            "Epoch 30/100, Loss: 0.4994667874915259\n",
            "Epoch 40/100, Loss: 0.37715798722846167\n",
            "Epoch 50/100, Loss: 0.29271422645875383\n",
            "Epoch 60/100, Loss: 0.24654760211706161\n",
            "Epoch 70/100, Loss: 0.19285879603454045\n",
            "Epoch 80/100, Loss: 0.1537406471158777\n",
            "Epoch 90/100, Loss: 0.11873412637838296\n",
            "Epoch 100/100, Loss: 0.09416724807981934\n",
            "Fold 29 - Accuracy: 0.6978, Balanced Accuracy: 0.6361\n",
            "Fold 30/50\n",
            "Epoch 10/100, Loss: 0.6803519063525729\n",
            "Epoch 20/100, Loss: 0.6106010609202914\n",
            "Epoch 30/100, Loss: 0.486476746974168\n",
            "Epoch 40/100, Loss: 0.3706771289860761\n",
            "Epoch 50/100, Loss: 0.2881863122736966\n",
            "Epoch 60/100, Loss: 0.21744461081646108\n",
            "Epoch 70/100, Loss: 0.17701439145538542\n",
            "Epoch 80/100, Loss: 0.14248408459954792\n",
            "Epoch 90/100, Loss: 0.1177881920227298\n",
            "Epoch 100/100, Loss: 0.1026089579142906\n",
            "Fold 30 - Accuracy: 0.7338, Balanced Accuracy: 0.6231\n",
            "Fold 31/50\n",
            "Epoch 10/100, Loss: 0.6806901970079967\n",
            "Epoch 20/100, Loss: 0.6271461801869529\n",
            "Epoch 30/100, Loss: 0.5395239517092705\n",
            "Epoch 40/100, Loss: 0.4248959007007735\n",
            "Epoch 50/100, Loss: 0.3117604819791658\n",
            "Epoch 60/100, Loss: 0.2512685608650957\n",
            "Epoch 70/100, Loss: 0.1829650822494711\n",
            "Epoch 80/100, Loss: 0.15455761179327965\n",
            "Epoch 90/100, Loss: 0.12591048276850156\n",
            "Epoch 100/100, Loss: 0.09738568269780704\n",
            "Fold 31 - Accuracy: 0.7286, Balanced Accuracy: 0.6293\n",
            "Fold 32/50\n",
            "Epoch 10/100, Loss: 0.688730917595051\n",
            "Epoch 20/100, Loss: 0.640559991200765\n",
            "Epoch 30/100, Loss: 0.541610163670999\n",
            "Epoch 40/100, Loss: 0.4259372960638117\n",
            "Epoch 50/100, Loss: 0.3328213619965094\n",
            "Epoch 60/100, Loss: 0.26705943434326734\n",
            "Epoch 70/100, Loss: 0.21204229140723194\n",
            "Epoch 80/100, Loss: 0.1727956180219297\n",
            "Epoch 90/100, Loss: 0.14153476776900115\n",
            "Epoch 100/100, Loss: 0.12206291600509926\n",
            "Fold 32 - Accuracy: 0.7643, Balanced Accuracy: 0.6920\n",
            "Fold 33/50\n",
            "Epoch 10/100, Loss: 0.6752497752507528\n",
            "Epoch 20/100, Loss: 0.608320207507522\n",
            "Epoch 30/100, Loss: 0.4949256545967526\n",
            "Epoch 40/100, Loss: 0.3916940457291073\n",
            "Epoch 50/100, Loss: 0.301068890425894\n",
            "Epoch 60/100, Loss: 0.23673214239102822\n",
            "Epoch 70/100, Loss: 0.1883996945840341\n",
            "Epoch 80/100, Loss: 0.1510864038158346\n",
            "Epoch 90/100, Loss: 0.11458522826433182\n",
            "Epoch 100/100, Loss: 0.11061077537360015\n",
            "Fold 33 - Accuracy: 0.7266, Balanced Accuracy: 0.6500\n",
            "Fold 34/50\n",
            "Epoch 10/100, Loss: 0.6818987131118774\n",
            "Epoch 20/100, Loss: 0.6139884480723629\n",
            "Epoch 30/100, Loss: 0.49062014398751436\n",
            "Epoch 40/100, Loss: 0.3705905322675352\n",
            "Epoch 50/100, Loss: 0.2839636223183738\n",
            "Epoch 60/100, Loss: 0.20844639837741852\n",
            "Epoch 70/100, Loss: 0.16372089860615907\n",
            "Epoch 80/100, Loss: 0.12899103043255983\n",
            "Epoch 90/100, Loss: 0.10234728524530376\n",
            "Epoch 100/100, Loss: 0.07883311007861737\n",
            "Fold 34 - Accuracy: 0.8201, Balanced Accuracy: 0.7213\n",
            "Fold 35/50\n",
            "Epoch 10/100, Loss: 0.6744465320198624\n",
            "Epoch 20/100, Loss: 0.5959327176765159\n",
            "Epoch 30/100, Loss: 0.4718808520723272\n",
            "Epoch 40/100, Loss: 0.36182909100143995\n",
            "Epoch 50/100, Loss: 0.27177565903575335\n",
            "Epoch 60/100, Loss: 0.22072156380724023\n",
            "Epoch 70/100, Loss: 0.18187923922582908\n",
            "Epoch 80/100, Loss: 0.1395545988171189\n",
            "Epoch 90/100, Loss: 0.1190956273564586\n",
            "Epoch 100/100, Loss: 0.10563684144505749\n",
            "Fold 35 - Accuracy: 0.7194, Balanced Accuracy: 0.5942\n",
            "Fold 36/50\n",
            "Epoch 10/100, Loss: 0.6796741728429441\n",
            "Epoch 20/100, Loss: 0.61703536245558\n",
            "Epoch 30/100, Loss: 0.5097172911520358\n",
            "Epoch 40/100, Loss: 0.3954729128766943\n",
            "Epoch 50/100, Loss: 0.30280136675746355\n",
            "Epoch 60/100, Loss: 0.23431166951303128\n",
            "Epoch 70/100, Loss: 0.17785474630417647\n",
            "Epoch 80/100, Loss: 0.14604715175098842\n",
            "Epoch 90/100, Loss: 0.1202686917450693\n",
            "Epoch 100/100, Loss: 0.10083309878353719\n",
            "Fold 36 - Accuracy: 0.8000, Balanced Accuracy: 0.6881\n",
            "Fold 37/50\n",
            "Epoch 10/100, Loss: 0.6780826692227964\n",
            "Epoch 20/100, Loss: 0.6233661152698375\n",
            "Epoch 30/100, Loss: 0.5234952878068995\n",
            "Epoch 40/100, Loss: 0.39624231722619796\n",
            "Epoch 50/100, Loss: 0.31823229955302346\n",
            "Epoch 60/100, Loss: 0.24480148487620884\n",
            "Epoch 70/100, Loss: 0.20577197356356514\n",
            "Epoch 80/100, Loss: 0.15827975256575477\n",
            "Epoch 90/100, Loss: 0.12624706176144104\n",
            "Epoch 100/100, Loss: 0.10804284391579805\n",
            "Fold 37 - Accuracy: 0.7786, Balanced Accuracy: 0.6562\n",
            "Fold 38/50\n",
            "Epoch 10/100, Loss: 0.6829653978347778\n",
            "Epoch 20/100, Loss: 0.6314360733543124\n",
            "Epoch 30/100, Loss: 0.5152292443173272\n",
            "Epoch 40/100, Loss: 0.40094232239893507\n",
            "Epoch 50/100, Loss: 0.31324821231620653\n",
            "Epoch 60/100, Loss: 0.24041097025786126\n",
            "Epoch 70/100, Loss: 0.19682468633566583\n",
            "Epoch 80/100, Loss: 0.16149048347558295\n",
            "Epoch 90/100, Loss: 0.12572420814207622\n",
            "Epoch 100/100, Loss: 0.10269661247730255\n",
            "Fold 38 - Accuracy: 0.6906, Balanced Accuracy: 0.6208\n",
            "Fold 39/50\n",
            "Epoch 10/100, Loss: 0.6785591436283929\n",
            "Epoch 20/100, Loss: 0.6115820024694715\n",
            "Epoch 30/100, Loss: 0.5048981606960297\n",
            "Epoch 40/100, Loss: 0.39432922431400846\n",
            "Epoch 50/100, Loss: 0.30419824591704775\n",
            "Epoch 60/100, Loss: 0.23492687248757907\n",
            "Epoch 70/100, Loss: 0.18532876803406648\n",
            "Epoch 80/100, Loss: 0.14448399502517922\n",
            "Epoch 90/100, Loss: 0.12510791075016772\n",
            "Epoch 100/100, Loss: 0.10962311990026917\n",
            "Fold 39 - Accuracy: 0.6906, Balanced Accuracy: 0.6084\n",
            "Fold 40/50\n",
            "Epoch 10/100, Loss: 0.6843821737501357\n",
            "Epoch 20/100, Loss: 0.6419215180255748\n",
            "Epoch 30/100, Loss: 0.5366609549080884\n",
            "Epoch 40/100, Loss: 0.421660253295192\n",
            "Epoch 50/100, Loss: 0.3322443531619178\n",
            "Epoch 60/100, Loss: 0.26033658341125204\n",
            "Epoch 70/100, Loss: 0.21349125659024273\n",
            "Epoch 80/100, Loss: 0.16917364299297333\n",
            "Epoch 90/100, Loss: 0.14241657295712717\n",
            "Epoch 100/100, Loss: 0.11401467687553829\n",
            "Fold 40 - Accuracy: 0.7698, Balanced Accuracy: 0.6659\n",
            "Fold 41/50\n",
            "Epoch 10/100, Loss: 0.6810585304542824\n",
            "Epoch 20/100, Loss: 0.6206975844171312\n",
            "Epoch 30/100, Loss: 0.49925058748986983\n",
            "Epoch 40/100, Loss: 0.380618205776921\n",
            "Epoch 50/100, Loss: 0.30935405249948855\n",
            "Epoch 60/100, Loss: 0.2342796093887753\n",
            "Epoch 70/100, Loss: 0.1924227379538395\n",
            "Epoch 80/100, Loss: 0.1589985685767951\n",
            "Epoch 90/100, Loss: 0.13630897027474861\n",
            "Epoch 100/100, Loss: 0.10771703706295402\n",
            "Fold 41 - Accuracy: 0.7214, Balanced Accuracy: 0.6362\n",
            "Fold 42/50\n",
            "Epoch 10/100, Loss: 0.6799586415290833\n",
            "Epoch 20/100, Loss: 0.6193771494759454\n",
            "Epoch 30/100, Loss: 0.5143947148764575\n",
            "Epoch 40/100, Loss: 0.38947661607353773\n",
            "Epoch 50/100, Loss: 0.31079059176974827\n",
            "Epoch 60/100, Loss: 0.2356326193721206\n",
            "Epoch 70/100, Loss: 0.1892533473394535\n",
            "Epoch 80/100, Loss: 0.15554928282896677\n",
            "Epoch 90/100, Loss: 0.12573708124734737\n",
            "Epoch 100/100, Loss: 0.0980718385566164\n",
            "Fold 42 - Accuracy: 0.7000, Balanced Accuracy: 0.6056\n",
            "Fold 43/50\n",
            "Epoch 10/100, Loss: 0.6690326244742782\n",
            "Epoch 20/100, Loss: 0.5885993794158653\n",
            "Epoch 30/100, Loss: 0.47782462062659087\n",
            "Epoch 40/100, Loss: 0.37457859074627914\n",
            "Epoch 50/100, Loss: 0.300945273703999\n",
            "Epoch 60/100, Loss: 0.2288014022288499\n",
            "Epoch 70/100, Loss: 0.18312092070226316\n",
            "Epoch 80/100, Loss: 0.14290935104643857\n",
            "Epoch 90/100, Loss: 0.12495108003969546\n",
            "Epoch 100/100, Loss: 0.10551462101715582\n",
            "Fold 43 - Accuracy: 0.6978, Balanced Accuracy: 0.6285\n",
            "Fold 44/50\n",
            "Epoch 10/100, Loss: 0.6773406774909408\n",
            "Epoch 20/100, Loss: 0.6044832865397135\n",
            "Epoch 30/100, Loss: 0.49451649961648164\n",
            "Epoch 40/100, Loss: 0.38308288874449553\n",
            "Epoch 50/100, Loss: 0.2997059766893034\n",
            "Epoch 60/100, Loss: 0.21556724276807573\n",
            "Epoch 70/100, Loss: 0.1633952596673259\n",
            "Epoch 80/100, Loss: 0.13100649637204628\n",
            "Epoch 90/100, Loss: 0.10593313406463023\n",
            "Epoch 100/100, Loss: 0.08173044826145526\n",
            "Fold 44 - Accuracy: 0.7626, Balanced Accuracy: 0.6211\n",
            "Fold 45/50\n",
            "Epoch 10/100, Loss: 0.6823561125331454\n",
            "Epoch 20/100, Loss: 0.6344122732127154\n",
            "Epoch 30/100, Loss: 0.5322061744001176\n",
            "Epoch 40/100, Loss: 0.3986980352136824\n",
            "Epoch 50/100, Loss: 0.302419979263235\n",
            "Epoch 60/100, Loss: 0.23026808047736133\n",
            "Epoch 70/100, Loss: 0.17745534109848518\n",
            "Epoch 80/100, Loss: 0.15490236630042395\n",
            "Epoch 90/100, Loss: 0.12447443742443014\n",
            "Epoch 100/100, Loss: 0.09729042152563731\n",
            "Fold 45 - Accuracy: 0.7410, Balanced Accuracy: 0.6695\n",
            "Fold 46/50\n",
            "Epoch 10/100, Loss: 0.6814732706105268\n",
            "Epoch 20/100, Loss: 0.6315393293345416\n",
            "Epoch 30/100, Loss: 0.5134841137462192\n",
            "Epoch 40/100, Loss: 0.39022500316301983\n",
            "Epoch 50/100, Loss: 0.3052283486834279\n",
            "Epoch 60/100, Loss: 0.23250527735109683\n",
            "Epoch 70/100, Loss: 0.17816175685988533\n",
            "Epoch 80/100, Loss: 0.14778660155004925\n",
            "Epoch 90/100, Loss: 0.11900512142865746\n",
            "Epoch 100/100, Loss: 0.09988194110768812\n",
            "Fold 46 - Accuracy: 0.7714, Balanced Accuracy: 0.6485\n",
            "Fold 47/50\n",
            "Epoch 10/100, Loss: 0.674281586099554\n",
            "Epoch 20/100, Loss: 0.5930697895862438\n",
            "Epoch 30/100, Loss: 0.48570695519447327\n",
            "Epoch 40/100, Loss: 0.3760323138148696\n",
            "Epoch 50/100, Loss: 0.2940703133742015\n",
            "Epoch 60/100, Loss: 0.23109041264763586\n",
            "Epoch 70/100, Loss: 0.1819109591069045\n",
            "Epoch 80/100, Loss: 0.14334468102013623\n",
            "Epoch 90/100, Loss: 0.11421029228303167\n",
            "Epoch 100/100, Loss: 0.09401918588965028\n",
            "Fold 47 - Accuracy: 0.7143, Balanced Accuracy: 0.6316\n",
            "Fold 48/50\n",
            "Epoch 10/100, Loss: 0.6750051741089139\n",
            "Epoch 20/100, Loss: 0.5963341806616101\n",
            "Epoch 30/100, Loss: 0.49046031066349577\n",
            "Epoch 40/100, Loss: 0.3776076819215502\n",
            "Epoch 50/100, Loss: 0.30861117584364756\n",
            "Epoch 60/100, Loss: 0.2334369234740734\n",
            "Epoch 70/100, Loss: 0.17383482147540366\n",
            "Epoch 80/100, Loss: 0.1321686522236892\n",
            "Epoch 90/100, Loss: 0.10698569566011429\n",
            "Epoch 100/100, Loss: 0.08321191410401038\n",
            "Fold 48 - Accuracy: 0.7410, Balanced Accuracy: 0.6727\n",
            "Fold 49/50\n",
            "Epoch 10/100, Loss: 0.6793175096865054\n",
            "Epoch 20/100, Loss: 0.6184647237813031\n",
            "Epoch 30/100, Loss: 0.49844553625142135\n",
            "Epoch 40/100, Loss: 0.3813748635627605\n",
            "Epoch 50/100, Loss: 0.29818467519901415\n",
            "Epoch 60/100, Loss: 0.23303646511501735\n",
            "Epoch 70/100, Loss: 0.17892915810699817\n",
            "Epoch 80/100, Loss: 0.14977865307419388\n",
            "Epoch 90/100, Loss: 0.12070245599305188\n",
            "Epoch 100/100, Loss: 0.09863477738367186\n",
            "Fold 49 - Accuracy: 0.7554, Balanced Accuracy: 0.6831\n",
            "Fold 50/50\n",
            "Epoch 10/100, Loss: 0.6781586254084552\n",
            "Epoch 20/100, Loss: 0.6190690309913071\n",
            "Epoch 30/100, Loss: 0.5084006565588491\n",
            "Epoch 40/100, Loss: 0.4011557212582341\n",
            "Epoch 50/100, Loss: 0.3128110595323421\n",
            "Epoch 60/100, Loss: 0.2368231564760208\n",
            "Epoch 70/100, Loss: 0.18898275053059613\n",
            "Epoch 80/100, Loss: 0.14743453788536567\n",
            "Epoch 90/100, Loss: 0.12149712277783288\n",
            "Epoch 100/100, Loss: 0.10342587682384032\n",
            "Fold 50 - Accuracy: 0.7698, Balanced Accuracy: 0.6449\n",
            "\n",
            "Average Metrics:\n",
            "Accuracy: 0.7350 ± 0.0408\n",
            "Balanced_accuracy: 0.6370 ± 0.0481\n",
            "Precision: 0.8315 ± 0.0366\n",
            "Recall: 0.8209 ± 0.0402\n",
            "F1: 0.8255 ± 0.0305\n",
            "Auc: 0.7287 ± 0.0431\n",
            "Kappa: 0.2678 ± 0.0931\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
          ]
        }
      ],
      "source": [
        "#Different sampling techniques using cross validation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, SubsetRandomSampler\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, roc_curve\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "\n",
        "class DenseModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, dropout_rate=0.5):\n",
        "        super(DenseModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def train_dense_model(model, train_loader, criterion, optimizer, epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_preds).flatten(), np.array(all_labels).flatten()\n",
        "\n",
        "\n",
        "def cross_validate_dense_model(X, y, input_dim, hidden_dim1, hidden_dim2, output_dim, criterion,\n",
        "                               n_splits=5, n_repeats=10, epochs=100, batch_size=32, sampling_method='smote'):\n",
        "    cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n",
        "    metrics = {\n",
        "        'accuracy': [], 'balanced_accuracy': [], 'precision': [],\n",
        "        'recall': [], 'f1': [], 'auc': [], 'kappa': []\n",
        "    }\n",
        "    all_roc_data = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X)):\n",
        "        print(f\"Fold {fold + 1}/{n_splits * n_repeats}\")\n",
        "\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Apply sampling method\n",
        "        if sampling_method == 'ros':\n",
        "            sampler = RandomOverSampler(random_state=42)\n",
        "        elif sampling_method == 'rus':\n",
        "            sampler = RandomUnderSampler(random_state=42)\n",
        "        elif sampling_method == 'smote':\n",
        "            sampler = SMOTE(random_state=42)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid sampling method. Choose 'ros', 'rus', or 'smote'.\")\n",
        "\n",
        "        X_train_resampled, y_train_resampled = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        train_dataset = TensorDataset(torch.FloatTensor(X_train_resampled), torch.FloatTensor(y_train_resampled))\n",
        "        val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "        model = DenseModel(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
        "        optimizer = optim.Adam(model.parameters(), lr= 0.0001)\n",
        "\n",
        "        train_dense_model(model, train_loader, criterion, optimizer, epochs)\n",
        "\n",
        "        val_preds, val_labels = evaluate_model(model, val_loader)\n",
        "\n",
        "        metrics['accuracy'].append(accuracy_score(val_labels, val_preds > 0.5))\n",
        "        metrics['balanced_accuracy'].append(balanced_accuracy_score(val_labels, val_preds > 0.5))\n",
        "        metrics['precision'].append(precision_score(val_labels, val_preds > 0.5))\n",
        "        metrics['recall'].append(recall_score(val_labels, val_preds > 0.5))\n",
        "        metrics['f1'].append(f1_score(val_labels, val_preds > 0.5))\n",
        "        metrics['auc'].append(roc_auc_score(val_labels, val_preds))\n",
        "        metrics['kappa'].append(cohen_kappa_score(val_labels, val_preds > 0.5))\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(val_labels, val_preds)\n",
        "        all_roc_data.append((fpr, tpr, metrics['auc'][-1]))\n",
        "\n",
        "        print(f\"Fold {fold + 1} - Accuracy: {metrics['accuracy'][-1]:.4f}, Balanced Accuracy: {metrics['balanced_accuracy'][-1]:.4f}\")\n",
        "\n",
        "    avg_metrics = {metric: np.mean(scores) for metric, scores in metrics.items()}\n",
        "    std_metrics = {metric: np.std(scores) for metric, scores in metrics.items()}\n",
        "\n",
        "    print(\"\\nAverage Metrics:\")\n",
        "    for metric, value in avg_metrics.items():\n",
        "        print(f\"{metric.capitalize()}: {value:.4f} ± {std_metrics[metric]:.4f}\")\n",
        "\n",
        "    # Plot ROC curves\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "    mean_tpr = np.zeros_like(mean_fpr)\n",
        "    for roc in all_roc_data:\n",
        "        fpr, tpr, auc_score = roc\n",
        "        mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
        "    mean_tpr /= len(all_roc_data)\n",
        "    mean_auc = np.mean([roc[2] for roc in all_roc_data])\n",
        "    plt.plot(mean_fpr, mean_tpr, lw=2, label=f'Mean ROC (AUC = {mean_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='r', lw=2, label='Random Guess')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=24)\n",
        "    plt.ylabel('True Positive Rate', fontsize=24)\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=24)\n",
        "    plt.legend(loc=\"lower right\", fontsize=18)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/MyDrive/HOB/Rerun/roc_curve_mol.png', format='png', bbox_inches='tight')\n",
        "    plt.savefig('/content/drive/MyDrive/HOB/Rerun/roc_curve_mol.eps', format='eps', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return avg_metrics, std_metrics\n",
        "\n",
        "# Usage\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "hidden_dim1 = 32\n",
        "hidden_dim2 = 16\n",
        "output_dim = 1\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "avg_metrics, std_metrics = cross_validate_dense_model(\n",
        "    X_train_tensor.numpy(), y_train_tensor.numpy(),\n",
        "    input_dim, hidden_dim1, hidden_dim2, output_dim, criterion,\n",
        "    n_splits=5, n_repeats=10, epochs=100, batch_size=32, sampling_method='smote'\n",
        ")\n",
        "\n",
        "save_metrics_to_excel(avg_metrics, std_metrics, '/content/drive/MyDrive/HOB/Rerun/metrics.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2199ANAXuaF",
        "outputId": "2720e76b-b32a-48e8-e60f-d9b5cc4e644e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run 1/10\n",
            "Epoch 10/100, Loss: 0.4739733257076957\n",
            "Epoch 20/100, Loss: 0.4651192453774539\n",
            "Epoch 30/100, Loss: 0.4574689946391366\n",
            "Epoch 40/100, Loss: 0.43904648450287903\n",
            "Epoch 50/100, Loss: 0.44070514088327234\n",
            "Epoch 60/100, Loss: 0.4320393909107555\n",
            "Epoch 70/100, Loss: 0.4420002644712275\n",
            "Epoch 80/100, Loss: 0.4211132973432541\n",
            "Epoch 90/100, Loss: 0.41651548919352616\n",
            "Epoch 100/100, Loss: 0.4239954244006764\n",
            "Run 2/10\n",
            "Epoch 10/100, Loss: 0.4761085103858601\n",
            "Epoch 20/100, Loss: 0.4551822150295431\n",
            "Epoch 30/100, Loss: 0.481801145456054\n",
            "Epoch 40/100, Loss: 0.44245425137606537\n",
            "Epoch 50/100, Loss: 0.4314325736327605\n",
            "Epoch 60/100, Loss: 0.41641678593375464\n",
            "Epoch 70/100, Loss: 0.44077564775943756\n",
            "Epoch 80/100, Loss: 0.40816370194608514\n",
            "Epoch 90/100, Loss: 0.40381878208030353\n",
            "Epoch 100/100, Loss: 0.4141152873635292\n",
            "Run 3/10\n",
            "Epoch 10/100, Loss: 0.484448949044401\n",
            "Epoch 20/100, Loss: 0.45472208342768927\n",
            "Epoch 30/100, Loss: 0.46384744210676715\n",
            "Epoch 40/100, Loss: 0.4300312873992053\n",
            "Epoch 50/100, Loss: 0.4459521607919173\n",
            "Epoch 60/100, Loss: 0.4259538149291819\n",
            "Epoch 70/100, Loss: 0.41820523752407596\n",
            "Epoch 80/100, Loss: 0.42773527042432263\n",
            "Epoch 90/100, Loss: 0.4322235773910176\n",
            "Epoch 100/100, Loss: 0.42328387905250897\n",
            "Run 4/10\n",
            "Epoch 10/100, Loss: 0.46390876173973083\n",
            "Epoch 20/100, Loss: 0.46605887738141144\n",
            "Epoch 30/100, Loss: 0.46113165332512424\n",
            "Epoch 40/100, Loss: 0.4590070660818707\n",
            "Epoch 50/100, Loss: 0.43454544923522254\n",
            "Epoch 60/100, Loss: 0.42864690585569903\n",
            "Epoch 70/100, Loss: 0.43082544749433344\n",
            "Epoch 80/100, Loss: 0.40952410752123053\n",
            "Epoch 90/100, Loss: 0.41195597973736847\n",
            "Epoch 100/100, Loss: 0.41140393167734146\n",
            "Run 5/10\n",
            "Epoch 10/100, Loss: 0.47076268765059387\n",
            "Epoch 20/100, Loss: 0.45947688682512805\n",
            "Epoch 30/100, Loss: 0.4495889747684652\n",
            "Epoch 40/100, Loss: 0.42862497744235123\n",
            "Epoch 50/100, Loss: 0.43846380710601807\n",
            "Epoch 60/100, Loss: 0.4284810572862625\n",
            "Epoch 70/100, Loss: 0.4243419427763332\n",
            "Epoch 80/100, Loss: 0.42360781268639996\n",
            "Epoch 90/100, Loss: 0.41435909271240234\n",
            "Epoch 100/100, Loss: 0.42612178962339053\n",
            "Run 6/10\n",
            "Epoch 10/100, Loss: 0.4808391319079833\n",
            "Epoch 20/100, Loss: 0.4659843092614954\n",
            "Epoch 30/100, Loss: 0.46431853012605145\n",
            "Epoch 40/100, Loss: 0.4292105869813399\n",
            "Epoch 50/100, Loss: 0.4509647488594055\n",
            "Epoch 60/100, Loss: 0.43090988289226184\n",
            "Epoch 70/100, Loss: 0.42724044688723306\n",
            "Epoch 80/100, Loss: 0.41137693145058374\n",
            "Epoch 90/100, Loss: 0.4327141303907741\n",
            "Epoch 100/100, Loss: 0.4131856736811725\n",
            "Run 7/10\n",
            "Epoch 10/100, Loss: 0.4870170287110589\n",
            "Epoch 20/100, Loss: 0.4670578173615716\n",
            "Epoch 30/100, Loss: 0.44857383790341293\n",
            "Epoch 40/100, Loss: 0.43521384488452564\n",
            "Epoch 50/100, Loss: 0.4424265446988019\n",
            "Epoch 60/100, Loss: 0.4273517524654215\n",
            "Epoch 70/100, Loss: 0.4361877143383026\n",
            "Epoch 80/100, Loss: 0.4128850129517642\n",
            "Epoch 90/100, Loss: 0.4023397361690348\n",
            "Epoch 100/100, Loss: 0.41015339439565485\n",
            "Run 8/10\n",
            "Epoch 10/100, Loss: 0.47451175342906604\n",
            "Epoch 20/100, Loss: 0.4572697308930484\n",
            "Epoch 30/100, Loss: 0.45474529334089975\n",
            "Epoch 40/100, Loss: 0.42623440447178756\n",
            "Epoch 50/100, Loss: 0.46121200106360694\n",
            "Epoch 60/100, Loss: 0.44637060029940173\n",
            "Epoch 70/100, Loss: 0.4568030366843397\n",
            "Epoch 80/100, Loss: 0.4282872595570304\n",
            "Epoch 90/100, Loss: 0.4276284900578586\n",
            "Epoch 100/100, Loss: 0.4252268042076718\n",
            "Run 9/10\n",
            "Epoch 10/100, Loss: 0.46814200011166657\n",
            "Epoch 20/100, Loss: 0.46133503859693353\n",
            "Epoch 30/100, Loss: 0.4529193477197127\n",
            "Epoch 40/100, Loss: 0.44668202237649396\n",
            "Epoch 50/100, Loss: 0.4370035380125046\n",
            "Epoch 60/100, Loss: 0.4616162831133062\n",
            "Epoch 70/100, Loss: 0.4380376379598271\n",
            "Epoch 80/100, Loss: 0.4300025538964705\n",
            "Epoch 90/100, Loss: 0.4134237732399594\n",
            "Epoch 100/100, Loss: 0.43126646226102655\n",
            "Run 10/10\n",
            "Epoch 10/100, Loss: 0.48437416418032214\n",
            "Epoch 20/100, Loss: 0.4849254814061252\n",
            "Epoch 30/100, Loss: 0.45937225358052686\n",
            "Epoch 40/100, Loss: 0.4541663581674749\n",
            "Epoch 50/100, Loss: 0.45516424016519025\n",
            "Epoch 60/100, Loss: 0.44785440780899743\n",
            "Epoch 70/100, Loss: 0.4457994306629354\n",
            "Epoch 80/100, Loss: 0.4413772184740413\n",
            "Epoch 90/100, Loss: 0.42690052633935754\n",
            "Epoch 100/100, Loss: 0.41698080707680096\n",
            "\n",
            "Average Metrics:\n",
            "Accuracy: 0.6444 ± 0.0419\n",
            "Balanced_accuracy: 0.6265 ± 0.0305\n",
            "Precision: 0.7223 ± 0.0265\n",
            "Recall: 0.7000 ± 0.1094\n",
            "F1: 0.7066 ± 0.0519\n",
            "Auc: 0.6718 ± 0.0119\n",
            "Kappa: 0.2520 ± 0.0660\n",
            "\n",
            "Average Results Dataframe:\n",
            "   accuracy  balanced_accuracy  precision  recall        f1       auc    kappa\n",
            "0  0.644444           0.626471   0.722276     0.7  0.706648  0.671849  0.25197\n",
            "\n",
            "Best Results Dataframe:\n",
            "   accuracy  balanced_accuracy  precision    recall      f1       auc  kappa\n",
            "0  0.733333           0.670168   0.722222  0.928571  0.8125  0.678571  0.375\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, roc_curve, classification_report\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Usage\n",
        "'''\n",
        "X_train_tensor = torch.tensor(atc_gnn_mol_features_train_df.values[:,:-1], dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(atc_gnn_mol_features_train_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(atc_gnn_mol_features_test_df.values[:,:-1], dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(atc_gnn_mol_features_test_df['Class'].values, dtype=torch.float32).view(-1, 1)'''\n",
        "\n",
        "X_train_tensor = torch.tensor(gnn_mol_features_train_df.values[:,:-1], dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(gnn_mol_features_train_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(gnn_mol_features_test_df.values[:,:-1], dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(gnn_mol_features_test_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "\n",
        "class DenseModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, dropout_rate=0.3):\n",
        "        super(DenseModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def train_dense_model(model, train_loader, criterion, optimizer, epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_preds).flatten(), np.array(all_labels).flatten()\n",
        "\n",
        "def run_experiment(X_train, y_train, X_test, y_test, input_dim, hidden_dim1, hidden_dim2, output_dim, criterion, epochs=100, batch_size=32):\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    model = DenseModel(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    train_dense_model(model, train_loader, criterion, optimizer, epochs)\n",
        "\n",
        "    test_preds, test_labels = evaluate_model(model, test_loader)\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(test_labels, test_preds > 0.5),\n",
        "        'balanced_accuracy': balanced_accuracy_score(test_labels, test_preds > 0.5),\n",
        "        'precision': precision_score(test_labels, test_preds > 0.5),\n",
        "        'recall': recall_score(test_labels, test_preds > 0.5),\n",
        "        'f1': f1_score(test_labels, test_preds > 0.5),\n",
        "        'auc': roc_auc_score(test_labels, test_preds),\n",
        "        'kappa': cohen_kappa_score(test_labels, test_preds > 0.5)\n",
        "    }\n",
        "\n",
        "    class_report = classification_report(test_labels, test_preds > 0.5, output_dict=True)\n",
        "\n",
        "    return metrics, class_report, model, test_preds, test_labels\n",
        "\n",
        "def main(X_train, y_train, X_test, y_test, input_dim, hidden_dim1, hidden_dim2, output_dim, criterion, n_runs=10, epochs=100, batch_size=32):\n",
        "    all_metrics = []\n",
        "    best_accuracy = 0\n",
        "    best_model = None\n",
        "    best_class_report = None\n",
        "    best_predictions = None\n",
        "    best_labels = None\n",
        "    best_metrics = None\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"Run {run + 1}/{n_runs}\")\n",
        "        metrics, class_report, model, test_preds, test_labels = run_experiment(X_train, y_train, X_test, y_test, input_dim, hidden_dim1, hidden_dim2, output_dim, criterion, epochs, batch_size)\n",
        "        all_metrics.append(metrics)\n",
        "\n",
        "        if metrics['accuracy'] > best_accuracy:\n",
        "            best_accuracy = metrics['accuracy']\n",
        "            best_model = model\n",
        "            best_class_report = class_report\n",
        "            best_predictions = test_preds\n",
        "            best_labels = test_labels\n",
        "            best_metrics = metrics\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_metrics = pd.DataFrame(all_metrics).mean().to_dict()\n",
        "    std_metrics = pd.DataFrame(all_metrics).std().to_dict()\n",
        "\n",
        "    # Create dataframes for average and best results\n",
        "    avg_df = pd.DataFrame([avg_metrics])\n",
        "    best_df = pd.DataFrame([best_metrics])\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs('/content/drive/MyDrive/HOB/Rerun', exist_ok=True)\n",
        "\n",
        "    # Save average results to CSV\n",
        "  #  avg_df.to_csv('/content/drive/MyDrive/HOB/average_results.csv', index=False)\n",
        "\n",
        "    # Save best results to CSV\n",
        " #   best_df.to_csv('/content/drive/MyDrive/HOB/best_results.csv', index=False)\n",
        "\n",
        "    # Save best predictions in a text file\n",
        " #   np.savetxt('/content/drive/MyDrive/HOB/best_predictions.txt', best_predictions, fmt='%.4f')\n",
        "\n",
        "    # Save best class-level results\n",
        "#    with open('/content/drive/MyDrive/HOB/best_class_results.txt', 'w') as f:\n",
        "  #      f.write(str(best_class_report))\n",
        "\n",
        "    # Print average results\n",
        "    print(\"\\nAverage Metrics:\")\n",
        "    for metric, value in avg_metrics.items():\n",
        "        print(f\"{metric.capitalize()}: {value:.4f} ± {std_metrics[metric]:.4f}\")\n",
        "\n",
        "    return avg_metrics, std_metrics, best_model, avg_df, best_df, best_predictions, best_labels\n",
        "\n",
        "\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "hidden_dim1 = 64\n",
        "hidden_dim2 = 64\n",
        "output_dim = 1\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "avg_metrics, std_metrics, best_model, avg_df, best_df, best_predictions, best_labels = main(\n",
        "    X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor,\n",
        "    input_dim, hidden_dim1, hidden_dim2, output_dim, criterion,\n",
        "    n_runs=10, epochs=100, batch_size=32\n",
        ")\n",
        "\n",
        "# Save the best model\n",
        "torch.save(best_model.state_dict(), '/content/drive/MyDrive/HOB/Rerun/best_model.pth')\n",
        "\n",
        "# Print dataframes\n",
        "print(\"\\nAverage Results Dataframe:\")\n",
        "print(avg_df)\n",
        "print(\"\\nBest Results Dataframe:\")\n",
        "print(best_df)\n",
        "\n",
        "# Create a dataframe with best predictions and true labels\n",
        "best_predictions_df = pd.DataFrame({\n",
        "    'True_Label': best_labels,\n",
        "    'Predicted_Probability': best_predictions,\n",
        "    'Predicted_Class': (best_predictions > 0.5).astype(int)\n",
        "})\n",
        "best_predictions_df.to_csv('/content/drive/MyDrive/HOB/Rerun/class_fm_run10.csv', index=False)\n",
        "avg_df.to_csv('/content/drive/MyDrive/HOB/Rerun/avg.csv', index=False)\n",
        "best_df.to_csv('/content/drive/MyDrive/HOB/Rerun/best.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExATwYtK4t63",
        "outputId": "e0717f91-8b6d-48c1-9a7d-cc8f199b886e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run 1/10\n",
            "Epoch 10/100, Loss: 0.6074029005625668\n",
            "Epoch 20/100, Loss: 0.5756185019717497\n",
            "Epoch 30/100, Loss: 0.5547844294239493\n",
            "Epoch 40/100, Loss: 0.5379039855564341\n",
            "Epoch 50/100, Loss: 0.5122784604044521\n",
            "Epoch 60/100, Loss: 0.4980377248104881\n",
            "Epoch 70/100, Loss: 0.49251872870851965\n",
            "Epoch 80/100, Loss: 0.4860232595135184\n",
            "Epoch 90/100, Loss: 0.48765866633723765\n",
            "Epoch 100/100, Loss: 0.4553864747285843\n",
            "Run 2/10\n",
            "Epoch 10/100, Loss: 0.604026652434293\n",
            "Epoch 20/100, Loss: 0.5698247490560308\n",
            "Epoch 30/100, Loss: 0.5398666420403648\n",
            "Epoch 40/100, Loss: 0.5438099372036317\n",
            "Epoch 50/100, Loss: 0.5224456418963039\n",
            "Epoch 60/100, Loss: 0.5149677886682398\n",
            "Epoch 70/100, Loss: 0.5233369715073529\n",
            "Epoch 80/100, Loss: 0.4939783522311379\n",
            "Epoch 90/100, Loss: 0.47895097469582276\n",
            "Epoch 100/100, Loss: 0.5076786577701569\n",
            "Run 3/10\n",
            "Epoch 10/100, Loss: 0.6030277434517356\n",
            "Epoch 20/100, Loss: 0.5726405504871818\n",
            "Epoch 30/100, Loss: 0.5621038219508003\n",
            "Epoch 40/100, Loss: 0.5355889446595136\n",
            "Epoch 50/100, Loss: 0.5249724344295614\n",
            "Epoch 60/100, Loss: 0.5173473866546855\n",
            "Epoch 70/100, Loss: 0.5272290408611298\n",
            "Epoch 80/100, Loss: 0.485746550209382\n",
            "Epoch 90/100, Loss: 0.516620042569497\n",
            "Epoch 100/100, Loss: 0.48635345434441285\n",
            "Run 4/10\n",
            "Epoch 10/100, Loss: 0.5964369475841522\n",
            "Epoch 20/100, Loss: 0.5933600059326958\n",
            "Epoch 30/100, Loss: 0.565709296394797\n",
            "Epoch 40/100, Loss: 0.5173511619077009\n",
            "Epoch 50/100, Loss: 0.5307206704336054\n",
            "Epoch 60/100, Loss: 0.5306343217106426\n",
            "Epoch 70/100, Loss: 0.5256012134692248\n",
            "Epoch 80/100, Loss: 0.52054904664264\n",
            "Epoch 90/100, Loss: 0.4957372134222704\n",
            "Epoch 100/100, Loss: 0.4995628051898059\n",
            "Run 5/10\n",
            "Epoch 10/100, Loss: 0.6171377599239349\n",
            "Epoch 20/100, Loss: 0.5843598781263127\n",
            "Epoch 30/100, Loss: 0.5672793169231976\n",
            "Epoch 40/100, Loss: 0.5737831697744482\n",
            "Epoch 50/100, Loss: 0.5464044362306595\n",
            "Epoch 60/100, Loss: 0.547490334686111\n",
            "Epoch 70/100, Loss: 0.5342606113237494\n",
            "Epoch 80/100, Loss: 0.5474927364026799\n",
            "Epoch 90/100, Loss: 0.5021963759380228\n",
            "Epoch 100/100, Loss: 0.5091860846561544\n",
            "Run 6/10\n",
            "Epoch 10/100, Loss: 0.5964971552876865\n",
            "Epoch 20/100, Loss: 0.5699838881983477\n",
            "Epoch 30/100, Loss: 0.5344356964616215\n",
            "Epoch 40/100, Loss: 0.5043214980293723\n",
            "Epoch 50/100, Loss: 0.5149929786429686\n",
            "Epoch 60/100, Loss: 0.5332280038034215\n",
            "Epoch 70/100, Loss: 0.49399254690198335\n",
            "Epoch 80/100, Loss: 0.5185798695858788\n",
            "Epoch 90/100, Loss: 0.4686453359968522\n",
            "Epoch 100/100, Loss: 0.4649835246450761\n",
            "Run 7/10\n",
            "Epoch 10/100, Loss: 0.5903011332539951\n",
            "Epoch 20/100, Loss: 0.5640442590503132\n",
            "Epoch 30/100, Loss: 0.5555783720577464\n",
            "Epoch 40/100, Loss: 0.5501199688981561\n",
            "Epoch 50/100, Loss: 0.5277533697731355\n",
            "Epoch 60/100, Loss: 0.5281676641281914\n",
            "Epoch 70/100, Loss: 0.5035473599153406\n",
            "Epoch 80/100, Loss: 0.4953893598388223\n",
            "Epoch 90/100, Loss: 0.4915818650932873\n",
            "Epoch 100/100, Loss: 0.49963464105830474\n",
            "Run 8/10\n",
            "Epoch 10/100, Loss: 0.604591998107293\n",
            "Epoch 20/100, Loss: 0.5634087271550122\n",
            "Epoch 30/100, Loss: 0.5841934856246499\n",
            "Epoch 40/100, Loss: 0.5739780953701805\n",
            "Epoch 50/100, Loss: 0.5890863450134501\n",
            "Epoch 60/100, Loss: 0.5478457072201897\n",
            "Epoch 70/100, Loss: 0.5275258974117392\n",
            "Epoch 80/100, Loss: 0.5443008349222296\n",
            "Epoch 90/100, Loss: 0.5051172805183074\n",
            "Epoch 100/100, Loss: 0.49933870224391713\n",
            "Run 9/10\n",
            "Epoch 10/100, Loss: 0.5836247310918921\n",
            "Epoch 20/100, Loss: 0.5594555577811073\n",
            "Epoch 30/100, Loss: 0.5501731818213182\n",
            "Epoch 40/100, Loss: 0.5357895782765221\n",
            "Epoch 50/100, Loss: 0.5352054816835067\n",
            "Epoch 60/100, Loss: 0.49803588583188896\n",
            "Epoch 70/100, Loss: 0.5037503777181401\n",
            "Epoch 80/100, Loss: 0.48546361747909994\n",
            "Epoch 90/100, Loss: 0.47658200386692495\n",
            "Epoch 100/100, Loss: 0.49467165680492625\n",
            "Run 10/10\n",
            "Epoch 10/100, Loss: 0.5871126406333026\n",
            "Epoch 20/100, Loss: 0.5752020250348484\n",
            "Epoch 30/100, Loss: 0.5528532485751545\n",
            "Epoch 40/100, Loss: 0.5337334301541833\n",
            "Epoch 50/100, Loss: 0.5249895663822398\n",
            "Epoch 60/100, Loss: 0.5118025611428654\n",
            "Epoch 70/100, Loss: 0.49647901513997245\n",
            "Epoch 80/100, Loss: 0.48903127978829775\n",
            "Epoch 90/100, Loss: 0.4764646944754264\n",
            "Epoch 100/100, Loss: 0.486207539544386\n",
            "\n",
            "Average Metrics:\n",
            "Accuracy: 0.5444 ± 0.0793\n",
            "Balanced_accuracy: 0.5658 ± 0.0480\n",
            "Precision: 0.6869 ± 0.0394\n",
            "Recall: 0.4786 ± 0.1876\n",
            "F1: 0.5467 ± 0.1526\n",
            "Auc: 0.6186 ± 0.0371\n",
            "Kappa: 0.1247 ± 0.0934\n",
            "\n",
            "Average Results Dataframe:\n",
            "   accuracy  balanced_accuracy  precision    recall        f1       auc  \\\n",
            "0  0.544444           0.565756    0.68695  0.478571  0.546658  0.618592   \n",
            "\n",
            "      kappa  \n",
            "0  0.124676  \n",
            "\n",
            "Best Results Dataframe:\n",
            "   accuracy  balanced_accuracy  precision    recall        f1       auc  \\\n",
            "0  0.644444           0.598739     0.6875  0.785714  0.733333  0.644958   \n",
            "\n",
            "      kappa  \n",
            "0  0.207048  \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, roc_curve, classification_report\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Usage\n",
        "'''\n",
        "X_train_tensor = torch.tensor(atc_gnn_mol_features_train_df.values[:,:-1], dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(atc_gnn_mol_features_train_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(atc_gnn_mol_features_test_df.values[:,:-1], dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(atc_gnn_mol_features_test_df['Class'].values, dtype=torch.float32).view(-1, 1)'''\n",
        "\n",
        "X_train_tensor = torch.tensor(gnn_mol_features_train_df.values[:,:-1], dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(gnn_mol_features_train_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(gnn_mol_features_test_df.values[:,:-1], dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(gnn_mol_features_test_df['Class'].values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "\n",
        "class DenseModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, dropout_rate=0.3):\n",
        "        super(DenseModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def train_dense_model(model, train_loader, criterion, optimizer, epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_preds).flatten(), np.array(all_labels).flatten()\n",
        "\n",
        "def run_experiment(X_train, y_train, X_test, y_test, input_dim, hidden_dim1, hidden_dim2, output_dim, criterion, epochs=100, batch_size=32, sampling_method='smote'):\n",
        "    # Convert tensors to numpy arrays for resampling\n",
        "    X_train_np = X_train.cpu().numpy()\n",
        "    y_train_np = y_train.cpu().numpy().ravel()\n",
        "    X_test_np = X_test.cpu().numpy()\n",
        "    y_test_np = y_test.cpu().numpy().ravel()\n",
        "\n",
        "    # Apply sampling method\n",
        "    if sampling_method == 'ros':\n",
        "        sampler = RandomOverSampler(random_state=42)\n",
        "    elif sampling_method == 'rus':\n",
        "        sampler = RandomUnderSampler(random_state=42)\n",
        "    elif sampling_method == 'smote':\n",
        "        sampler = SMOTE(random_state=42)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid sampling method. Choose 'ros', 'rus', or 'smote'.\")\n",
        "\n",
        "    X_train_resampled, y_train_resampled = sampler.fit_resample(X_train_np, y_train_np)\n",
        "    X_test_resampled, y_test_resampled = X_test_np, y_test_np  # We don't resample the test set\n",
        "\n",
        "    # Convert back to tensors\n",
        "    X_train_tensor = torch.tensor(X_train_resampled, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train_resampled, dtype=torch.float32).view(-1, 1)\n",
        "    X_test_tensor = torch.tensor(X_test_resampled, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test_resampled, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    model = DenseModel(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    train_dense_model(model, train_loader, criterion, optimizer, epochs)\n",
        "\n",
        "    test_preds, test_labels = evaluate_model(model, test_loader)\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(test_labels, test_preds > 0.5),\n",
        "        'balanced_accuracy': balanced_accuracy_score(test_labels, test_preds > 0.5),\n",
        "        'precision': precision_score(test_labels, test_preds > 0.5),\n",
        "        'recall': recall_score(test_labels, test_preds > 0.5),\n",
        "        'f1': f1_score(test_labels, test_preds > 0.5),\n",
        "        'auc': roc_auc_score(test_labels, test_preds),\n",
        "        'kappa': cohen_kappa_score(test_labels, test_preds > 0.5)\n",
        "    }\n",
        "\n",
        "    class_report = classification_report(test_labels, test_preds > 0.5, output_dict=True)\n",
        "\n",
        "    return metrics, class_report, model, test_preds, test_labels\n",
        "\n",
        "def main(X_train, y_train, X_test, y_test, input_dim, hidden_dim1, hidden_dim2, output_dim, criterion, n_runs=10, epochs=100, batch_size=32):\n",
        "    all_metrics = []\n",
        "    best_accuracy = 0\n",
        "    best_model = None\n",
        "    best_class_report = None\n",
        "    best_predictions = None\n",
        "    best_labels = None\n",
        "    best_metrics = None\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"Run {run + 1}/{n_runs}\")\n",
        "        metrics, class_report, model, test_preds, test_labels = run_experiment(X_train, y_train, X_test, y_test, input_dim, hidden_dim1, hidden_dim2, output_dim, criterion, epochs, batch_size, sampling_method = 'smote')\n",
        "        all_metrics.append(metrics)\n",
        "\n",
        "        if metrics['accuracy'] > best_accuracy:\n",
        "            best_accuracy = metrics['accuracy']\n",
        "            best_model = model\n",
        "            best_class_report = class_report\n",
        "            best_predictions = test_preds\n",
        "            best_labels = test_labels\n",
        "            best_metrics = metrics\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_metrics = pd.DataFrame(all_metrics).mean().to_dict()\n",
        "    std_metrics = pd.DataFrame(all_metrics).std().to_dict()\n",
        "\n",
        "    # Create dataframes for average and best results\n",
        "    avg_df = pd.DataFrame([avg_metrics])\n",
        "    best_df = pd.DataFrame([best_metrics])\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs('/content/drive/MyDrive/HOB/', exist_ok=True)\n",
        "\n",
        "    # Save average results to CSV\n",
        "  #  avg_df.to_csv('/content/drive/MyDrive/HOB/average_results.csv', index=False)\n",
        "\n",
        "    # Save best results to CSV\n",
        " #   best_df.to_csv('/content/drive/MyDrive/HOB/best_results.csv', index=False)\n",
        "\n",
        "    # Save best predictions in a text file\n",
        " #   np.savetxt('/content/drive/MyDrive/HOB/best_predictions.txt', best_predictions, fmt='%.4f')\n",
        "\n",
        "    # Save best class-level results\n",
        "#    with open('/content/drive/MyDrive/HOB/best_class_results.txt', 'w') as f:\n",
        "  #      f.write(str(best_class_report))\n",
        "\n",
        "    # Print average results\n",
        "    print(\"\\nAverage Metrics:\")\n",
        "    for metric, value in avg_metrics.items():\n",
        "        print(f\"{metric.capitalize()}: {value:.4f} ± {std_metrics[metric]:.4f}\")\n",
        "\n",
        "    return avg_metrics, std_metrics, best_model, avg_df, best_df, best_predictions, best_labels\n",
        "\n",
        "\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "hidden_dim1 = 64\n",
        "hidden_dim2 = 64\n",
        "output_dim = 1\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "avg_metrics, std_metrics, best_model, avg_df, best_df, best_predictions, best_labels = main(\n",
        "    X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor,\n",
        "    input_dim, hidden_dim1, hidden_dim2, output_dim, criterion,\n",
        "    n_runs=10, epochs=100, batch_size=32\n",
        ")\n",
        "\n",
        "# Save the best model\n",
        "torch.save(best_model.state_dict(), '/content/drive/MyDrive/HOB/Rerun/best_model.pth')\n",
        "\n",
        "# Print dataframes\n",
        "print(\"\\nAverage Results Dataframe:\")\n",
        "print(avg_df)\n",
        "print(\"\\nBest Results Dataframe:\")\n",
        "print(best_df)\n",
        "\n",
        "# Create a dataframe with best predictions and true labels\n",
        "best_predictions_df = pd.DataFrame({\n",
        "    'True_Label': best_labels,\n",
        "    'Predicted_Probability': best_predictions,\n",
        "    'Predicted_Class': (best_predictions > 0.5).astype(int)\n",
        "})\n",
        "best_predictions_df.to_csv('/content/drive/MyDrive/HOB/Rerun/class_smotefm_run10.csv', index=False)\n",
        "avg_df.to_csv('/content/drive/MyDrive/HOB/Rerun/avg.csv', index=False)\n",
        "best_df.to_csv('/content/drive/MyDrive/HOB/Rerun/best.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}